---
layout: post
title: 'ðŸ‘¾ #44 DDPG, TD3, SAC and PPO comparison'
date: 2022-08-28 00:00:00
description: discussions about continuous control models
tags: rl
categories: rl
---

# Introduction 

Reinforcement learning has been widely used in many applications. With the emergence of deep learning, the speed of development has been accelerated. Many reinforcement learning methods are proposed which improve the sample efficiency, encourage exploration, and robust training. The RL training obtains samples in online-manner and the training is usually unstable if the environment is more dynamic. Therefore, more robust algorithms such as TRPO, PPO, double-Q, and TD3 are proposed. The original papers includes the detailed comparison between other algorithms, but detailed tuning of hyperparameters could be different.  One example is learning-rate decay which progressively reduces the learning rate while training to make the neural network stable. Such implementation detail could be different among the practitionors. 
In this experiment,  we compare four control RL algorithms, DDPG, PPO, TD3, and SAC+TD3 in six control tasks.
We used the CleanRL implementation which is based on non-modular programming.. There are several issues on the comparison of RL models 
* The implementation of the algorithm can be different.
* The combination of Model architecture + RL algorithm 


# Models : TD3, DDPG, SAC, PPO 

We briefly introduce the main parts of model algorithms using simple notations. When algorithms are described, the detailed components would not be described such as gamma or other additional loss terms.

* Simple Notations 
  * `NQ` : next Q-value 
  * `S0`, `S1` : current state and the next state 
  * `R` : reward 
  * `A1` : actor whose input is the observation and outputs the policy. 
  * `Q1`, `Q2` : Q-network 1 and 2 
  * `TQ1`, `TQ2` : Target Q-network 1 and 2 to compute the Q value of the next state and action. 

<hr/>

## ðŸ‘¾ DDPG 
Components : (`Q1`, `TQ1`, `A1`, `TA1`)

* `NQ` : `R` + `TQ1`(`S1`, `TA1`(`S1`))
* Q-loss : ( `Q1`(`S0`) -  `NQ` )^2
* Actor : maximize `Q1`(`S0`, `A1`(`S0`))
* Target is updated softly

DDPG uses single Q ands single agent with targets to update the models properly. 
Target Q-network is used to compute the Q-value of the next-state Q-value. This technique is originally used in the DQN paper. 
Actor is used to sample continuous action given the current state. The Actor maximizes the TD error instead of the MC return. 

<hr/>


## ðŸ‘¾ TD3 

Components : (`Q1`, `Q2`, `TQ1`, `TQ2`, `A1`, `TA1`)
* `NQ` : `R` +  min(`TQ1`(`S1`, `TA1`(`S1`), `TQ2`(`S1`, `TA1`(`S1`) )
* Q-loss : ( `Q1`(`S0`) -  `NQ` )^2, ( `Q2`(`S0`) -  `NQ` )^2
* Actor : maximize `Q1`(`S0`, `A1`(`S0`))

TD3 has the sample training logic with the DDPG, but has two Q-networks to avoid the maximum bias of the Q-network. 
The prediction of the next state value is the minimum between two Q-values obtained from two Qs. Note that the actor maximizes the Q value from the Q1 network. I believe this part could be implement differently. 

<hr/>


## ðŸ‘¾ SAC 
Components: (`Q1`, `Q2`, `TQ1`, `TQ2`, `A1`)
* `NQ` : `R` +  min(`TQ1`(`S1`, `TA1`(`S1`), `TQ2`(`S1`, `TA1`(`S1`) )
* Q-loss : ( `Q1`(`S0`) -  `NQ` )^2, ( `Q2`(`S0`) -  `NQ` )^2
* Actor : maximize min(`Q1`(`S0`, `A1`(`S0`)), `Q2`(`S0`, `A1`(`S0`))) + log pi `A1`(`S0`)

SAC uses soft value function which has entropy term with the reward function. It assumes soft Bellman operator and known to the explore more than the entropy loss term.  The advantage of three models are 
* DDPG : when the environment simple and does not require exploration, and there may be no maximum bias. 
* TD3 : when Q should be learned more conservatively than DDPG. 
* SAC : when the environment requires more exploration. 

<hr/>

## ðŸ‘¾ PPO 
Components : (`C1`, `A1`)
* `V` : (`C1`(`S0`) - `R`) 
* `PI_old` : `A1`(`S0`)  [rollout in the environment]
* `Ratio` : `A1` (`S0`) /  `PI_old` (`S0`)
* Actor : maximize  min( `Ratio` * (`R` - `V`), clip(`Ratio`, 1-e, 1+e) * (`R`-`V`))

PPO has no Q function and this is the only on-policy training algorithm. 
* Advantage : Update the policy slightly from the current policy. The update is expected to be stable. 
* Disadvantage :  sample inefficient, the training update is done based on the rollout samples. 

<hr/>


# Experiments 

<center>
<div class="row mt-3">
        {% include figure.html path="assets/img/exp44/return.png" class="img-fluid-100 rounded z-depth-2 max-width-3" zoomable=true %}
</div>
<p> Figure 1. Training results on 6 envrionments with 4 models averaged over 3 seeds. </p>
</center>


The ðŸ‘¾DDPG had the highest return in the Swimmer-v2 environment whose control is relatively simpler than other environments, while the ðŸ‘¾DDPG had the worst performance in Ant-v3, Walker2d-v2, and Hopper-v0 environment. The simplest model should be used in the environment where the training is guaranteed. ðŸ‘¾TD3 and ðŸ‘¾SAC had quite similar return graphs, while the ðŸ‘¾SAC had higher return. It is because ðŸ‘¾SAC is trained to maximize reward and the entropy both. The result shows that ðŸ‘¾SAC does not sacrifice the return. The ðŸ‘¾PPO had lower variance in return compared to other models and the trainings were quite slow in Ant-v3 and HalfCheetah.  However, the x-axis is the number of samples used in the training and the ðŸ‘¾PPO requires more samples compared to off-policy algorithms. In practice, we should test all the RL algorithms in a given environment. Here are some criteria to the choice of models. 

* **environment-perspective**
  1. Is the environment is simple  : ðŸ‘¾DDPG, ðŸ‘¾PG 
  2. Is the environment is little bit complex : ðŸ‘¾TD3, ðŸ‘¾PPO
  3. Is the environment is quite complex  : ðŸ‘¾SAC, ðŸ‘¾PPO + entropy 

* **policy-perspective**
  1. Do you want  on-policy  : ðŸ‘¾PPO
  2. Do you want  off-policy :  ðŸ‘¾SAC