---
layout: post
title: 'üç∫ #20 ViT Class-wise Attention Similarity Score (CASS)'
date: 2022-06-03 00:00:00
description: Visualize global explanation of vision transformer by combining attention and gradient.  
tags: XAI, VISION
categories: XAI
---

# 1. Introduction 

Transformer uses attention mechanism which gives weights to more related features. In a supervised learning of viT, the highly weighted features are the regions of the object in an image. Visualization of attention score gives the decision of the transformer. However, enven though the model is trained to maximize the target label, the output probability can have high entropy because of multiple classes objects in an image. Therefore, the  visualization of attention scores must consider the target class. To takcle this problem, we propose **Class-wise Attention Similarity Score (CASS)** which combines input attribution method for the target class and attention matrices. 


# 2. Background 

The Figures below shows the meaning of the attention heatmap and the layer-wise analysis. 
For more information, read the [post: üîÆ #12 Attention Score in Vision Transformer](https://fxnnxc.github.io/blog/2022/exp_12/)

<center>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/exp12/viT.png" class="img-fluid rounded z-depth-1" zoomable=true %}
                <p style="color:black"> Figure 1. Vision transformer structure and an attention heatmap example. </p>
    </div>
        <div class="col-sm mt-6 mt-md-0">
          {% include figure.html path="assets/img/exp12/index_17_quantile_0.png" class="img-fluid rounded z-depth-1" zoomable=true %}
        <p>
        Figure 2. Aggregation of heads and layers for an eagle image instance. 
        </p>
    </div>
</div>
</center>


# 3. Method: CASS

We propose **Class-wise Attention Similarity Score (CASS)** which computes the simiarity score between input attribution $\phi^c(x_i)$ and attention scores $A_{l,h}(x_i)$.  


<p>
Given decision $f^c(x_i)$ of function $f$ on the class $c$, input attribution is computed by $\phi^c(x_i)$. The attention score $A_{l,h}(x_i)$ represents the attention score of head $h$ in layer $l$ for instance $x_i$.  First, the scores are normalized to gaurantee  
$||\phi^c(x_i)||_2 = 1$ and $||A_{l,h}(x_i)||=1$. Then, the similarity of the attentino score and the input attribution is calculated by </p>

$$
S_{l,h}^c(x_i) = \cos{\Big(\phi^c(x_i)$, A_{l,h}(x_i)  \Big)}
$$

Note that it measures the difference between attention map and the input attribution. By computing $S_{h,l}^c(x_i)$ over all heads and layers, 
we obtain the **CASS** 

$$v^c(x_i) = [S_{1,1}^c(x_i), S_{1,2}^c(x_i), \cdots,  S_{L, H-1}^c(x_i), S_{L,H}^c(x_i)]^{\mathrm{T}}$$

Figure 3 shows an exmple of input attribution and attention heatmaps.

<center>
<div class="row mt-3">
        {% include figure.html path="assets/img/exp20/heads.png" class="img-fluid rounded z-depth-1" zoomable=true %}
</div>
<p>
Figure 3. Input attribution and attention heatmaps for a shark image. The input attribution resembles some attention patterns in the early layers, while there are little heads whose pattern similar with the SmoothGrad. 
</p>
</center>

# 4. Result 

We evaluated our method on the ImageNet 2012 Validation dataset. The dataset has 50000 images with 1000 classes. Therefore, 50 images for each class. We used the SmoothGrad for the input attribution method. 

## 4.1 CASS 

We first computed CASS for 150 samples and found that some heads had low similarity score for all the samples (see Figure 4). By checking all 50,000 samples, we found two properties of viT (see Figure 5). 

1. **The similarity score decreases almost monotonically in the deeper layers.** 
2. **The degree of similarity are not same for all the heads in the same layer.**

Two properties support the idea that there is specific role of each head in viT and not all the heads have same degree of focusing on the object regions. Only some  heads are focuseson the main parts of an object. 

<center>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/exp20/similarity_150.png" class="img-fluid rounded z-depth-1" zoomable=true %}
                <p style="color:black"> Figure 4. CASS for 3 classes, 50 samples for each classes. The 77th head has low similarity score compared with other heads in the same layer. This supports idea that each head has the differnt degree of similarity with input attribution. </p>
    </div>
</div>
      <div class="col-sm mt-6 mt-md-0">
        {% include figure.html path="assets/img/exp20/similarity_full.png" class="img-fluid rounded z-depth-1" zoomable=true %}
      <p>
      Figure 5. CASS for all ImageNet2012 validation images. The dataset consists of 1000 classes, 50 samples for each classes. The head index is sorted by the mean score of all the samples in the same layer. The 95% confidence interval is also plotted.
      </p>
      </div>
</center>


## 4.2 Application of CASS

CASS can be directly used to weighting the attention heatmap. Each element of attention heatmap $A_{l,h}(x_i)$ can be weighted by $v^c(x_i)_{l,h}$. This gives better attetion heatmap. Figure 6 shows two examples of our proposed CASS attention heatmap. The result shows that our method highlights

<center>
<div class="row mt-3">
        {% include figure.html path="assets/img/exp20/cass.png" class="img-fluid rounded z-depth-1" zoomable=true %}
</div>
      <p>
        Figure 6. Qualitative results of attention aggregation.  
        </p>
</center>


# 5. Conclusion 

In this experiment, we present Class-wise Attention Similarity Score. We empirically show that the similarity score can identify the role of heads. Some heads have higher relation with object region while some heads do not.  The internal structure of deep learning is known as a black-box. The role of the internal parts are not explained before. This experiment show that the roles can be identified by computing the similarity score with an input attribution. 


