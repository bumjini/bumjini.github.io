---
layout: post
title: 'üìú One-hop feature Reuse for Meta Learning'
date: 2022-12-16 00:00:00
description: Reduce the loss of information by directly passing to the final layer. 
tags: vision
categories: vision
---

üîñ check out the [paper format](https://drive.google.com/file/d/1sdEju45BpNj_5TQxvyE3ZbG9QeSqnIOn/view?usp=share_link)


<p style='text-align:center'>
Let's talk about <strong> CNN Filters  </strong> 
</p>

Each CNN Filter convolves input features. 
For example, the first input channel of CNN filters are distribution of colors. 

It is known that **each channel captures different patterns such as strides**. 
The channel outputs are activated by ReLU and large score has more meaning. 
Therefore, the Global Average Pooling (GAP) can explain the channels.  


When we average the pixels for each channel, **we lose much information and only mean statistics of each channel is preserved**. The limited information could be used to separate the color classification. However, input has higher semantics and local regions have more meaningful information such as 
eyes  and ears of a dog image. These information is captured in the higher layer when the deep neural network is large enough. 

<!-- Concept -->
<center>
<div class="row mt-3">
        {% include figure.html path="assets/img/one_hop/concept.png" class="img-fluid rounded z-depth-2 max-width-3" zoomable=true %}
</div>
<p> highscore GAP Samples form meaningful clusters   </p>
</center>


<hr/>


# Main Idea


Our idea is simple. 

<blockquote>
Consider the distribution of Global Average Pooling (GAP) and directly map the information to the middle layer of deep neural network for one-hop feature transportation. 
</blockquote>

The network can obtain beneficial information from the one-hop transportation. 
This technique can be applied to any CNN based model architecture where the role of channels is clearly distinguished. 


<!-- One Hop -->
<center>
<div class="row mt-3">
        {% include figure.html path="assets/img/one_hop/one_hop.png" class="img-fluid rounded z-depth-2 max-width-3" zoomable=true %}
</div>
<p> GAP in internal channels are passed to the last parts to consist one-hop features </p>
</center>



Theoretically speaking, even though the sequential processing with fixed representation loose information. 
It is even true with Residual Connection as the deep neural network should compose the compact output representation in any way. 
Data Processing Inequality says, the mutual information of far random variables is less than the close one.

<!-- DataProcessingIneq -->
<center>
<div class="row mt-3">
        {% include figure.html path="assets/img/one_hop/dpi.png" class="img-fluid rounded z-depth-2 max-width-3" zoomable=true %}
</div>
<!-- <p> Scatter plot for each $\epsilon$</p> -->
</center>

<hr/>

Figure below shows the GAP statistics for randomly sampled
1000 inputs for pretrained ProtoNet. Note that the mean
and the standard deviation are in the same block, while each
block has largely different statistics over channels. The
trend is different also for dataset and model architectures.
Therefore, we should consider the statistics of GAP before
finetuning with OH features.


<!-- GAP -->
<center>
<div class="row mt-3">
        {% include figure.html path="assets/img/one_hop/gap.png" class="img-fluid rounded z-depth-2 max-width-3" zoomable=true %}
</div>
<p> GAP statistics</p>
</center>

<hr/>


# Encoding Architectures


**SimpleOH** is a single-layer transformation to  match the dimension size of one-hop features with final hidden representation. Matching is required as the larger dimension has a disadvantage on Euclidean distance. 


**LayerAttentionSQ** is a task specific OH feature encoding
and compares the OH features based on the Multi-Head
Attention (MHA) for each query. The main idea is that ‚Äùhow
support OH-features should be encoded (query in MHA) for
query data and how query OH-features should be encoded
for support data‚Äù. MHA has layer normalization before
vector dot-product.


**LayerAttentionIntra** is a variation of LayerAttention and
encodes the support OH features by self-attention, instead
of cross attention with query OH features. Therefore, the encoding does not depends on the query dataset and the sup-
port dataset should be naturally encoded for task separation.

<!-- Architecture -->
<center>
<div class="row mt-3">
        {% include figure.html path="assets/img/one_hop/architecture.png" class="img-fluid-100 rounded z-depth-2 max-width-3" zoomable=true %}
</div>
<p> Architectures to encode support and query dataset for one-hop features</p>
</center>


<hr/>


# Experimental Results

The Table below shows the overall test performance of pretrained
and finetued models with OH Meta learning methods. We
found that additional finetuning of pretrained model with
dropout does not improve the generalization performance, while OH Meta learners show generally improved perfor-
mance and OH Meta learners generalize well for most cases.



<!-- ACC -->
<center>
<div class="row mt-3">
        {% include figure.html path="assets/img/one_hop/acc.png" class="img-fluid rounded z-depth-2 max-width-3" zoomable=true %}
</div>
</center>

There are some cases where the performance is **worse than**
the baseline ProtoNet. As the most failures are observed
with freezed network, we conclude that the reason is **the
balancing of existing last feature and OH-features**. This
problem is solved when the encoder is not freezed.

In the case of CNN model structure, SimpleOH without
freezing shows the best performance. One reason is that
**CNN has few number of filters** than ResNet and there is
no residual connection. The encoded OH features have
**more clear meaning** for each channel and distinguishable by comparing the OH features for support and query data.

On the other hand, ResNet12 showed the best performance
with LayerAttentionIntra and LayerAttentionSQ and **more complex and advanced encoding** 
of OH features benefits the training. We found that ResNet12 had the optimal per-
formance for Omniglot dataset, while shows worse performance than CNN in MiniImageNet, DermNet, and ISIC.
One reason is **the overfitting of training set**. 

One of the major observations is **the effectiveness of freezing**. 
The performance of OH Meta learners with freezing its encoder shows significantly worse performance than training the encoder again. In the case of freezed encoder, the output of last feature $f(x)$ is fixed and the OH features
are responsible for Euclidean distance of support and query.
However, **it is hard for freezed encoder to match the balance.**
 

<hr/>

# Conclusion 

<div>
<p>
Neural networks are efficient learners and compact representations are formed in the middle of layers.
<tag style="color:blue">This work shows that the knowledge in the trained neural networks can be extracted by GAP. </tag>
 We show that the GAP patterns can be used for few-shot classification for both simple CNN and
ResNet12 architectures.
</p>

<p>
We acknowledge that the evaluation of OH Meta is done
with ProtoNet only. As there are several meta learning
algorithms successfully adapted for few-shot classification,
it is necessary to evaluate OH features of the state-of-the-art
models. In addition, several pretrained image encoders has
been shown to generalize well to downstream tasks. This
work could be done with such powerful encoders in future.
</p>

</div>