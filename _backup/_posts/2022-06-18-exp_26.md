---
layout: post
title: '〽️ #26 DDPG buffer size and target update frequency'
date: 2022-06-19 00:00:00
description: Understanding the effect of buffer size and smooth target update ratio on the performance.  
tags: rl, ddpg
categories: rl
---

# 1. Introduction 

Deep Deterministic Policy Gradient (DDPG) is the most widely used continuous action policy. The policy of DDPG outputs the action whose support is continuous space. When there are multiple actions to be conducted together, the policy outputs multiple actions. The term deterministic comes from the fact that the output of the model is deterministic. Because DDPG outputs a deterministic action, exploration is done by adding noise to the action. In the early stage of training, large noise is added to the action to encourage exploration. The epsilon decreases along training and low noise is added to an action.  


<center>
{% include figure.html path="assets/img/exp26/ddpg.png" class="img-fluid rounded z-depth-1" zoomable=false %}
<p> Figure 1. DDPG architecture. Critic can be removed if we use MC return </p>
</center>



### Exploration Strategy 
* Training: when training the model, we add noise to the chosen action by Ornstein-Uhlenbeck Process which uses drift term to the opposite value direction. 
* Evaluation: when evaluating the model, the noise term is set to zero. 

### Sensitive Parameters : Buffer Size and Tau

Because we optimize Critic and Actor together, we used Buffer to store recent transitions. When the buffer stores only current states, the critic network may forget the past critic values. Therefore, appropriate size of buffer must be chosen. 

Similar to DQN, DDPG uses target network update. Most of DDPG implementation uses smooth target update, instead of hard target update. The hard target update copies directly the current parameters the target network, while the smooth target update interpolates parameters of two networks. 


# 2. Experiments 


We tested three environments: Hopper, Ant, and MountainCar Continuous.  Buffer size and target update ratio is tested in hopper environment because the optimal policy is easily learned in the environment. Then, detailed experiments are conducted for testing tau sensitivity in Ant and MountainCar continuous environments. The results show that buffer size should be large enough and tau should be appropriately tuned. The result is similar with the result of [DQN buffer size and target update frequency experiment](https://fxnnxc.github.io/blog/2022/exp_24/).


### Hopper : BufferSize and Tau


<center>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <img style="border: 3px solid gold; border-radius: 7px;"  width="200px" src="https://www.gymlibrary.ml/_images/hopper.gif">
        <p><a href="https://www.gymlibrary.ml/environments/mujoco/hopper/">OpenAI Hopper Documentation</a></p>
    </div>
</div>
</center>

To conduct fair comparison, we fixed other hyperparameters such as learning, batch size and the model structure and trainied the model with different buffer size and tau. Figure 2 shows the training and the evaluation results of DDPG training. The buffersize of `5K` showed the best performance among other buffer sizes. We can conclude that with high tau ratio, small buffer size is better (dark red). However if the buffer size is large, small tau is appropriate to learn (light green). The best performance achieved by `5k` buffer size and `1.0e-3` which are middle values. 

<center>
<div class="row mt-3">
        {% include figure.html path="assets/img/exp26/hopper.png" class="img-fluid rounded z-depth-1" zoomable=true %}
</div>
<p> Figure 2. Training and evaluation result of DDPG in the Hopper environment.  </p>
</center>


### Ant, MountainCar : Tau

Even though the buffer size is important, target smooth update variable `Tau` is main focus of this experiment. Tau directly handles deep learning model parameter and controls the target variables which are important to learn updated Q-values. Note that frequent udpate must be encouraged to learn the optimal value function and only instability disturbs the update. 


<center>
<div class="row mt+3">
    <div class="col-sm mt-0 ml-mt-md-0">
        <img style="border: 3px solid gold; border-radius: 7px;"  height="200px" src="https://www.gymlibrary.ml/_images/ant.gif">
        <p><a href="https://www.gymlibrary.ml/environments/mujoco/ant/">OpenAI Ant Documentation</a></p>
    </div>
    <div class="col-sm ">
        <img style="border: 3px solid gold; border-radius: 7px;"  height="200px" src="https://www.gymlibrary.ml/_images/mountain_car_continuous.gif">
        <p><a href="https://www.gymlibrary.ml/environments/classic_control/mountain_car_continuous/">OpenAI MountainCar Documentation</a></p>
    </div>
</div>
</center>

Figure 3 shows the performance of different `Tau` values. The large `Tau` made the model unstable and showed almost `-1500` return. Therefore, we can conclude that high tau value makes neural netowrks diverge. Remark that small `Tau` showed slow learning in the `Hopper` environment. By combining both results, we can conclude that **`Tau` must be tuned to learn fast and not to diverge**. 


<center>
<div class="row mt-3">
        {% include figure.html path="assets/img/exp26/ant_mountain.png" class="img-fluid rounded z-depth-1" zoomable=true %}
</div>
<p> Figure 3. Training and evaluation result of DDPG in the Ant and MountainCar environments.  </p>
</center>


# 3. Conclusion

Reinforcement learning is known to be unstable. One reason is the learning environment, where the agent must be learned the current situation while obtaining advanced samples. Learning requires a large number of iterations (rememeber how much epoch is required to learn in vision and nlp domain). Another reason is additional hyperparamters sensitive to the learning. The configuration of the learning, buffer size and dynamic programming is required to learn a model. In this experiment, we show that the imporatnce of appropriate hyperparmeters. 

Here is our conclusion about hyperparameters:

* **Best : Appropriate Buffer Size and Tau** 
* **Second: Large buffer size + Small Tau update**
* **Third: Small buffer size + Large Tau update**


<hr/>