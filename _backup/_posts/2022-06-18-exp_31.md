---
layout: post
title: 'üéØ #31 Generalized Advantage Estimation with PPO'
date: 2022-07-08 00:00:00
description: Understanding the effect of lambda in generalized advantage estimation with proximal policy gradient
tags: rl, ppo, gae
categories: rl
---

# 1. Introduction 

Generalized Advantage Estimation (GAE) is a method to approximate the advantage based on lambda-returns. The idea of lambda-return is to approximate the return at multiple reward terms. For example, when rewards are given by 1.0 2.0, and 3.0, we can approximate the value of timestep 1 as $1.0 + \gamma v(s_2)$ and timestep 2 as $1.0 + 2.0 \gamma + \gamma v(s_3)  $. When the approximation uses multiple steps to approximate the value, it contains many rewards, and the variance is high because the reward is random variable. In the case of small steps, such as Temporal Difference (TD-0), the variance is low, but the approximation could be unstable. Lambda return balances multi step return (n-return) by lambda term. GAE is actually the same idea of Lambda return with the use of an advantage function instead of just value function. 


#### üöÄ LunarLander-v2 
To test the effect of $lambda$, we trained a **PPO** model in LunarLander-v2 environment. The environment has conditional reward for each event. The reward are compsed with

* Landing on pad: 100-140 points 
* Crash: -100 
* Each leg with ground is + 10 points 
* Firing the main engine is -0.3 points  

The environment has both short-term reward and long-term reward. Therefore, it is adequate to compare the effect of $\lambda$. However, there is a problem of fair comparison. Reinforcement learning has multiple hyperparameters to be tuned and different lambda value could need different hyperparameters. Therefore, we searched hyperparameters for 20 times randomly and showed the best performance for each lambda. [‚Ä¶] means an interval and {‚Ä¶} means a set. 
* Discount factor: {0.95, 0.99}
* Learning rate: [1e-4, 1e-3]
* PPO clip coefficient: [0.1, 0.5]

<!-- TODO insert the gae equation  -->


#### $\lambda \in \set{0.0, 0.5, 0.9, 0.95, 1.0}$

<center>
<div class="row">
    <div class="w-50">
        {% include figure.html path="assets/img/exp31/entropy.png" class="img-fluid-100 rounded z-depth-1" zoomable=true %}
        <p>Figure 1. Entropy while training. As the lambda term decreases, the entropy decreases. </p>
    </div>
    <div class="w-50">
        {% include figure.html path="assets/img/exp31/return.png" class="img-fluid-100 rounded z-depth-1" zoomable=true %}
        <p>Figure 2. return while training. Lambda values of 0.5, 0.9, 0.95 showed stable training.</p>
    </div>
</div>
</center>

Figure 1 showed an entropy of policy while training. We can see that the entropy decreases quickly as the lambda decreases. In the case of $\lambda=1.0$ (MC), the return has high variance and the training is slow. Even though $\lambda=0.0$ has the sharpest decreases in entropy, entropy does not show how the agent performs well. Entropy only shows how much an agent certain about the action. 

In the case of return, the best performance was PPO with $\lambda=0.9$. The return of MC has almost no change while TD-0 showed decreasing return after getting the pick first. It is due to the fact that, TD-0 requires multiple steps to update the result of an epiosode. ‚ö†Ô∏è Even though the agent achieved the best performance earlier than other values of lambda, it does not mean that the policy is optimal because it could be just luck. The model must show a continuous high scores.  **Hence we concluded that $\lambda=0.9$ is the best.**


## Hyperparameter Test

Here we present the result of hyperparamter tuning for each $\lambda \in \set{0.0, 0.5, 0.9, 0.95, 1.0}$. In this test, we randomly selected hyperparamters (learning rate, gamma, ppo_clip) to see the stability of training. 



We observed another disadvantage of TD-0 in hyperparameter test. The result shows that the learning becomes more stable as lambda increases. TD-0 showed the highest variance in episode returns and MC showed the smallest returns.  Lambda = 0.0 had the highest variance.  This is due to the fact that lambda =0.0 is equal to TD-0 and the update depends without considering the future reward because the later parts are approximated in the next state. Hence, Lambda=0.0 is week at long horizon environment. (LunarLander-v2 receives big positive reward by landing or negative at crash. Therefore, TD learning requires more steps to be stabilized.)


<center>
<div class="row mt-3">
        {% include figure.html path="assets/img/exp31/0.0.png" class="img-fluid rounded z-depth-2 max-width-3" zoomable=true %}
</div>
<p> Figure 3. $\lambda=0.0$ </p>
</center>

<center>
<div class="row mt-3">
        {% include figure.html path="assets/img/exp31/0.5.png" class="img-fluid rounded z-depth-1" zoomable=true %}
</div>
<p> Figure 4. $\lambda=0.5$  </p>
</center>



<center>
<div class="row mt-3">
        {% include figure.html path="assets/img/exp31/0.9.png" class="img-fluid rounded z-depth-1" zoomable=true %}
</div>
<p> Figure 5. $\lambda=0.9$</p>
</center>

<center>
<div class="row mt-3">
        {% include figure.html path="assets/img/exp31/0.95.png" class="img-fluid rounded z-depth-1" zoomable=true %}
</div>
<p> Figure 6. $\lambda=0.95$ </p>
</center>


<center>
<div class="row mt-3">
        {% include figure.html path="assets/img/exp31/1.0.png" class="img-fluid rounded z-depth-1" zoomable=true %}
</div>
<p> Figure 7. $\lambda=1.0$  </p>
</center>

Lambda = 1.0 showed the slowest learning curve. The increasing started at approximately 600 timesteps, while other lambda values required 200~300 timesteps to increasing the return. This was due to the fact that Monte Carlo had high variance in approximation for each episode and the training is instable. However, it showed the smallest variance in performance for hyperparameters. 


### Conclusion

* When the environment is sensitive to hyperparameters, don‚Äôt use TD-0. Because the learning could be instable. 
* When stable training is required, try to use MC or lambda close to 1 such as 0.9, 0.95. 
* When properly tuned, TD-0 could show the best performance. But, if there is limited resource, try GAE with lambda>0. 
