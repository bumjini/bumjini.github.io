---
layout: distill
title: 'Probing Neurons with CLIP Concepts'
date: 2023-08-25
description: Project
img: 'https://drive.google.com/uc?export=view&id=1FnJ-NHGqqCgAC6XoNJJCSeOjfUU-TeTc'
---


In this work, we take experiments on probing neurons with concepts annotated by CLIP model. 



## Datasets

We use ImageNet1K to train a classifier and probing both. 
The images for concepts are generated by the **CLIP model** with labels in two groups. 

*  <code style='background-color:#FFFFDD'>Color</code> : [red, yellow, blue, green, none]
*  <code style='background-color:#FFDDFF'>Pattern</code>: [zigzagged, striped, dotted, none] 
*  <code style='background-color:#DDFFFF'>Sex</code> : [male, female, none]

We use the caption : `“This image has a concept: <concept>”`  to generate the likelihoods for concepts. We sort the images with the likelihoods for concepts respectively and manually select 100 images for the conceptual dataset. Here are 20 images from the first to 100th image with step size 5. 



<table>
</tr>
<tr>
    <td> <img src="https://drive.google.com/uc?export=view&id=12YKj_2j5EAA11u13bl5lpzJEyAJdYRXS" style="width:100%" onClick="window.open(this.src)" role="button"> </td>
    <td> <img src="https://drive.google.com/uc?export=view&id=1FnJ-NHGqqCgAC6XoNJJCSeOjfUU-TeTc" style="width:100%" onClick="window.open(this.src)" role="button"> </td> 
    <td> <img src="https://drive.google.com/uc?export=view&id=1Lq5L4l5gqU7R2khDAK1m9uDcqrN6LxA5" style="width:100%" onClick="window.open(this.src)" role="button"> </td>
    <td> <img src="https://drive.google.com/uc?export=view&id=1GNOXnGONa67d6IxdIfqJY_KPWuJInEW-" style="width:100%" onClick="window.open(this.src)" role="button"> </td>
</tr>
<tr style='text-align:center'>
    <td> <code style='background-color:#FFFFDD'>Red</code> Concept </td>
    <td> <code style='background-color:#FFFFDD'>Blue</code> Concept </td>
    <td> <code style='background-color:#FFFFDD'>Green</code> Concept </td>
    <td> <code style='background-color:#FFFFDD'>Yellow</code> Concept </td>
</tr>
<tr>
    <td> <img src="https://drive.google.com/uc?export=view&id=1kcuUlNltgQfFDB7rrwSYC0goecVYOmIv" style="width:100%" onClick="window.open(this.src)" role="button"> </td>
    <td> <img src="https://drive.google.com/uc?export=view&id=13BhwpGgdMF8G5BAyEfZi9YMrFtH-n5NT" style="width:100%" onClick="window.open(this.src)" role="button"> </td>
    <td> <img src="https://drive.google.com/uc?export=view&id=1DFDe-Q6oa8vbAC9qMeaCfFR8eP6idfll" style="width:100%" onClick="window.open(this.src)" role="button"> </td>
    <td> </td>
</tr>
<tr style='text-align:center'>
    <td> <code style='background-color:#FFDDFF'>Dotted</code> Concept </td>
    <td> <code style='background-color:#FFDDFF'>Striped</code> Concept </td>
    <td> <code style='background-color:#FFDDFF'>Zigzagged</code> Concept </td>
    <td> </td>
</tr>
<tr>
    <td> <img src="https://drive.google.com/uc?export=view&id=1SvldJZq_woAK3KKdP9aRtkWXCP357Oya" style="width:100%" onClick="window.open(this.src)" role="button"> </td>
    <td> <img src="https://drive.google.com/uc?export=view&id=1n95wXWhFRTlzpdxcbIocAw65ssYAzm5J" style="width:100%" onClick="window.open(this.src)" role="button"> </td>
    <td> </td>
    <td> </td>
</tr>
<tr style='text-align:center'>
    <td> <code style='background-color:#DDFFFF'>Female</code> Concept </td>
    <td> <code style='background-color:#DDFFFF'>Male</code> Concept </td>
    <td> </td>
    <td> </td>
</tr>
</table>


## Models 

We use MoblineNet-v2 model trained on ImageNet1k dataset. The model is trained for 300 epochs and we use every 30 epochs for probings. 

### trained model 


<iframe src="https://api.wandb.ai/links/bumjin/9r7xwq8y"  style="border:1px solid #000000;height:512px;width:100%">


### probing classifier 


There are two types of classifiers; linear and nonlinear. Following the work in <d-cite key=”othello”> we use ReLU architecture for nonlinear probing classifiers. 
We use labels in the form of `prob-<neurons>-<classifier type>`.

* prob-neuron-linear
* prob-layer-linear
* prob-full-linear
* prob-neuron-nonlinear
* prob-layer-nonlinear
* prob-full-nonlinear


## Results 
