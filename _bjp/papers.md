---
layout: distill
authors: 
    - name: Bumjin Park
      affiliations:
        name: KAIST
bibliography: all.bib
giscus_comments: true
disqus_comments: true
date: 2023-07-10
featured: true
toc:
  - name: Problems
title: 'Papers'
description: 'All the papers...'
importance: 2 
---


### End-to-end memory networks.

### Parameter-Efficient Transfer Learning for NLP  

* Info <d-cite key="houlsby2019parameter" /> :  [[Github](https://github.com/google-research/adapter-bert)], `Efficiency`, 

* PEFT  [[github](https://github.com/huggingface/peft)] : State-of-the-art Parameter-Efficient Fine-Tuning (PEFT) methods




### Injecting Domain Knowledge in Language Models for Task-oriented Dialogue Systems

### Memorizing Transformers 

### LoRA: Low-Rank Adaptation of Large Language Models

* Info <d-cite key="hu2021lora"/> :  



### Chain-of-Thought Prompting Elicits Reasoning in Large Language Models


### Locating and Editing Factual Associations in GPT
* Info : `FactualGPT`



### Mass Editing Memory in a Transformer (MEMIT)



### Memory-Based Model Editing at Scale



### Attcat: Explaining transformers via attentive class activation tokens

### Quantifying attention flow in transformers

### Transformer interpretability beyond attention visualization

### Scaling Transformer to 1M tokens and beyond with RMT


### On Provable Copyright Protection for Generative Models


### Poisoning Language Models During Instruction Tuning


### Bag of Tricks for Training Data Extraction from Language Models,


### Tree-Ring Watermarks: Fingerprints for Diffusion Images that are Invisible and Robust


### CLIP-Dissect: Automatic Description of Neuron Representations in Deep Vision NetworksDownload 

* Info  <d-cite key="oikarinen2022clip"/> : [[pdf](https://openreview.net/forum?id=iPWiwWHc1V)],  `CLIP`, `Network Dissection`, 




### The Future of Fundamental Science Led by Generative Closed-Loop Artificial Intelligence

* 2023.07 

Regarding the the contribution of generative models on scientific discoveries. 


### StyleDrop: Text-To-Image Generation in Any Style

* 2023.06 [[pdf](https://arxiv.org/abs/2306.00983)]

Non-autoregressive modeling, `Muse` + `Style` 


### Muse:Text-To-Image Generation via Masked Generative Transformers

* 2023.01 [[pdf](https://arxiv.org/abs/2301.00704)]

Non-autoregressive modeling
