---
layout: post
title: 'ICLR 2023 paper reviews'
date: 2023-04-19 00:00:00-0400
description: ''
category: ICLR
subcategory : 2023
---




---
<tag class="box-demo-link" style="background:#b4ffff; color:#000000; font-size:18px">Transformer</tag>
<tag class="box-demo-link" style="background:#64DE3A; color:#000000; font-size:18px">IRIS</tag>
<tag class="box-demo-link" style="background:#3549F3; color:#FFFFFF; font-size:18px">Model-based RL</tag>

# Transformers are Sample-Efficient World Models (IRIS)

IRIS (Imagination with auto-Regression over an Inner Speech)


* a world model composed of a discrete autoencoder and an autoregressive Transformer. 
* With the equivalent of only two hours of gameplay in the Atari 100k benchmark, IRIS achieves a mean human normalized score of 1.046, and outperforms humans on 10 out of 26 games



### world models
* pure representation learning (Schwarzer et al., 2021)
* lookahead search (Schrittwieser et al., 2020; Ye et al., 2021),
* learning in imagination (Ha & Schmidhuber, 2018; Kaiser et al., 2020; Hafner et al., 2020; 2021).
* SimPLe recently showed promise in the more challenging Atari 100k benchmark (Kaiser et al., 2020)


### Why did the authors used a discrete autoencoder 
A naive approach would consist in treating pixels as image tokens, but standard Transformer
architectures scale quadratically with sequence length, making this idea computationally intractable.
To address this issue, VQGAN (Esser et al., 2021) and DALL-E (Ramesh et al., 2021) employ a discrete
autoencoder (Van Den Oord et al., 2017)




<Blockquote>

[Deprecated - will be moved to other post]
<h3> How to Compute Human Normalized Score </h3>
It is defined as score_agent−score_random score_human−score_random, where score_random comes from a random policy, and score_human is obtained from human players

$$
\frac{score\_agent - score\_random}{score\_human - score\_random}
$$
</Blockquote>


### Method 

* $x_t \in \mathbb{R}^{h\times w \times 3}$
* $a_t\in \{1, \cdots, A\}$ 
* $r_t \in \mathbb{R}$ reward 
* episode termination $d_t \in \{0, 1\}$ termination

* $E: \mathbb{R}^{h\times w\times 3} \rightarrow \{1,\cdots, N\}^K$ converts an input image $x_t$ into $K$ tokens from a vocabulary size of $N$. 
* $\mathcal{E} = \{e_i\}_{i=1}^N \in \mathbb{R}^{N\times d}$ be the corresponding embedding table of $d$-dimensional vectors. 
* $y_t \in \mathbb{R}^{K\times d}$ be the output of a convolution layer given image $x_t$.
* $z_t = (z_t^1, \cdots, z_t^k)$ : output tokens, the index of the closest embedding vector in $\mathcal{E}$.

#### Dynamics 


* a raw sequence $(x_0, a_0, \cdots, x_t, a_t)$ is transformed into $(z_0^1, \cdots, z_0^K, a_0, \cdots, z_t^1, \cdots, z_t^K, a_t, )$

* Transition : $$\hat{z}_{t+1} \sim p_G(\hat{z}_{t+1} \vert z_{\le t}, a_{\le t})$$ with  $$\hat{z}_{t+1}^k \sim p_G(\hat{z}_{t+1}^k \vert z_{\le t}, a_{\le t}, z_{t+1}^{< k})$$
* Reward: $$\hat{r}_t \sim p_G(\hat{r}_t \vert z_{\le t}, a_{\le t})$$
* Termination : $$\hat{d}_t \sim p_G(\hat{d}_t \vert z_{\le t}, a_{\le t})$$

---