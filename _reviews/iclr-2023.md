---
layout: post
title: 'ICLR 2023 paper reviews'
date: 2023-04-19 00:00:00-0400
description: ''
category: ICLR
subcategory : 2023
---




---
<tag class="box-demo-link" style="background:#b4ffff; color:#000000; font-size:18px">Transformer</tag>
<tag class="box-demo-link" style="background:#64DE3A; color:#000000; font-size:18px">IRIS</tag>
<tag class="box-demo-link" style="background:#3549F3; color:#FFFFFF; font-size:18px">Model-based RL</tag>

# Transformers are Sample-Efficient World Models (IRIS)

Imagination of the world can be modeled with transformers. This paper uses a discrete **autoencoder** and a **transformer decoder** to model **dynamics of the environment** and achieved the state-of-the-art performance on Atari environment with few number of offline samples. 


### IRIS (Imagination with auto-Regression over an Inner Speech)
* a world model composed of a discrete autoencoder and an autoregressive Transformer. 
* With the equivalent of only two hours of gameplay in the Atari 100k benchmark, IRIS achieves a mean human normalized score of 1.046, and outperforms humans on 10 out of 26 games

### Why did the authors used a discrete autoencoder 
A naive approach would consist in treating pixels as image tokens, but standard Transformer
architectures scale quadratically with sequence length, making this idea computationally intractable.
To address this issue, VQGAN (Esser et al., 2021) and DALL-E (Ramesh et al., 2021) employ a discrete
autoencoder (Van Den Oord et al., 2017)

### Required Notations


Raw state, action, reward, and done are defined as follows 

* $x_t \in \mathbb{R}^{h\times w \times 3}$
* $a_t\in \{1, \cdots, A\}$ 
* $r_t \in \mathbb{R}$ reward 
* episode termination $d_t \in \{0, 1\}$ termination

The required representations for the transformers are as follows 

* $E: \mathbb{R}^{h\times w\times 3} \rightarrow \{1,\cdots, N\}^K$ converts an input image $x_t$ into $K$ tokens from a vocabulary size of $N$. 
* $\mathcal{E} = \{e_i\}_{i=1}^N \in \mathbb{R}^{N\times d}$ be the corresponding embedding table of $d$-dimensional vectors. 
* $y_t \in \mathbb{R}^{K\times d}$ be the output of a convolution layer given image $x_t$.
* $z_t = (z_t^1, \cdots, z_t^k)$ : output tokens, the index of the closest embedding vector in $\mathcal{E}$.

### How to model dynamics 

The authors sequentially feed the tokens converted from an image and an action with history length. 
The model autoregressively generates image tokens and activation. The reward and the status of done are computed at the end of the action.  

* a raw sequence $(x_0, a_0, \cdots, x_t, a_t)$ is transformed into $(z_0^1, \cdots, z_0^K, a_0, \cdots, z_t^1, \cdots, z_t^K, a_t, )$
* Transition : $$\hat{z}_{t+1} \sim p_G(\hat{z}_{t+1} \vert z_{\le t}, a_{\le t})$$ with  $$\hat{z}_{t+1}^k \sim p_G(\hat{z}_{t+1}^k \vert z_{\le t}, a_{\le t}, z_{t+1}^{< k})$$
* Reward: $$\hat{r}_t \sim p_G(\hat{r}_t \vert z_{\le t}, a_{\le t})$$
* Termination : $$\hat{d}_t \sim p_G(\hat{d}_t \vert z_{\le t}, a_{\le t})$$


<Blockquote>
[Deprecated - will be moved to other post]
<h3> World models </h3>
* pure representation learning (Schwarzer et al., 2021)
* lookahead search (Schrittwieser et al., 2020; Ye et al., 2021),
* learning in imagination (Ha & Schmidhuber, 2018; Kaiser et al., 2020; Hafner et al., 2020; 2021).
* SimPLe recently showed promise in the more challenging Atari 100k benchmark (Kaiser et al., 2020)
</Blockquote>


<Blockquote>

[Deprecated - will be moved to other post]
<h3> How to Compute Human Normalized Score </h3>
It is defined as score_agent−score_random score_human−score_random, where score_random comes from a random policy, and score_human is obtained from human players

$$
\frac{score\_agent - score\_random}{score\_human - score\_random}
$$
</Blockquote>


---

# Relative Representations Enable Zero-Shot Latent Space Communication 


---

# Transformers Learn Shortcuts to Automata 

<tag class="box-demo-link" style="background:#b4ffff; color:#000000; font-size:18px">automata</tag>
<tag class="box-demo-link" style="background:#b4ddaa; color:#000000; font-size:18px">expressiveness</tag>
<tag class="box-demo-link" style="background:#b4ffff; color:#000000; font-size:18px">automata</tag>



RNN uses sequential processing of inputs and thus represent any **length $T$ system**. However, the computation of $L$-layer transformers is **far less than the length $T$**. There is an immediate mismatch between Classical sequential modeling of computation (Turing machines) vs Transformer architecture For example, distilBret can handle thousands of tokens with 6 sequential layers. This paper theoretically shows how to view transformers as semiautomation. 


### Semiautomata

<tag class="box-demo-link" style="background:#b4ffff; color:#000000; font-size:18px">semiautomation</tag>

$$
A := (Q, \Sigma, \delta)
$$

Same with the deterministic Markov model. 

* $Q$ : a set of states, in transformer, outputs, 
* $\Sigma$ : a set of actions  (input alphabet), in transformer, input sequences, 
* $\delta : Q \times \Sigma \rightarrow Q$ : transition function 



<tag class="box-demo-link" style="background:#b4ffff; color:#000000; font-size:18px">simulation</tag>

Starting from $q_0$, run the transition for $T$ steps. 

$$
\mathcal{A}_{T, q_0}
$$ 


### A seqeunce-to-sequence neural network of length $T$. 

* $f_nn : \mathbb{R}^{T\times d } \times \Theta \rightarrow \mathbb{R}^{T\times d }$
* encoding layer  : $E: \Sigma \rightarrow \mathbb{R}^d$
* decoding layer  :  $E: \mathbb{R}^d \rightarrow Q$

RNN defines a semiautomation with infinitely many state and inputs: $Q = \Sigma = \mathbb{R}^d$. 

### $T$-layer Transformer 

$T$-layer transformer can trivially simulate a semiautomation at length $T$ sequentially: the $t$-th layer can implement the state transition $q_{t-1}\rightarrow q_t$. 

<Blockquote>
<strong> (Theorem) </strong>
 Transformers can simulate all semiautomata $A= (Q, \Sigma, \delta)$ at length $T$, with depth $O(\log T)$, embedding dimension $O(\vert Q \vert)$, attention width $O(\vert Q \vert )$, and MLP width $O(\vert Q \vert^2)$
</Blockquote>

---