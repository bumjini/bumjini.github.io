---
layout: post
title: 'üéØ #1 The Dynamics of LRP in CIFAR 10 Training'
date: 2022-04-25 11:12:00-0400
description: an example of a blog post with some math
tags: RL LRP XAI
categories: XAI-posts
---

<h1 style="font-family:Cursive"> Abstract </h1>

Deep learning model is a black box model that we can not access the decision process. There are many researches to explain the decision of models with LRP, IG, and sentitivity.
However, these methods focused on the trained model and do not consider the training procedure. 
I hypothesize that the training procedure of a model includes a bias and it can be used to explain the decision process of a model. Therefore, I trained CNN model with CIFAR10 dataset and quantifies the decision of the model in the training process. 


<h1 style="font-family:Cursive">  üéØ Setup</h1>

We trained CIFAR 10 dataset supplied by `torchvision.datasets` and here are some samples. The number of training samples is `50,000`. 

<center>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/exp1/Samples.png" class="img-fluid rounded z-depth-10" zoomable=false %}
        <p style="color:blue"> Figure 1. CIFAR 10 images  </p>
    </div>
</div>
</center>



Here is our CNN Model structure. Even though the LRP scores are affected by the model structure, CNN structure models show similar behavior in general. 

```python
self.conv1 = nn.Conv2d(in_channels= 3, out_channels=16, kernel_size=3, stride=1)	
self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=2)
self.max_pool = nn.MaxPool2d(kernel_size=2)
self.fc1 = nn.Linear(in_features=1568, out_features=512)
self.fc2 = nn.Linear(in_features=512, out_features=10)
```

<br/>

<h2> Training Loss </h2>
<center>
<div class="row mt-3" >

<div class="col-sm mt-0 mt-md-0">
    <p style="text-align:left" > We trained the model for <code>5</code> epochs with <code>100</code> minibatch size, and learning rate <code>0.001</code>. At the end of back propagation, we computed LRP scores for <code>100</code> fixed samples. Therefore, we can track the dynamics of LRP for fixed samples.  You can see that the training loss decreases nicely. Even though the model does not converge yet, it is fine becuase our focus is the training process. </p>
        </div>
    <div class="col-sm-4 mt-5 mt-md-0">
        {% include figure.html path="assets/img/exp1/training.png" class="img-fluid rounded z-depth-1" zoomable=true %}
        <p style="color:blue"> Figure 2. training loss </p>

    </div>

</div>
</center>


### Which LRP rule?

There are generally three types of LRPs, `gamma`, `epsilon`, and `z_plus`. We only considered `z_plus` rule because it was the most easy to tunning relevance propagation. 

```python
rules = [
    # for all layers
    {"z_plus":True,  "eps":1e-9}   
]
```


---
<br/>

<h1 style="font-family:Cursive">  üöÄ Result</h1>


### Learning Dynamics

We selected `100` image samples before learning starts. These images are used to quantify the decision of the model by (1) **how the prediction changes and** (2) **how the LRP score changes** 
The result in the Figure  shows that  the relevance score is proportional to the prediction score. 

<center>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/exp1/Dynamics.png" class="img-fluid rounded z-depth-1" zoomable=false %}
                <p style="color:black"> Figure 3. Prediction $(P_i - 0 )$ <br/> and relevance ($R_i - R_0$) </p>
    </div>
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/exp1/Difference.png" class="img-fluid rounded z-depth-1" zoomable=false %}
                <p style="color:black"> Figure 4. Difference between successive iteration in prediction $(|P_{i+1} - P_{i}|)$ <br/> and relevance$(|R_{i+1} - R_i|)$ </p>
    </div>
</div>
</center>

### ‚ö†Ô∏è Warning 
Actually, the result in Figure 3 is not an interesting result because LRP has a conservative property. 


### Heatmaps 


In this section, we present the heatmap of relevance and the differnece of relevances for two successive time steps. 
**Figure 1** shows that the visible change appears at the early stage of the training. **Figure 2** shows that the visible change appears at the edge parts of the truck image. 
<center>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/exp1/LRPs.png" class="img-fluid rounded z-depth-1" zoomable=true %}
                <p style="color:black"> Figure 5. Relevance Score $R_i$ </p>
    </div>
        <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/exp1/LRPd.png" class="img-fluid rounded z-depth-1" zoomable=true %}
        <p>
        Figure 6. LRP heatmap change $\| R_i - R_{i+1} \|$ while training for the truck image 
        </p>
    </div>
</div>
</center>




<center>
<div class="row mt-3">

</div>
</center>

<h1 style="font-family:Cursive">  What I've learned</h1>

* prediction score describes how much the model certain about the prediction 
* relevance describes how much the pixel contributed to the prediction. 

* LRP changes like the prediction score changes. Therefore, we can think **LRP changes smoothly**. 
* The most change occurs at the positions with high relevance scores. 
    * (I hypothesize that **relevance changes a lot  at the important positions**)
    * **high relevance $\rightarrow$ more change**. 

---


