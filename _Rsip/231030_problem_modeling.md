---
layout: share-distill
authors: 
    - name: Bumjin Park
      affiliations:
        name: KAIST
bibliography: all.bib
giscus_comments: false
disqus_comments: false
date: 2023-10-29
featured: true
img: 
title: '[7] Mathematically modeling SIP '
description: 'SIP에 대한 수학적인 모델링'
---


## 결론 

기존 Classification 문제는 X, Y의 관계가 존재한다는 가정하에 Y 를 구분하는 X의 특징을 찾는 것을 목표로 한다. 원천 추정 문제는 공통의 특징이 아닌, 생성 위치가 동일하다는 가정이다.
따라서 **원천 정보는 일반적으로 데이터에 없는 특징**이기에 문장와 원천을 관계를 찾아내는 것이 아니라 **문장과 원천을 연결 짓기 방법**을 생성하느 것이다. 이는 문장의 변화를 기준으로 두 가지 방식이 있다. 

* **A) 문장 고정 가정** : 고정된 문장에 대해서 구분된 원천을 추정하는 연구 
* **B) 문장 변화 가정**  : 문장 변화를 허용하여 원천과 문장을 연결짓는 연구 


#### A) 문장 고정 가정 

고정된 문장 표현을 GPT에 넣고 원천을 구분할 수 있는 특징이 있다는 가정하에 **여러 원천들을 맵핑 시키는 분류 학습 연구.** 
문장 $X$ 와 원천 $S$에 대해서 Joint distribution $P(X, S)$ 에서 $P(S|X)$ 를 유도하는 과정. 

1. X에 원천이 구분되는 특징이 **명확하게 있는** 경우 : Classification 학습으로 해결 가능 
2. X에 원천이 구분되는 특징이 **일부 있는** 경우 : 최소한의 구분되는 특징으로 Classification 학습 
3. X에 원천이 구분되는 특징이 **없는** 경우 : Classification 학습 불가능. 

#### B) 문장 변화 가정 

문장과 원천은 관계가 없다. 관계를 생성하기 위해서 문장 변화 방법은 워터마크처럼 문장을 변화하여 문장과 원천을 연결짓는다. 
Joint distribution $P(X, S)$ 에서 $P(\hat{X}, S)$ 로 모델링을 바꾸는 과정. 

* `나는 밥을 먹었다.` --> `원천:James's 일기, 나는 밥을 먹었다` (direct  source)
* `나는 밥을 먹었다.` --> `원천:EJRF, 나는 밥을 먹었다` (hash type source)
* `나는 밥을 먹었다.` --> `나는 밥을 꽤나 먹었다` (invisible, watermark)



--- 

> 원천은 없는 정보지만, 필요한 정보이다. 

> 기존 Classification 문제는 X, Y의 관계가 존재한다는 가정하에 Y 를 구분하는 X의 특징을 찾는 것을 목표로 한다. 원천 추정 문제는 공통의 특징이 아닌, 생성 위치가 동일하다는 가정이다. 따라서 $X$ 에 클래스를 구분하는 특징이 없을 수 있다. 

따라서 연구 방향은 원천을 구분할 수 있는 특징의 수준에 따라서 나뉜다. 
1. X에 원천이 구분되는 특징이 **있는** 경우 : Classification 학습으로 해결 가능 
2. X에 원천이 구분되는 특징이 **일부 있는** 경우 : 최소한의 구분되는 특징으로 Classification 학습 
3. X에 원천이 구분되는 특징이 **없는** 경우 : Classification 학습 불가능. 


문장 X에 대해서 주어진 원천 $S_X$ 에 대하여, 강제적으로 joint P(X,S) 를 형성하는  방식을 찾는다.  
원천소스 추정의 문제는 문장 $X$ 와 원천 $S$ 에 대해서 다음과 같은 $P(X,S)$ 를 모델링 하는 것이다. 

$$
\begin{gather*}
\operatorname{maximize} P(X, \hat{S}), \hat{S}=S_X  \\
\operatorname{minimize} P(X, \tilde{S} ), \tilde{S} \ne S_X 
\end{gather*}
$$

문장 X와 함께 주어진 원천 혹은 겹치는 경우 원천들을 나타내는 $S$ 에 대해서 joint 를 최대화하면서 해당하지 않는 원천들에 대해서는 확률을 최소화하는 문제이다. 
더 나아가서 GPT 모델의 원천을 찾는 문제는, 모델에 문장과 모델의 관계는 모델 $\theta$ 에 문장 $X$ 를 넣어서 도출된 표현 $H=H(X;\theta)$ 와의 joint를 학습하는 문제로 변환된다. 


$$
\begin{gather*}
\operatorname{maximize} P(H, \hat{S}), \hat{S}=S_{H}  \\
\operatorname{minimize} P(H, \tilde{S} ), \tilde{S} \ne S_{H}
\end{gather*}
$$

* 문장의 표현을 탐구하여, 원래 원천과의 관계성을 학습시키는 연구 분야. 이를 통해, 해당 문장이 모델에 대해서 해당 원천이라고 추정되어야 하는 이유를 찾는 연구. 


### 원천 워터마크 (문장 변화 가정)

워터마크는 제 3자에게 보이지 않는 정보를 숨기는 경우이다. 정보를 숨기기 위해서 문장의 변경이 가능하다. 
워터마크 연구는 문장 $X$ 변화가 자유롭다. 따라서 워터마크를 숨기고, 숨겨진 워터마크를 다시 찾아내는 형태이다. 이를 원천 추정에 적용하면, 

1. 원천이 동일해지도록 문장을 변경한다. 
2. 원천이 동일해지도록 문장 표현을 변경한다. (hidden)



이러한 경우는 문장 혹은 문장표현을 강제적으로 바꿔서 원천이 표시되도록 나타내는 방법이다. 가장 단순한 형태는 원천의 링크를 문장에 표현하는 것이다. 
* `나는 밥을 먹었다.` --> `원천:James's 일기, 나는 밥을 먹었다` (direct  source)
* `나는 밥을 먹었다.` --> `원천:EJRF, 나는 밥을 먹었다` (hash type source)
* `나는 밥을 먹었다.` --> `나는 밥을 꽤나 먹었다` (invisible, watermark)


> 원천소스 문제는 X에 Y를 구분하는 특징이 없는 경우가 허다하다. 이 경우, $X \Rightarrow \hat{X}$ 로 변경하여 원천을 결합해야 한다. 그러나 이 문제는, 원천을 안다는 가정이다. 


----

$X$에 원천 $S$ 의 표현을 강제적으로 주는 경우, 원천을 주고, 원천을 찾는 형태이다. 이는 정답을 주고 정답을 찾으라는 경우이다. 따라서 제대로 된 문제가 아니다. 



## Introduction 

기존 원천소스 추정 문제를 명확한 분류 문제로 설계하는 것은 잘못되었다 <d-footnote> 명확하다는 말의 의미는 정확한 레이블을 찾는 문제를 말한다. 이와는 다르게 원천 소스 맵핑 문제는 멀티레이블이다.  </d-footnote> 대신에 원천 소스 추정문제는 멀티레이블적인 성격을 띄며, 해당 원천으로서 특징을 가지고 있는지 아닌지 판단하는 문제이다. 따라서 기존 Extreme Classification (EC) 문제라고 했던 것보다는 Extreme Multi-label Classification (XML) 문제이다. 단, XML에서 해당 레이블의 특징을 가지고 있다는 가정은 원천 소스 문제에서는 성립하지 않을 수 있다. 왜냐하면 원천으로 추정된 문장들은 서로 어떠한 공통된 특징이 없을 수 있기 때문이다. **원천소스에 맵핑이 된다는 사실**은 존재하지만, 맵핑이 되는 기준이 세워지지 않았던 것이다. 이에 본 연구에서는 **문장이 원천소스에 맵핑이 된다는 사실이 정확하게 무엇을 나타내는지** 연구한다. 


---

Thoughts

---

Thoughts

---

Thoughts

---



## 분포 

원천소스 $S$ 와 문장 $X$ 에 대한 관계는 두 변수의 joint distribution $ p(S, X) $으로 나타낼 수 있다. 원천소스에 해당하는 문장들이 있고, 이 문장은 원천 소스와 관련되어 있다. 
생성모델의 원천소스를 추정한다는 말은 학습 데이터 혹은 생성된 문장 $X$에 대해서 그 원천 $P(S|X)$ 을 모델링 하는 것이다. Pretraining 단계에서는 문장에 대한 출처가 확실하기 때문에 $P(S=s_X | X) = 1$ 을 만족하는 것처럼 보이지만, 동일한 문장이 다른 두 원천 $s_X$와 $s'_X$에 존재할 수 있으므로 $P(S=s_X | X) = 1/2$ 와 $P(S=s'_X | X) = 1/2$ 으로 확률이 달라진다. 물론 대부분의 문장은 sparse 하기에, <d-footnote> (NLP 성질, 단어 확인 필요) </d-footnote> $p(S|X)$ 의 확률 분포는 낮은 엔트로피를 가진다. 파라미터 $\theta$ 를 가지는 예측 GPT 모델에 대해서 원천을 추정하는 $P(S|H(X;\theta))$  경우는 단순히 문장이 아니라 문장에 대한 표현 $H$ 이며, $P(S|X)$ 와는 추론에서 큰 차이를 보인다. 


### $P(S, X)$ 혹은 $P(S, H)$
 
원천과 문장을 모델링하는 $P(S,X)$ 와 다르게, $P(S,H)$는 원천과 문장의 표현 벡터의 관계를 모델링한다. 
다음 단어에 대한 선호도로부터 원천을 추정하는 것이다. 

* 원천의 데이터다 
* 원천에 대해서 구분되는 특징이 있다. 
* 원천의 특징이 있다. 
* 원천을 추가한다.  $P(S, H)$


원천 guided generation. 

---



텍스트로부터 원천에 대해서 명확하게 찾을 수 없다면, 우리가 모델링 해야 하는 부분은 텍스트와 원천의 join distribution 이다. 


## 가정들 


문장과 레이블 데이터에 대한 학습에서 원천소스 맵핑 가능의 가정은 다음과 같다. 

1. 원천소스 구분 표현 존재의 가정 : 주어진 문장 혹은 문장 표현이 원천을 구분할 수 있는 특징이 있다. 
2. 불합리한 특징의 가정 
3. 추가 방향성 표현의 가정  

### 원천소스구분 표현 존재의 가정 

문장 $x = (x_1, x_2, \cdots, x_T)$에 대한 임베딩 $e = (e_1, e_2, \cdots, e_T)$ 로부터 생성되는 심층 표현 $h = (h_1, h_2, h_T)$가 주어졌을 때, 
심층 표현은 문장에 대한 모델의 해석으로, 문장에 대한 정보를 담고있다. 이로부터 유하한 원천 소스 셋 $S = [N_s]$ 에 대한 원천 소스 추정 문제는 다음과 같이 쓰인다. 

$$
\hat{s} = \arg\max_s p(S|h)
$$

심층 표현으로부터 원천 소스를 추정하는 경우 가정은 다음과 같다. 

> <p style="font-style:normal"> (표현 포함의 가정) </p> 심층 표현은 원천소스를 구분할 특징을 내보하고 있다.

해당 가정은 문장 표현 자체가 원천소스를 추정하기 충분하다는 가정이다.

---

### 불합리한 특징의 가정 

원천소스 구분 문제에서 

1. 원천 내에서 특징이 너무 많으며, 원천들은 특징을 공유한다. 
2. 동일한 문장이 서로 다른 원천에서 자주 등장한다. 


두 원천의 고유한 특징 혹은 concept 을  $F = (f_1, f_2, \cdots, f_N) $ 과 $G = (g_1, g_2, \cdots, g_N)$ 라고 하면, 
두 원천을 구분하는 특징의 개수는 교집합을 활용하여 쓸 수 있다. 

$$
\vert F \cap G  \vert
$$

그러나 동일한 원천 내에서 $F$ 가 아예 존재하지 않을 가능성이 있다. 임의의 $f$ 특징이 $F$에 존재한다면, 원천 내에 $f$ 가 존재하지 않는 특징을 넣으면 되기 때문이다. 
예를 들어서, 사람과 강아지를 분류할 때, 뾰족한 귀의 유무라면, 귀가 찍히지 않은 강아지 샘플을 넣어서 귀라는 특징으로 분류되는 경우가 막힐 수 있다. 
원천소스 추정에서는 원천소스를 구분하는 명확한 특징들이 다른 곳에서도 나타날 수 있다. 

* 원천 A 내용 : 해리포터에 대한 설명 
* 원천 B 내용 : 해리포터를 좋아하는 이유 
* 원천 C 내용 : 해리포터 개봉일에 대한 정보  

원천소스는 두 가지 특징을 가지고 있고, 이는 Classification의 해결 가능성에 의문을 제시한다. 

1. 원천 내에 다양한 특징이 존재할 수 있다. (다중 특성 암기 필요)
2. 서로 다른 원천 소스들의 특징은 겹칠 수 있다. (분류 불가능성)

그러므로 아래 방식으로 원천소스를 추정하는 것은 잘못 정의되었을 가능성이 높다. 

$$
\hat{s} = \arg\max_s p(S|h)
$$

---

## 원천은 선택하는 것이다. 

텍스트로부터 원천에 대해서 명확하게 찾을 수 없다면, 우리가 모델링 해야 하는 부분은 텍스트와 원천의 join distribution 이다. 
$$
p(S, X)
$$ 

실용적으로 모델링 하는 것은 

원천에 대한 문장의 확률 $P(X|S)$ 및 문장에 대한 원천의 확률이다. $P(S|X)$ 따라서 명확한 문제는 classification 이 아니라 distribution matching이다. 
원천을 추정한다는 것은, 

$P(S|X) = \frac{P(X|S) P(S)}{p(X)}$를 계산하는 것이다. 



---- 

원천에 대한 선호 prior $p(S)$로부터 샘플된 원천에 대한 표현을 생성하여 원천을 입력으로 주는 방식이다. 

$$
s \sim p(S)
$$

원천에 대한 샘플링으로부터 방향 벡터 $v_s$ 를 얻으면 해당 정보는 심층 표현에 인코딩 된다.  

$$ 
p(x|h + v_s) \approx p(S|h)
$$

기존 LLM의 표현을 유지하면서 원천을 복원하면서 된다. 

$$ 
\max \ell(p(x|h + \hat{v}_s), p(x|h) ) - \alpha \ell(s, \hat{s})
$$
첫번째 텀은 원천 소스 표현을 집어 넣을지라도 단어 복원을 그대로 진행하는 경우이다. 
두 번째 텀은 추정된 원천 $\hat{s} = g(h + \hat{v}_s)$ 을 복원하는 것이다. 

해당 Objective 는 원천소스 인코딩을 진행하면서 원천소스 복원을 그대로 진행한다. 



