---
layout: share-distill
title: '[연구 3] Decision Tree & SVR'
date: 2023-10-08
giscus_comments : true
description: "First Receipt 데이터 머신러닝을 활용한 점도 예측"
authors: 
    - name: Bumjin Park
      affiliations:
        name: KAIST
img: https://drive.google.com/uc?export=view&id=16-ZXy2tI6s72_agj7pzy4PTM7IrcB4hd
---


## 1. Report Info   

본 글에서는 `first_receipt` 에 대해서 Regression 모델 두 가지를 학습하여, 성능 비교를 진행한다. 
머신러닝 모델로 학습해본 결과 단순 Decision Tree 기반 모델은 제대로 예측할 수 없음을 확인하였다. 이는 Regression 예측이 샘플에 대한 평균값을 사용하는데, 노드 수가 적기 때문에 예측의 범위가 중앙에 올려있는 경우가 많다. 이와는 반대로 입력값에 회기식을 만들어서 예측하는 Support Vector Regression 모델은 성능이 보장되는 것을 확인하였다. 코드 개수에 대해서는 코드가 늘어날수록 SVR 은 학습/테스트 데이터 모두 성능이 향상되었지만, DT 모델은 성능이 향상되지 않았고, 트리의 깊이도 더욱 증가하지 않았다. 따라서, Regression 기반 Decision Tree는 부적합한 것을 확인하였고, 더욱 복잡한 Tree 형태를 고려할 계획이다. 

한 가지 아쉬운 점은 Tree 기반 모델의 해석의 유용성을 접목하는 것이다. 이를 위해서 Classification 기반 모델에 대해서 Tree 를 적용해야 하는데, 레이블을 5000단위로 생성할 경우 20개 정도의 레이블이 생기므로 단순 Decision Tree Classification으로도 성능을 내기 어려울 것으로 판단된다. 복잡한 Tree인 Random Forest 나 XGBoost 의 형태를 고려할 계획이다. 


<div class="spanbox" markdown="1" style="width:40rem;background-color:#FFFDFD;">
1. 👨🏻‍💻 **Code Release**:  Tag [v23.10.12.2](https://github.com/fxnnxc/kolmar/tree/v23.10.12.2)
2. 📂 **Raw Data** :  (1, `raw_data:first_receipt`)
3. 🗂️ **Datasets** :  (2, `ds:first_receipt_ml`)
4. 👾 **Models** :  (0, '`decision_tree_regressor`') / (1, `svr`)
5. 🦄 **Trainers** : (1, `sklearn_regressor`)
6. 📌 **Related Notebooks** 
  * [svr_rmse.ipynb](https://github.com/fxnnxc/kolmar/blob/v23.10.12.1/labs/first_receipt_rmse/plot_rmse_svr.ipynb)
  * [dt_rmse.ipynb](https://github.com/fxnnxc/kolmar/blob/v23.10.12.1/labs/first_receipt_rmse/plot_rmse_tree.ipynb)
</div>



## 2. 학습 결과    
* 학습 모델은 두 가지 Support Vector Regressor (SVR) 과 Decision Tree Regressor (DT) 
* 예측 값은 1/10000 값으로 낮춰서 예측한다. 

## 2.1. Training RMSE (Root Mean Squared Error)

<div class="spanbox" markdown="1" style="width:100%">

* MSE : mean squared error 
* RMSE : root mean squared error 

$$ 
MSE = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2
$$

$$ 
RMSE = \sqrt{\frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2}
$$

아래 그림은 입력에 사용된 코드 개수에 따라서 성능을 보여준다. (좌: SVR, 우:DT)

<img src="https://drive.google.com/uc?export=view&id=10-vwD_CzikpZFPvHji2Qs-EP-m8jb5TY" style='width:49%'>
<img src="https://drive.google.com/uc?export=view&id=1vbVyGzec2w298efKHKNzHVmOace7Wtdl" style='width:49%'>
</div>

--- 

## 2.2. Parameter Search

<div class="spanbox" markdown="1" style="width:100%">


여러 파라미터 중에서 최적의 파라미터를 찾는다. 

### 2.2.1 Decision Tree 

<d-code language='python'>
param_grid = {
                    "splitter":["best","random"],
                    'max_depth': [1,2,3,5,7,9],
                    'min_samples_split': [2, 3, 4, 5, 10],
                    "min_weight_fraction_leaf":[0.1, 0.3, 0.5],
                    "max_features":["log2", "sqrt"],
                    "max_leaf_nodes":[None,10, 20, 30, 50, 100]
                    }
</d-code>

* Decision Tree : 'max_depth'

<img src="https://drive.google.com/uc?export=view&id=1rbjPF06wY8xkaIQfF2l-qW2XByNETeLp" style='width:100%'>


#### 예시) 코드 100 개 모델 

학습 완료 후, 학습된 모델의 정보를 보면 다음과 같다. 

<d-code language='python'>

best_params:
  max_depth: 7
  max_features: sqrt
  max_leaf_nodes: null
  min_samples_split: 2
  min_weight_fraction_leaf: 0.1
  splitter: random
train_results:
  rmse: 1.2203531353731971
test_results:
  rmse: 1.2753255313844698

|--- GYH026 <= 4.52
|   |--- HJK244 <= 0.55
|   |   |--- FGH202 <= 2.84
|   |   |   |--- CVB114 <= 0.57
|   |   |   |   |--- XCV072 <= 3.86
|   |   |   |   |   |--- value: [5.10]
|   |   |   |   |--- XCV072 >  3.86
|   |   |   |   |   |--- value: [5.46]
|   |   |   |--- CVB114 >  0.57
|   |   |   |   |--- value: [4.88]
|   |   |--- FGH202 >  2.84
|   |   |   |--- value: [4.76]
|   |--- HJK244 >  0.55
|   |   |--- value: [4.09]
|--- GYH026 >  4.52
|   |--- value: [5.87]

</d-code>

---

<img src="https://drive.google.com/uc?export=view&id=1mtE_2yeXeac1vUJF_b75SSPEGDPoX0Tv" style='width:100%'>


**단순 Tree 모델은 학습이 거의 되지 않는 것을 확인할 수 있다.**


---

### 2.2.2  SVR : C, kernel type, degree 


<d-code language='python'>
param_grid = {
                'kernel' : ['linear', 'poly', 'rbf', 'sigmoid', ],
                'degree' : [1,2,3,4,5,6],
                'C' : [0.5, 1.0, 2.0, 3.0],
                'epsilon' : [0.1, 0.2, 0.3]
            }
</d-code>


<img src="https://drive.google.com/uc?export=view&id=1raHjdlTfaSxx5b7LWsyHLUNXJsl6_ez6" style='width:100%'>

</div>

--- 

## 2.3 Error (prediction - target)

<div class="spanbox" markdown="1" style="width:100%">



###  2.3.1 DT
<img src="https://drive.google.com/uc?export=view&id=1KGjhEqhIJ_k6uZolp-PXGqMGnLyKtRoa" style='width:49%'>
<img src="https://drive.google.com/uc?export=view&id=1MZ1OGvXjHF3U5-LLWZCFA45MB9tYxMlq" style='width:49%'>


### 2.3.2 SVR
<img src="https://drive.google.com/uc?export=view&id=1VOgfS065VeZyBa516uOD3rVlzIC9Z4dD" style='width:49%'>
<img src="https://drive.google.com/uc?export=view&id=1EawHk8Xyg5aObOJnA1umyzjod0pKBSxl" style='width:49%'>

</div>



---

## 3. 학습 데이터 구성 

1. **0값 코드** : 기존 존재하지 않는 코드에 대체할 수 있는 값은 (0, 평균, 중앙값) 등이 있는데, 정도 예측 모델은 모든 코드의 값이 존재하며, 다만 그 값이 0인 경우와 동일하므로 최종적으로 0을 집어넣는 것으로 처리한다. 
2. **정규화**: 기존 값 입력을 그대로 사용해야 하므로, 값을 정규화하지 않는다. 
3. **이상치 제거**: 물질의 양을 그대로 나타내므로 이상치를 제거하지 않는다. 
4. 예측값: 기존 10000~80000 범위의 값을 1/10000로 줄여서 예측. 

<div class="spanbox" markdown="1" style="width:100%">
### 3.1 예측값 분포 

<img src="https://drive.google.com/uc?export=view&id=16-ZXy2tI6s72_agj7pzy4PTM7IrcB4hd" style='width:100%'>
</div>

<div class="spanbox" markdown="1" style="width:100%">

### 3.2 실제 데이터 분포 / 학습데이터 분포 (0 값 포함)

<img src="https://drive.google.com/uc?export=view&id=13cSTKqQQ6W4wb1Mx0PcNAWuwJa0wkrvr" style='width:49%'>
<img src="https://drive.google.com/uc?export=view&id=1L_JVd1oasKDdVA8s23tNF8AsYndP3SpI" style='width:49%'>

</div>



## Appendix: SVR 설명 

* $e^{-\gamma (a-b)^2} $
 * $\gamma$ 가 클수록, 점들이 가까울수록 더 적은 값으로 된다. 
 * (a,b) 사이가 멀수록 값이 작다. (가까울수록 크다.) 
 * [video](https://www.youtube.com/watch?v=Qc5IyLW_hns)
 * after modifying the data distribution, the linear support vectors are fitted. 



