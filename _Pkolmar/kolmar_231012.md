---
layout: share-distill
title: '[Kolmar] Decision Tree & SVR'
date: 2023-10-08
giscus_comments : true
description: "머신러닝을 활용한 점도 예측"
authors: 
    - name: Bumjin Park
      affiliations:
        name: KAIST

---


## 1. Report Info   

본 글에서는 `first_receipt` 에 대해서 Regression 모델 두 가지를 학습하여, 성능 비교를 진행한다. 

* 👨🏻‍💻 Code Release Tag [v23.10.12.1](https://github.com/fxnnxc/kolmar/tree/v23.10.12.1)
* 📂 Raw Data :  (1, `raw_data:first_receipt`)
* 🗂️ Datasets :  (2, `ds:first_receipt_ml`)
* 👾 Models :  (0, '`decision_tree_regressor`') / (1, `svr`)
* 🦄 Trainers : (1, `sklearn_regressor`)
* 📌 Related Notebook 
  * [svr_rmse.ipynb](https://github.com/fxnnxc/kolmar/blob/v23.10.12.1/labs/first_receipt_rmse/plot_rmse_svr.ipynb)
  * [dt_rmse.ipynb](https://github.com/fxnnxc/kolmar/blob/v23.10.12.1/labs/first_receipt_rmse/plot_rmse_tree.ipynb)



## 2. 학습 결과    
* 학습 모델은 두 가지 Support Vector Regressor (SVR) 과 Decision Tree Regressor (DT) 
* 예측 값은 1/10000 값으로 낮춰서 예측한다. 

## 2.1. Training RMSE (Root Mean Squared Error)

<div class="spanbox" markdown="1" style="width:60rem">

* MSE : mean squared error 
* RMSE : root mean squared error 

$$ 
MSE = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2
$$

$$ 
RMSE = \sqrt{\frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2}
$$

아래 그림은 입력에 사용된 코드 개수에 따라서 성능을 보여준다. (좌: SVR, 우:DT)

<img src="https://drive.google.com/uc?export=view&id=10-vwD_CzikpZFPvHji2Qs-EP-m8jb5TY" style='width:49%'>
<img src="https://drive.google.com/uc?export=view&id=1vbVyGzec2w298efKHKNzHVmOace7Wtdl" style='width:49%'>
</div>

--- 

## 2.2. Parameter Search

<div class="spanbox" markdown="1" style="width:60rem">


여러 파라미터 중에서 최적의 파라미터를 찾는다. 

### 2.2.1 Decision Tree 

<d-code language='python'>
param_grid = {
                    "splitter":["best","random"],
                    'max_depth': [1,2,3,5,7,9],
                    'min_samples_split': [2, 3, 4, 5, 10],
                    "min_weight_fraction_leaf":[0.1, 0.3, 0.5],
                    "max_features":["log2", "sqrt"],
                    "max_leaf_nodes":[None,10, 20, 30, 50, 100]
                    }
</d-code>

* Decision Tree : 'max_depth'

<img src="https://drive.google.com/uc?export=view&id=1rbjPF06wY8xkaIQfF2l-qW2XByNETeLp" style='width:100%'>


#### 예시) 코드 100 개 모델 

학습 완료 후, 학습된 모델의 정보를 보면 다음과 같다. 

<d-code language='python'>

best_params:
  max_depth: 7
  max_features: sqrt
  max_leaf_nodes: null
  min_samples_split: 2
  min_weight_fraction_leaf: 0.1
  splitter: random
train_results:
  rmse: 1.2203531353731971
test_results:
  rmse: 1.2753255313844698

|--- GYH026 <= 4.52
|   |--- HJK244 <= 0.55
|   |   |--- FGH202 <= 2.84
|   |   |   |--- CVB114 <= 0.57
|   |   |   |   |--- XCV072 <= 3.86
|   |   |   |   |   |--- value: [5.10]
|   |   |   |   |--- XCV072 >  3.86
|   |   |   |   |   |--- value: [5.46]
|   |   |   |--- CVB114 >  0.57
|   |   |   |   |--- value: [4.88]
|   |   |--- FGH202 >  2.84
|   |   |   |--- value: [4.76]
|   |--- HJK244 >  0.55
|   |   |--- value: [4.09]
|--- GYH026 >  4.52
|   |--- value: [5.87]

</d-code>

---

<img src="https://drive.google.com/uc?export=view&id=1mtE_2yeXeac1vUJF_b75SSPEGDPoX0Tv" style='width:100%'>


**단순 Tree 모델은 학습이 거의 되지 않는 것을 확인할 수 있다.**


---

### 2.2.2  SVR : C, kernel type, degree 


<d-code language='python'>
param_grid = {
                'kernel' : ['linear', 'poly', 'rbf', 'sigmoid', ],
                'degree' : [1,2,3,4,5,6],
                'C' : [0.5, 1.0, 2.0, 3.0],
                'epsilon' : [0.1, 0.2, 0.3]
            }
</d-code>


<img src="https://drive.google.com/uc?export=view&id=1raHjdlTfaSxx5b7LWsyHLUNXJsl6_ez6" style='width:100%'>

</div>

--- 

## 2.3 Error (prediction - target)

<div class="spanbox" markdown="1" style="width:60rem">



###  2.3.1 DT
<img src="https://drive.google.com/uc?export=view&id=1KGjhEqhIJ_k6uZolp-PXGqMGnLyKtRoa" style='width:49%'>
<img src="https://drive.google.com/uc?export=view&id=1MZ1OGvXjHF3U5-LLWZCFA45MB9tYxMlq" style='width:49%'>


### 2.3.2 SVR
<img src="https://drive.google.com/uc?export=view&id=1VOgfS065VeZyBa516uOD3rVlzIC9Z4dD" style='width:49%'>
<img src="https://drive.google.com/uc?export=view&id=1EawHk8Xyg5aObOJnA1umyzjod0pKBSxl" style='width:49%'>

</div>


---

## 3. 학습 데이터 구성 

1. **0값 코드** : 기존 존재하지 않는 코드에 대체할 수 있는 값은 (0, 평균, 중앙값) 등이 있는데, 정도 예측 모델은 모든 코드의 값이 존재하며, 다만 그 값이 0인 경우와 동일하므로 최종적으로 0을 집어넣는 것으로 처리한다. 
2. **정규화**: 기존 값 입력을 그대로 사용해야 하므로, 값을 정규화하지 않는다. 
3. **이상치 제거**: 물질의 양을 그대로 나타내므로 이상치를 제거하지 않는다. 
4. 예측값: 기존 10000~80000 범위의 값을 1/10000로 줄여서 예측. 

<div class="spanbox" markdown="1" style="width:40rem">
### 3.1 예측값 분포 

<img src="https://drive.google.com/uc?export=view&id=16-ZXy2tI6s72_agj7pzy4PTM7IrcB4hd" style='width:100%'>
</div>

<div class="spanbox" markdown="1" style="width:60rem">

### 3.2 실제 데이터 분포 / 학습데이터 분포 (0 값 포함)


<img src="https://drive.google.com/uc?export=view&id=1qKn1LcbM5-jmg-DEGa0qp8z1anyGEdmy" style='width:49%'>
<img src="https://drive.google.com/uc?export=view&id=1bJoNoChB_oGJa_9d20ZA8-SCNlKXKbx4" style='width:49%'>

</div>


