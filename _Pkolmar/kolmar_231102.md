---
layout: share-distill
title: '[연구 6] Feature Based Materiality Prediction'
date: 2023-10-27
giscus_comments : true
description: "물성에 대한 예측 수식적 모델링"
authors: 
    - name: Bumjin Park
      affiliations:
        name: KAIST

img: https://drive.google.com/uc?export=view&id=1coZdkpkxr83eALyFK1bMsYLfKYrUi1U8
---


## 소개 

여러 원료를 섞는 경우, 처방이라고 불리는 복잡한 상태의 혼합물이 만들어진다. 처방은 여러 원료들을 특정 비율로 섞은 물질로 고유의 기능과 물성을 가지고 있다. 대표적으로 화장품은 여러 원료들을 한데 섞어서 기능성을 띄고 사용감에 대한 물성을 가진다. 처방이라고 불리는 물질에 대한 기능과 물성은 물질을 혼합하는데 숙련된 연구자라면 감각적으로 예측할 수 있다. 그러나, 이를 위해서는 수많은 물질들을 다뤄보고 각 원료가 지니는 어떤 고유한 특징을 감각적으로 배워야 하며, 마지막으로 배합의 결과를 예측할 수 있어야 한다. 
 
물성 학습 과정에서 그들을 섞는 것에 대한 감각적인 인식을 이해하고 모델링 할 수 있다면, 감각적이던 물질 혼합에 대한 직감을 수식화하며 모델링 할 수 있고, 이를 활용하여 조합적으로 에측 가능한 수많은 처방들을 시도해볼 수 있다. 이 모델로부터 얻을 수 있는 효과는 다음과 같다. 원하는 목표 물성을 달성하기 위해 혼합물의 비율을 엮으로 조정할 수 있으며, 동일한 기능성과 물성을 가지는 화장품에 대해서 구성 원료를 바꿈으로써 비용감축을 이뤄낼 수 있다. 

따라서 감각적으로 모델링 하는 기능과 물성을 측정할 수 있는 모델이 있고, 그 모델이 정확하다면, 수많은 경우의 수를 바탕으로 실험을 했던 과거와 다르게 수많은 조합을 시험해볼 수 있으므로  얻을 수 있는 경제적 이익은 무궁무진하다. 


모든 원료에 대한 특성들을 $f_1, f_2, f_3, \cdots $ 라고 하자…. 이들은 모든 원료에 대해서 우리가 측정할 수 있는, 혹은 표현할 수 있는 모든 객관적인 특성을 나타낸다. 즉, 물질이 지니는 고유의 특성이며 이는 수치적으로 0~1 값으로 표현할 수 있다.

예를 들어서, 점성이 0~100,000 범위로 측정된다면, 1/100000 으로 낮출 경우 점성을 0~1로 나타낼 수 있다. 
비슷하게, Ph, 점성과 탄성이 존재하여, 세 값이 [0.5, 0.7, 0.8] 으로 상태를 표현할 수 있다. 
이 표현 벡터를 feature vector (FV) 라고 부르자. 
$$ 
F = [F_1, F_2, \cdots]
$$


### 아키텍처의 가정 
1. 모든 물질은 특정 양 A가 주어지면 feature vector $F_A$ 로 나타낼 수 있다
2. 모든 물질은 feature vector 로부터 물성은 결정된다. 

1번은 물질의 양이 다를 경우 특성이 다를 수 있음을 가정한다. 그러나 이 경우에도 특정 양에 대해서 유일한 feature vector 로 표현이 가능하다는 가정이다. 2번에 의해서 물성을 예측하는데 있어서 물질의 feature vector로부터 유도될 수 있음을 나타낸다. 

* 만일 가정 1이 없다면, 물질의 특성을 나타내는 표현 자체가 불가능하거나, 특성 자체가 확률성을 띄는 것이다. 예를 들어서 온도, 시간에 따라서 특성이 변하는 경우에 해당한다. 따라서 고정된 환경에서 물질의 특성에 변화가 없다는 가정이다. 
* 이는 물질의 특징을 feature vector로 표현한 정보로 특정 물성이 결정난다는 결정론이다. 해당 가정이 없다면, 예측 모델링이 불가능하거나 측정될 수 있는 feature FA가 아닌 다른 것들로 물성이 결정난다. 


### 혼합물 모델링 
여러 물질을 섞어서 혼합물을 만드는 경우, 물질의 양과 feature vector 의 관계를 유도해야 한다. 
이를 위해서 물질의 양에 상관없이 FA 가 결정난다고 가정하고, 여러 물질을 혼합하는 경우 feature vector 의 형태로 선형적으로 더해진다는 가정을 하자. 

3. 특정 물질은 충분한 양이 있다면, 양에 상관없는 feature vector를 지닌다. 
4. 여러 원료를 혼합시 feature vector는 넣는 비율만큼 선형적으로 증가한다.

예시는 
두 물질에 대한 고정된 FV들이 존재하고
물의 FV : [0.3, 0.5, 0.8]
꿀의 FV : [0.7, 0.2, 0.3]

물 70 과 꿀 30을 섞는 경우, 비율 0.7 과 0.3에 의해서 혼합물의 FV는 다음과 같이 결정난다. 

$$
FV = 0.7 \cdot [0.3, 0.5, 0.8] + 0.3 \cdot  [0.7, 0.2, 0.3] = [0.42, 0.41, 0.65]
$$

* 만일 가정 3이 없다면, 물질의 양에 대해서 서로 다른 특성을 가정해야 하므로 모델링이 더욱 복잡해진다. 
* 만일 가정 4가 없다면, 비율에 대해서 선형적으로 특성이 증가하는 것이 아닌, 다른 방식으로 더해진다. 


### 물성 예측 모델링 



만일 원료를 넣는 양*물성 특징으로 특징을 만드는데 기여한다고 가정하면 다음과 같은 식이 만들어진다. 

Gg(x) = p * A

Gg 함수가 반드시 해당 형태일 필요는 없다. 

원료들의 비선형절인 결합이라면 다음과 같이 쓸 수 있고, 대신 이 경우는 f에 대해서 softmax 로 normalize 를 해야 한다.

<center>
<img src="https://drive.google.com/uc?export=view&id=1coZdkpkxr83eALyFK1bMsYLfKYrUi1U8" style='width:50%;border:1px solid #FFDDDD;'>
</center>




## 딥러닝 기반 특성 학습 


<center>
<img src="https://drive.google.com/uc?export=view&id=1G6vwUnhroT9NshPX-uspkQFEK8GhiqVX" style='width:40%'>
</center>



### 비선형 함수의 결합


<img src="https://drive.google.com/uc?export=view&id=16bC2ZxkRl587IsP1gTIe9WjO8Pk2t5-j" style='width:100%;border:1px solid #FFDDDD;'>



## 학습 결과 


* 데이터 : 1차 + 2차 처방 데이터  (샘플수 1745개 / 코드 개수 5485개)
* **학습데이터** (1221, 5485), (1221,) 
* **테스트데이터** (524, 5485), (524,)


ACT | HIDDEN_DIM | N_ENTRIES | NUM_LAYERS | OPTIM |  (Eval+Train)/2 | Eval | Train 
|:-:|:-:|:-:|:-:|:-: |---|---|---|
sigmoid | 128 | 100 | 4 | adam | **3406** | **5535** | **1277**
sigmoid | 128 | 50 | 4 | adam | **3606** | **5877** | **1335**
sigmoid | 256 | 100 | 2 | adam | **3630** | **6028** | **1232**
sigmoid | 128 | 100 | 2 | adam | **3821** | **6217** | **1425**
tanh | 128 | 50 | 4 | adam | **14972** | **15045** | **14900**
tanh | 128 | 10 | 6 | adam | **14974** | **15047** | **14901**
relu | 128 | 50 | 2 | adam | **14991** | **15094** | **14888**
relu | 256 | 50 | 6 | adam | **20708** | **20931** | **20485**



### 8개 학습 모델에 대한 학습 커브 

위 : 테스트 데이터
아래 : 테스트 데이터

<div class="spanbox" markdown="1" style="width:100%;background-color:#FFFDFD;margin-left:0rem;">

#### Top 5 Model Training Curves

<img src="https://drive.google.com/uc?export=view&id=14FUmw8bGBah6EVQDB6Q7uAaXiLAcD4zV" style='width:100%'>

#### Bottom 5 Model Training Curves

<img src="https://drive.google.com/uc?export=view&id=1Rz3Xqj9wTYDyccLh9G0SGC3QsQU7lOgL" style='width:100%'>

</div>


### Hyperparameter Test

<div class="spanbox" markdown="1" style="width:60rem;background-color:#FFFDFD;margin-left:-5rem;">
<center>
<img src="https://drive.google.com/uc?export=view&id=1YiixXR5uBuWe28shhZ1Q9FbANdlVrZTy" style='width:100%'>
</center>
</div>



### 전체 모델 성능

테스트와 학습 에러에 대해서 평균낸 점수를 기준으로 정렬하였다. 



ACT | HIDDEN_DIM | N_ENTRIES | NUM_LAYERS | OPTIM |  (Eval+Train)/2 | Eval | Train 
|:-:|:-:|:-:|:-:|:-: |---|---|---|
sigmoid | 128 | 100 | 4 | adam | **3406** | **5535** | **1277**
sigmoid | 128 | 50 | 4 | adam | **3606** | **5877** | **1335**
sigmoid | 256 | 100 | 2 | adam | **3630** | **6028** | **1232**
sigmoid | 128 | 100 | 2 | adam | **3821** | **6217** | **1425**
sigmoid | 256 | 50 | 2 | adam | **4049** | **6392** | **1706**
sigmoid | 128 | 10 | 4 | adam | **4348** | **6118** | **2578**
sigmoid | 128 | 10 | 2 | adam | **4511** | **6598** | **2424**
sigmoid | 256 | 10 | 2 | adam | **4543** | **6344** | **2741**
sigmoid | 128 | 50 | 2 | adam | **5472** | **7177** | **3768**
tanh | 256 | 10 | 6 | sgd | **6386** | **7177** | **5595**
tanh | 128 | 10 | 4 | sgd | **6575** | **7394** | **5756**
tanh | 256 | 10 | 4 | sgd | **6590** | **7302** | **5879**
tanh | 128 | 10 | 6 | sgd | **6592** | **7298** | **5885**
tanh | 256 | 10 | 2 | sgd | **6651** | **7370** | **5932**
tanh | 128 | 10 | 2 | sgd | **6672** | **7338** | **6005**
relu | 256 | 100 | 4 | adam | **7376** | **8941** | **5811**
tanh | 256 | 100 | 6 | sgd | **7379** | **8009** | **6748**
tanh | 128 | 50 | 6 | sgd | **7423** | **7897** | **6949**
relu | 128 | 100 | 6 | adam | **7487** | **8618** | **6355**
tanh | 128 | 50 | 4 | sgd | **7530** | **8013** | **7047**
relu | 128 | 100 | 4 | adam | **7582** | **8692** | **6473**
tanh | 256 | 50 | 4 | sgd | **7625** | **8141** | **7109**
relu | 256 | 10 | 4 | adam | **7681** | **9456** | **5907**
relu | 128 | 10 | 2 | adam | **7938** | **9357** | **6519**
relu | 256 | 50 | 2 | sgd | **7951** | **8360** | **7542**
relu | 256 | 10 | 4 | sgd | **8023** | **8613** | **7432**
tanh | 256 | 50 | 2 | sgd | **8067** | **8603** | **7531**
relu | 256 | 50 | 2 | adam | **8142** | **9337** | **6946**
relu | 256 | 100 | 2 | sgd | **8144** | **8540** | **7749**
relu | 128 | 10 | 6 | sgd | **8278** | **8919** | **7637**
relu | 256 | 10 | 6 | sgd | **8299** | **8902** | **7696**
tanh | 256 | 50 | 6 | sgd | **8320** | **8731** | **7910**
relu | 256 | 50 | 4 | adam | **8351** | **9808** | **6893**
relu | 256 | 100 | 4 | sgd | **8527** | **8821** | **8233**
tanh | 128 | 100 | 6 | sgd | **8547** | **8988** | **8105**
sigmoid | 256 | 10 | 2 | sgd | **8583** | **9100** | **8065**
tanh | 128 | 100 | 2 | sgd | **8665** | **9098** | **8232**
relu | 128 | 50 | 2 | sgd | **8746** | **9134** | **8358**
relu | 256 | 50 | 4 | sgd | **8784** | **9171** | **8397**
tanh | 128 | 100 | 4 | sgd | **8861** | **9216** | **8507**
relu | 128 | 10 | 4 | sgd | **8889** | **9325** | **8454**
tanh | 128 | 50 | 2 | sgd | **8901** | **9273** | **8528**
relu | 128 | 50 | 4 | sgd | **8963** | **9348** | **8579**
relu | 128 | 10 | 2 | sgd | **9009** | **9472** | **8547**
tanh | 256 | 100 | 4 | sgd | **9051** | **9393** | **8709**
sigmoid | 128 | 10 | 4 | sgd | **9074** | **9562** | **8587**
relu | 128 | 50 | 6 | sgd | **9108** | **9534** | **8683**
relu | 128 | 100 | 2 | sgd | **9303** | **9661** | **8945**
relu | 256 | 50 | 6 | sgd | **9358** | **9725** | **8990**
sigmoid | 256 | 10 | 4 | sgd | **9358** | **9832** | **8884**
relu | 128 | 100 | 4 | sgd | **9402** | **9667** | **9138**
relu | 256 | 100 | 6 | sgd | **9407** | **9682** | **9133**
relu | 128 | 100 | 6 | sgd | **9422** | **9693** | **9151**
sigmoid | 128 | 50 | 2 | sgd | **9866** | **10254** | **9478**
sigmoid | 128 | 100 | 2 | sgd | **9947** | **10320** | **9574**
sigmoid | 256 | 50 | 2 | sgd | **9952** | **10302** | **9601**
sigmoid | 256 | 100 | 2 | sgd | **9990** | **10326** | **9654**
tanh | 256 | 100 | 2 | sgd | **10204** | **10539** | **9869**
sigmoid | 128 | 50 | 4 | sgd | **10355** | **10667** | **10043**
relu | 128 | 10 | 6 | adam | **10483** | **11235** | **9731**
sigmoid | 128 | 100 | 4 | sgd | **11127** | **11292** | **10961**
relu | 128 | 50 | 6 | adam | **12026** | **12344** | **11708**
relu | 128 | 100 | 2 | adam | **12098** | **13506** | **10691**
sigmoid | 256 | 50 | 4 | sgd | **12943** | **12937** | **12949**
relu | 128 | 10 | 4 | adam | **14390** | **17277** | **11503**
relu | 256 | 100 | 6 | adam | **14807** | **15482** | **14133**
sigmoid | 256 | 100 | 4 | sgd | **14934** | **14991** | **14877**
sigmoid | 128 | 10 | 6 | sgd | **14942** | **14998** | **14885**
sigmoid | 128 | 50 | 6 | sgd | **14944** | **15000** | **14887**
relu | 256 | 10 | 2 | sgd | **14944** | **15000** | **14887**
relu | 256 | 10 | 2 | adam | **14944** | **15000** | **14888**
sigmoid | 256 | 10 | 6 | sgd | **14944** | **15000** | **14887**
sigmoid | 128 | 100 | 6 | adam | **14944** | **15000** | **14888**
relu | 256 | 10 | 6 | adam | **14944** | **15000** | **14888**
relu | 256 | 100 | 2 | adam | **14944** | **15000** | **14888**
sigmoid | 256 | 10 | 6 | adam | **14944** | **15000** | **14888**
tanh | 128 | 10 | 2 | adam | **14944** | **15000** | **14888**
sigmoid | 256 | 100 | 6 | sgd | **14944** | **15000** | **14888**
relu | 128 | 50 | 4 | adam | **14944** | **15000** | **14888**
sigmoid | 128 | 10 | 6 | adam | **14944** | **15000** | **14888**
tanh | 128 | 100 | 4 | adam | **14944** | **15000** | **14888**
sigmoid | 256 | 50 | 6 | adam | **14944** | **15000** | **14888**
sigmoid | 128 | 100 | 6 | sgd | **14944** | **15000** | **14888**
sigmoid | 256 | 100 | 4 | adam | **14944** | **15000** | **14888**
tanh | 128 | 50 | 6 | adam | **14944** | **15000** | **14888**
sigmoid | 256 | 100 | 6 | adam | **14944** | **15001** | **14888**
sigmoid | 256 | 50 | 6 | sgd | **14944** | **15001** | **14888**
sigmoid | 128 | 50 | 6 | adam | **14944** | **15001** | **14888**
sigmoid | 256 | 10 | 4 | adam | **14944** | **15001** | **14888**
tanh | 128 | 100 | 2 | adam | **14944** | **15002** | **14887**
tanh | 256 | 10 | 4 | adam | **14945** | **15002** | **14888**
tanh | 256 | 100 | 4 | adam | **14946** | **15003** | **14888**
tanh | 128 | 10 | 4 | adam | **14946** | **15003** | **14888**
sigmoid | 256 | 50 | 4 | adam | **14946** | **15003** | **14888**
tanh | 256 | 10 | 6 | adam | **14946** | **15004** | **14888**
tanh | 256 | 100 | 6 | adam | **14948** | **15006** | **14889**
tanh | 256 | 100 | 2 | adam | **14948** | **15008** | **14888**
tanh | 256 | 50 | 4 | adam | **14948** | **15007** | **14889**
tanh | 128 | 50 | 2 | adam | **14950** | **15010** | **14890**
tanh | 256 | 50 | 2 | adam | **14955** | **15018** | **14891**
tanh | 256 | 10 | 2 | adam | **14963** | **15030** | **14897**
tanh | 128 | 100 | 6 | adam | **14965** | **15037** | **14894**
tanh | 256 | 50 | 6 | adam | **14966** | **15036** | **14896**
tanh | 128 | 50 | 4 | adam | **14972** | **15045** | **14900**
tanh | 128 | 10 | 6 | adam | **14974** | **15047** | **14901**
relu | 128 | 50 | 2 | adam | **14991** | **15094** | **14888**
relu | 256 | 50 | 6 | adam | **20708** | **20931** | **20485**