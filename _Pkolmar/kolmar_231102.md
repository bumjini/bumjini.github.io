---
layout: share-distill
title: '[연구 6] Feature Based Materiality Prediction'
date: 2023-11-02
giscus_comments : true
description: "물성에 대한 예측 수식적 모델링"
authors: 
    - name: Bumjin Park
      affiliations:
        name: KAIST

img: https://drive.google.com/uc?export=view&id=1coZdkpkxr83eALyFK1bMsYLfKYrUi1U8
---

##  Report Info   

<div class="spanbox" markdown="1" style="width:40rem;background-color:#FFFDFD;">
1. 👨🏻‍💻 **Code Release**:  Tag [v23.11.01.1](https://github.com/fxnnxc/kolmar/tree/v23.11.01.1)
2. 📂 **Raw Data** :  (`raw_data:receipt_3`)   <br> # 3.1 은 1차 및 2처 처방을 합치고 코드별로 합산한 데이터
3. 🗂️ **Datasets** :  (`ds:receipt_3_dl`)  
4. 👾 **Models** :  MLP 
5. 🦄 **Trainers** : (1, `mlp_regressor`)
6. 📌 **Related Notebooks** 
  * [train_rmse.ipynb](https://github.com/fxnnxc/kolmar/blob/v23.11.01.1/labs/receipt_3_mlp/train_rmse.ipynb)

</div>



## 소개 

여러 원료를 섞는 경우, 처방이라고 불리는 복잡한 상태의 혼합물이 만들어진다. 처방은 여러 원료들을 특정 비율로 섞은 물질로 고유의 기능과 물성을 가지고 있다. 대표적으로 화장품은 여러 원료들을 한데 섞어서 기능성을 띄고 사용감에 대한 물성을 가진다. 처방이라고 불리는 물질에 대한 기능과 물성은 물질을 혼합하는데 숙련된 연구자라면 감각적으로 예측할 수 있다. 그러나, 이를 위해서는 수많은 물질들을 다뤄보고 각 원료가 지니는 어떤 고유한 특징을 감각적으로 배워야 하며, 마지막으로 배합의 결과를 예측할 수 있어야 한다. 
 
물성 학습 과정에서 그들을 섞는 것에 대한 감각적인 인식을 이해하고 모델링 할 수 있다면, 감각적이던 물질 혼합에 대한 직감을 수식화하며 모델링 할 수 있고, 이를 활용하여 조합적으로 에측 가능한 수많은 처방들을 시도해볼 수 있다. 이 모델로부터 얻을 수 있는 효과는 다음과 같다. 원하는 목표 물성을 달성하기 위해 혼합물의 비율을 엮으로 조정할 수 있으며, 동일한 기능성과 물성을 가지는 화장품에 대해서 구성 원료를 바꿈으로써 비용감축을 이뤄낼 수 있다. 

따라서 감각적으로 모델링 하는 기능과 물성을 측정할 수 있는 모델이 있고, 그 모델이 정확하다면, 수많은 경우의 수를 바탕으로 실험을 했던 과거와 다르게 수많은 조합을 시험해볼 수 있으므로  얻을 수 있는 경제적 이익은 무궁무진하다. 


모든 원료에 대한 특성들을 $f_1, f_2, f_3, \cdots $ 라고 하자…. 이들은 모든 원료에 대해서 우리가 측정할 수 있는, 혹은 표현할 수 있는 모든 객관적인 특성을 나타낸다. 즉, 물질이 지니는 고유의 특성이며 이는 수치적으로 0~1 값으로 표현할 수 있다.

예를 들어서, 점성이 0~100,000 범위로 측정된다면, 1/100000 으로 낮출 경우 점성을 0~1로 나타낼 수 있다. 
비슷하게, Ph, 점성과 탄성이 존재하여, 세 값이 [0.5, 0.7, 0.8] 으로 상태를 표현할 수 있다. 
이 표현 벡터를 feature vector (FV) 라고 부르자. 
$$ 
F = [F_1, F_2, \cdots]
$$


### 아키텍처의 가정 
1. 모든 물질은 특정 양 A가 주어지면 feature vector $F_A$ 로 나타낼 수 있다
2. 모든 물질은 feature vector 로부터 물성은 결정된다. 

1번은 물질의 양이 다를 경우 특성이 다를 수 있음을 가정한다. 그러나 이 경우에도 특정 양에 대해서 유일한 feature vector 로 표현이 가능하다는 가정이다. 2번에 의해서 물성을 예측하는데 있어서 물질의 feature vector로부터 유도될 수 있음을 나타낸다. 

* 만일 가정 1이 없다면, 물질의 특성을 나타내는 표현 자체가 불가능하거나, 특성 자체가 확률성을 띄는 것이다. 예를 들어서 온도, 시간에 따라서 특성이 변하는 경우에 해당한다. 따라서 고정된 환경에서 물질의 특성에 변화가 없다는 가정이다. 
* 이는 물질의 특징을 feature vector로 표현한 정보로 특정 물성이 결정난다는 결정론이다. 해당 가정이 없다면, 예측 모델링이 불가능하거나 측정될 수 있는 feature FA가 아닌 다른 것들로 물성이 결정난다. 

따라서 다음과 같은 형태의 예측이 진행된다. 

<center>
<img src="https://drive.google.com/uc?export=view&id=1coZdkpkxr83eALyFK1bMsYLfKYrUi1U8" style='width:50%;border:1px solid #FFDDDD;'>
</center>





### 혼합물 모델링 
여러 물질을 섞어서 혼합물을 만드는 경우, 물질의 양과 feature vector 의 관계를 유도해야 한다. 
이를 위해서 물질의 양에 상관없이 FA 가 결정난다고 가정하고, 여러 물질을 혼합하는 경우 feature vector 의 형태로 선형적으로 더해진다는 가정을 하자. 

3. 특정 물질은 충분한 양이 있다면, 양에 상관없는 feature vector를 지닌다. 
4. 여러 원료를 혼합시 feature vector는 넣는 비율만큼 선형적으로 증가한다.

예시는 
두 물질에 대한 고정된 FV들이 존재하고
물의 FV : [0.3, 0.5, 0.8]
꿀의 FV : [0.7, 0.2, 0.3]

물 70 과 꿀 30을 섞는 경우, 비율 0.7 과 0.3에 의해서 혼합물의 FV는 다음과 같이 결정난다. 

$$
FV = 0.7 \cdot [0.3, 0.5, 0.8] + 0.3 \cdot  [0.7, 0.2, 0.3] = [0.42, 0.41, 0.65]
$$

* 만일 가정 3이 없다면, 물질의 양에 대해서 서로 다른 특성을 가정해야 하므로 모델링이 더욱 복잡해진다. 
* 만일 가정 4가 없다면, 비율에 대해서 선형적으로 특성이 증가하는 것이 아닌, 다른 방식으로 더해진다. 




## 딥러닝 기반 특성 학습 

**(딥러닝 기반 모델링은 위에서 설명한 혼합물 모델링이 아닌, 단순 원료의 양을 맵핑하여 학습한 결과이다.)**


딥러닝 모델은 입력에 대해서 무수히 많은 뉴런들로 입력값을 처리하고, 최종 출력을 내놓는 형태이다. 특징은 크게

* 레이어의 개수  : 병렬적인 노드가 있는 레이어의 수. 보통 2~6개를 사용한다. 
* 레이어의 노드 수 : hidden_dim 이라고도 불리며, 뉴런의 숫자이다. 보통 128~1024 개를 사용한다.


아래 그림은 입력의 차원이 3, 출력의 차원이 1인 모델이다. 레어어는 3개이며, 중간 레이어는 3개의 노드를 가지고 있다. 

<center>
<img src="https://drive.google.com/uc?export=view&id=1G6vwUnhroT9NshPX-uspkQFEK8GhiqVX" style='width:40%'>
</center>



### 비선형 함수의 결합

Multi-layer Perceptron (MLP) 모델은 각 레이어가 선형 결합이고, 레이어의 연결이 비선형으로 이루어진 형태이다. 보통 비선형 함수는  `Sigmoid`, `ReLU`, `Tanh` 와 같은 함수들을 사용한다. 아래 그림에서 비선형 부분은 초록색으로 표시하였다. 


<img src="https://drive.google.com/uc?export=view&id=16bC2ZxkRl587IsP1gTIe9WjO8Pk2t5-j" style='width:100%;border:1px solid #FFDDDD;'>


## MLP 학습 결과 


* 데이터 : 1차 + 2차 처방 데이터  (샘플수 1745개 / 코드 개수 5485개)
* **학습데이터** (1221, 5485), (1221,) 
* **테스트데이터** (524, 5485), (524,)


최고 성능을 보이는 모델은 학습에 대해서 234 에러 / 테스트에 대해서 4922 에러를 보였다. 

ACT | HIDDEN_DIM | N_ENTRIES | NUM_LAYERS | OPTIM |  (Eval+Train)/2 | Eval | Train 
|:-:|:-:|:-:|:-:|:-: |---|---|---|
sigmoid | 128 | -1 | 6 | adam | **2578** | **4922** | **234**
sigmoid | 128 | -1 | 4 | adam | **2622** | **5042** | **202**
tanh | 128 | -1 | 6 | sgd | **2665** | **4950** | **381**
sigmoid | 256 | -1 | 4 | adam | **2673** | **5133** | **214**
relu | 256 | -1 | 4 | sgd | **2721** | **5124** | **318**
relu | 256 | -1 | 6 | sgd | **2735** | **5180** | **290**
sigmoid | 256 | -1 | 2 | adam | **2743** | **5264** | **222**


### 8개 학습 모델에 대한 학습 커브 

위 : 테스트 데이터
아래 : 테스트 데이터

그림은 학습경과에 따라서 Error 감소를 보여준다. 

<div class="spanbox" markdown="1" style="width:100%;background-color:#FFFDFD;margin-left:0rem;">

#### Top 5 Model Training Curves

<img src="https://drive.google.com/uc?export=view&id=14FUmw8bGBah6EVQDB6Q7uAaXiLAcD4zV" style='width:100%'>

#### Bottom 5 Model Training Curves

<img src="https://drive.google.com/uc?export=view&id=1Rz3Xqj9wTYDyccLh9G0SGC3QsQU7lOgL" style='width:100%'>

</div>


### Hyperparameter Test

MLP를 학습할 때, 하이퍼파라미터에 따라서 학습 진행 과정을 보여준다. 

<div class="spanbox" markdown="1" style="width:60rem;background-color:#FFFDFD;margin-left:-5rem;">
<center>
<img src="https://drive.google.com/uc?export=view&id=1YiixXR5uBuWe28shhZ1Q9FbANdlVrZTy" style='width:100%'>
</center>
</div>



### 전체 모델 성능

테스트와 학습 에러에 대해서 평균낸 점수를 기준으로 정렬하였다. 


ACT | HIDDEN_DIM | N_ENTRIES | NUM_LAYERS | OPTIM |  (Eval+Train)/2 | Eval | Train 
|:-:|:-:|:-:|:-:|:-: |---|---|---|
sigmoid | 128 | -1 | 6 | adam | **2578** | **4922** | **234**
sigmoid | 128 | -1 | 4 | adam | **2622** | **5042** | **202**
tanh | 128 | -1 | 6 | sgd | **2665** | **4950** | **381**
sigmoid | 256 | -1 | 4 | adam | **2673** | **5133** | **214**
relu | 256 | -1 | 4 | sgd | **2721** | **5124** | **318**
relu | 256 | -1 | 6 | sgd | **2735** | **5180** | **290**
sigmoid | 256 | -1 | 2 | adam | **2743** | **5264** | **222**
tanh | 128 | -1 | 4 | sgd | **2747** | **5099** | **394**
tanh | 256 | -1 | 6 | sgd | **2754** | **5126** | **381**
tanh | 256 | -1 | 4 | sgd | **2839** | **5272** | **405**
relu | 128 | -1 | 6 | sgd | **2850** | **5211** | **489**
sigmoid | 128 | -1 | 2 | adam | **2916** | **5562** | **270**
relu | 128 | -1 | 4 | sgd | **2979** | **5564** | **393**
tanh | 128 | -1 | 2 | sgd | **3005** | **5509** | **500**
relu | 256 | -1 | 2 | sgd | **3024** | **5678** | **371**
relu | 128 | -1 | 2 | sgd | **3039** | **5620** | **458**
tanh | 256 | -1 | 2 | sgd | **3060** | **5612** | **508**
tanh | 128 | -1 | 2 | adam | **3211** | **5754** | **669**
sigmoid | 128 | -1 | 2 | sgd | **3523** | **5576** | **1470**
sigmoid | 256 | -1 | 2 | sgd | **3605** | **5504** | **1706**
sigmoid | 128 | -1 | 4 | sgd | **3703** | **5527** | **1878**
tanh | 128 | -1 | 4 | adam | **3719** | **5685** | **1754**
tanh | 256 | -1 | 2 | adam | **3863** | **5950** | **1776**
sigmoid | 256 | -1 | 4 | sgd | **3932** | **5619** | **2246**
sigmoid | 128 | -1 | 6 | sgd | **3989** | **5673** | **2305**
relu | 128 | -1 | 6 | adam | **4022** | **6548** | **1495**
sigmoid | 256 | -1 | 6 | sgd | **4152** | **5594** | **2710**
relu | 128 | -1 | 4 | adam | **4432** | **8374** | **490**
relu | 256 | -1 | 4 | adam | **4725** | **8854** | **595**
relu | 256 | -1 | 2 | adam | **4846** | **9242** | **451**
relu | 128 | -1 | 2 | adam | **5261** | **9703** | **820**
tanh | 256 | -1 | 4 | adam | **14840** | **15000** | **14679**
tanh | 128 | -1 | 6 | adam | **14931** | **14971** | **14890**
relu | 256 | -1 | 6 | adam | **14944** | **15000** | **14888**
sigmoid | 256 | -1 | 6 | adam | **14944** | **15000** | **14888**
tanh | 256 | -1 | 6 | adam | **14961** | **15030** | **14891**