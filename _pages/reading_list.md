---
layout: page
permalink: /reading_list/
title: Reading List
description: 
nav: false
order : 1
---


The list of research progress 


<div align=left markdown="1">


### Any 

|Name| Key Words|
|---|---|
|Learning a Discriminative Null Space for Person Re-identification|  Null Sapce| 
|implicit Generation and Modeling with Energy-Based Models| Energy based model base paper | 
|Training products of experts by minimizing contrastive divergence | contrastive learning, Hinton |
|Unsupervised Learning of Compositional Energy Concepts | Energy Based Model | 
|Amortized Variational Inference: Towards the Mathematical Foundation and Review | Variational Inference |
|Guiding Pretraining in Reinforcement Learning with Large Language Models  | Language model for RPG game, template, goal |
|Towards Explainable Visual Anomaly Detection | | 
|ConceptFusion: Open-set Multimodal 3D Mapping | 
|Sim-to-real transfer by adapting the action selected by the simulation policy with a learned inverse dynamics model in the real world||

# Meta Learning

|Name| Key Words|
|---|---|
|Adaptive Risk Minimization: Learning to Adapt to Domain Shift| 
|Meta-Learning Symmetries by Reparameterization| 
|Gradient Surgery for Multi-task Learning |



### NeurIPS 2022

| üìú Name | üóÇ Keywords | ‚úîÔ∏è Done|
|---|---|---|
|Semantic Field of Words Represented as Non-Linear Functions, Du et al, 2022 [üîó](https://neurips.cc/virtual/2022/poster/54361) |NLP, Representation | | 
|Explainability Via Causal Self-Talk, Roy et al., 2022 [üîó](https://neurips.cc/virtual/2022/poster/54775)| Interpretability, Causality | |
|CS-Shapley: Class-wise Shapley Values for Data Valuation in Classification, Schoch et al., 2022 [üîó](https://neurips.cc/virtual/2022/poster/53147) |Sharpley |
|Supervised Dimensionality Reduction and Visualization using Centroid-Encoder, Ghosh et al., 2022 [üîó](https://neurips.cc/virtual/2022/poster/56127)||
|Learning Manifold Dimensions with Conditional Variational Autoencoders, Zheng et al., 2022 [üîó](https://neurips.cc/virtual/2022/poster/54434)|||
|Listen to Interpret: Post-hoc Interpretability for Audio Networks with NMF, Parekh et al,. 2022 [üîó](https://neurips.cc/virtual/2022/poster/54064)|||
|Make an Omelette with Breaking Eggs: Zero-Shot Learning for Novel Attribute Synthesis, Li et al., 2022 [üîó](https://neurips.cc/virtual/2022/poster/54453)|||
|Understanding Robust Learning Through the lens, Cianfarani et al., 2022 [üîó](https://neurips.cc/virtual/2022/poster/55106)|||




### ICLR 2022

| üìú Name | üóÇ Keywords | ‚úîÔ∏è Done|
|---|---|---|
|The Information Geometry of Unsupervised Reinforcement Learning [üîó](https://arxiv.org/abs/2110.02719)|||
|AdaRL: What, Where, and How to Adapt in Transfer Reinforcement Learning [üîó](https://arxiv.org/abs/2107.02729)| ||


### NeurIPS 2021

| üìú Name | üóÇ Keywords | ‚úîÔ∏è Done|
|---|---|---|
|Improving Computational Efficiency in Visual Reinforcement Learning via Stored Embeddings [üîó](https://arxiv.org/abs/2103.02886)|||  


### NeurIPS 2017

| üìú Name | üóÇ Keywords | ‚úîÔ∏è Done|
|---|---|---|
|SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability[üîó](https://arxiv.org/abs/1706.05806)|||






### Vision Transformer 
* An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale, ICLR 2021
* Swin Transformer: Hierarchical Vision Transformer using Shifted Windows, ICCV 2021
* CvT: Introducing Convolutions to Vision Transformers, ICCV 2021
* CoAtNet: Marrying Convolution and Attnetion for All Data Sizes
* Masked Autoencoders are Scalable Vision Learners, CVPR 2022.
* XCiT: Cross-Covariance Image Transformers, NeurIPS 2021.
* Deepvit: Towards deeper vision transformer
* Scaling vision transformers
* Emerging properties in self-supervised vision transformers
* Tokens-to-token vit: Training vision transformers from scratch on imagenet

### MultiModality 
* Learning Transferable Visual Models From Natural Language Supervision (CLIP)
* Zero-Shot Text-to-Image Generation, ICML 2021, DALLE
* CogView: Mastering Text-to-Image Generation via Transformers
* Scaling Autoregressive Models for Content-Rich Text-to-Image Generation
* Flamingo: a Visual Language Model for Few-Shot Learning
* Transferring Pre-trained Multimodal Representations with Cross-modal Similarity Matching
* MACK: Multimodal Aligned Conceptual Knowledge for Unpaired Image-text Matching
* M-VADER: A Model for Diffusion with Multimodal Context
* GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models (GLIDE)
* High-Resolution Image Synthesis with Latent Diffusion Models
* Improving Sample Quality of Diffusion Models Using Self-Attention Guidance


### Generator
* Auto-Encoding Variational Bayes
* beta-VAE Learning Basic Visual Concepts with a Constrained Variational Framework
* Understanding disentangling in Œ≤-VAE
* Neural Discrete Representation Learning
* Chroma-VAE: Mitigating Shortcut Learning with Generative Classifiers
* Generative Adversarial Nets, NIPS 2014.
* InfoGAN
* A Style-Based Generator Architecture for Generative Adversarial Networks, CVPR 2019.
* Taming Transformers for High-Resolution Image Synthesis (VQGAN)
* ViTGAN: Training GANs with Vision Transformers, ICLR 2022


### Transformer 
* On Layer Normalization in the Transformer Architecture 
* Self-attention with relative position representations
* BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
* Language Models are Unsupervised Multitask Learners
* Longformer: The long-document transformer
* Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer
* Big bird: Transformers for longer sequences
* An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale, ICLR 2021
* BEiT: BERT Pre-Training of Image Transformers
* Masked Autoencoders are Scalable Vision Learners, CVPR 2022.

###  Neural Architecture Search  
* Neural Architecture Search with Reinforcement Learning
* Large-Scale Evolution of Image Classifiers
* Hierarchical Representations for Efficient Architecture Search,
* DARTS: Differentiable Architecture Search
* BayesNAS: A Bayesian Approach for Neural Architecture Search
* NAT: Neural Architecture Transformer for Accurate and Compact Architectures
* Understanding and Robustifying Differentiable Architecture Search
* Once-for-All: Train One Network and Specialize it for Efficient Deployment,
* Neural Architecture Search in A Proxy Validation Loss Landscape
* Neural Architecture Search without Training

# 2023-02


# 2023-01

* Locating and Editing Factual Associations in GPT (2023.01.02)
* Subspace clustering in high-dimensions: Phase transitions \& Statistical-to-Computational gap (2023.01.02)
*Learning (Very) Simple Generative Models Is Hard (2023.01.02)
* Learning to Follow Instructions in Text-Based Games
* Self-Supervised Learning with an Information Maximization Criterion
* Off-Team Learning
* Learning to Share in Multi-Agent Reinforcement Learning
* Decision Trees with Short Explainable Rules
* Estimating and Explaining Model Performance When Both Covariates and Labels Shift
* VICE: Variational Interpretable Concept Embeddings
* Towards Understanding Grokking: An Effective Theory of Representation Learning
* Chroma-VAE: Mitigating Shortcut Learning with Generative Classifiers
* The Lipschitz Constant of Self-Attention
* Invertible Residual Networks
* Tsetlin Machine Embedding: Representing Words Using Logical Expressions
* Task-Induced Representation Learning (2023.01.09)
* Disentangled Explanations of Neural Network Predictions by Finding Relevant Subspaces 
* A Two-Stage approach for Inference in Neural Networks
* Full-gradient representation for neural network visualization
* Towards Automatic Concept-based Explanations
* Visualizing and Measuring the Geometry of BERT
* Concept whitening for interpretable image recognition
* Using p-values for the comparison of classifiers: pitfalls and alternatives
* Computing valid p-values for image segmentation by selective inference.
* Confidence Sets and Hypothesis Testing in a Likelihood-Free Inference Setting
* Hypothesis Testing Using Pairwise Distances and Associated Kernels
* Scalable and explainable visually-aware recommender systems
* Learning Probabilistic Models from Generator Latent Spaces with Hat EBM
* Generating Long Sequences with Sparse Transformers
* Reformer: The Efficient Transformer
* Explaining knowledge distillation by quantifying the knowledge
* Farewell to mutual information: Variational distillation for cross-modal person re-identification


* Robustness in deep learning: The good (width), the bad (depth), and the ugly (initialization)
* S3GC: Scalable Self-Supervised Graph Clustering 
* A Pattern Discovery Approach to Multivariate Time Series Forecasting
* Rapidly Mixing Multiple-try Metropolis Algorithms for Model Selection Problems
* Quantifying Statistical Significance of Neural Network-based Image Segmentation by Selective Inference
* A Theoretical Understanding of Gradient Bias in Meta-Reinforcement Learning
* Analyzing Sharpness along GD Trajectory: Progressive Sharpening and Edge of Stability
* Conformal Frequency Estimation with Sketched Data
* Self-Consistent Dynamical Field Theory of Kernel Evolution in Wide Neural Networks
* Learning to Branch with Tree MDPs
* SageMix: Saliency-Guided Mixup for Point Clouds
* Wasserstein Logistic Regression with Mixed Features
* On Embeddings for Numerical Features in Tabular Deep Learning
* Temporally Disentangled Representation Learning
* Data-Driven Model-Based Optimization via Invariant Representation Learning
* Poisson Flow Generative Models
* On Translation and Reconstruction Guarantees of the Cycle-Consistent Generative Adversarial Networks
* [Re] Reproducibility Study of ‚ÄúCounterfactual Generative Networks
* FedAvg with Fine Tuning: Local Updates Lead to Representation Learning
* Calibrating Deep Neural Networks Using Explicit Regularisation and Dynamic Data Pruning
* Estimating Information Flow in Deep Neural Networks
* Using effective dimension to analyze feature transformations in deep neural networks
* Task-Induced Representation Learning



### Action Space 

* Learning Action Translator for Meta Reinforcement Learning on Sparse-Reward Tasks
* Learning Cross-Domain Correspondence for Control with Dynamics Cycle-Consistency (HiP-BMDP)
* Meta-q-learning (MQL)
* Efficient off-policy meta-reinforcement learning via probabilistic context variables (PEARL)



</div>

