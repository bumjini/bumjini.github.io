

원천의 메모리를 가져와서 사용하는 부분은 잘 작동한다. 또한 학습이 충분히 더욱 많이 진행되면 성능이 오를 것으로 판단된다. 
가지고 있고, 키를 셋하면 생성한다. 

여러 개의 메모리를 가지고 있는데, 어떤 목적으로 잘 사용하도록 학습할 것인가? 
단순히 암기된 원천 내용을 내뱉는 것이 아니라 이것을 효율적으로 사용하는 것이 목적인가? 

이러한 GPT 구조로부터 얻을 수 있는 이점은 메모리를 명시적으로 눈에 띄게 준다는 점이다. 
그러나, 이 방식으로 하면 단점은 다음과 같다. 
오늘 할 일은 현재 버전의 장점과 단점을 찾고, 모델을 개선하는 것이다. 


마지막 부분에만 있는 메모리 사용 이유, 


1. 워터마크를 학습해서, 생성을 유도할 수 있는가? 임베딩에 특정 패턴을 집어넣고 원하는 카피라이트 문구 생성 
  * Parametric embedding 을 만들어서 watermark처럼 임베딩을 업데이트 하여 내용물을 생성하도록 학습한다. 이 때, 임베딩은 입력단에 넣는다. 
  * memory pool : replacing MLP to memorize the copyright contents 
  * LoRA : existing memory adaptation for copyright contents. 
2. 메모리 학습을 통한 카피라이트 문구 생성 유도 
3. Adaptation을 통해 생성을 조절하는 방법 



### 학습된 모델 평가 방법 

1. 카피라이트 문구를 얼마나 많이 생성해낼 수 있는가? (정량적으로 평가 가능. ROUGE Score, BLUE)

* Plain Text 에 대해서 향후 생성되는 내용 검토
* 50 개 토큰을 주고, 향후 50, 100개에 대해서 얼마나 생성할 수 있는가? 


평가 잣대 : 외우는 성능이 우수할수록 좋다. 

2. 학습하는데 얼마나 오래 걸리는가? 

-----

### 메모리기반 트랜스포머 업데이트 연구

질문 1. 메모리 사이즈는 어떻게 결정하는가? 
질문 2. 메모리는 어떤 레이어에 넣어야 하는가? 


------

글쓰기 

생성 자체에 대해서 제한을 두는 이유는 여러가지가 있다. 대표적으로 생성 속도를 빠르게 하기 위해서 [ ] 이다. 


충분히 많은 내용을 암기하기 위해서는 적절한 수준이 필요하다. 
1) 단순히 메모리를 바꾸는 것은 컨텐츠 생성에 얼마나 효율적인가?

Residual connection 이 있다. MLP 값은 더해지면서 layernorm을 거치지 않는다. 따라서 값이 크다면 큰 영향을 줄 수 있다. 
메모리 hidden 개수는 rank와 연관되어 있으며, rank 가 1 이라면, 해당 메모리가 활성화되면서 나올 있는 기대 벡터 방향은 단 1개이다. 
반대로 메모리 수가 증가함에 따라서, 담을 수 있는 표현 자체도 월등히 많아진다. 


메모라이제이션 성능 평가 PPL 
모델 평가 : PPL

학습 시간 : 얼마나 많은 시간이 걸리는가? 
암기 능력 : 얼마나 원본 컨텐츠를 암기해낼 수 있는가? 

Wikipedia 에 대해서 Large Memory Pool 을 학습시키고, 워터마크를 이용해서 메모리 추출을 하면 어떤가? 
명시적으로 컨텐츠에 대한 보호를 할 수 있다. 
개별 학습은 너무 오래 걸리며, 표현 학습의 장점이 


메모리 바꾸는 방식이 아닌 경우 학습이 오래 걸리며, 원하는 컨텐츠 내용의 생성을 보장해주지 않는다. 