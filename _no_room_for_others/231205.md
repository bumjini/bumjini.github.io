---
layout: distill
title:   ' 231205 🚀' 
date: 2023-12-03
giscus_comments : true
description: 
bibliography: all.bib
authors: 
    - name: Bumjin Park
      affiliations:
        name: KAIST

---

* Confident Adaptive Language Modeling : 중간 레이어의 값으로 단어 샘플링이 바뀌는 정도를 확률적으로 모델링하여, 적절히 원하는 수준에서 시간을 단축하며 단어 생성을 가능하게 만들었다. 레이어를 지나면서 생기는 차이에 대해서 (1) softmax (2) hiddens, (3) classifier 를 활용하여 비교하였고, softmax 방식이 더 적은 레이어가 필요하고 classifier 방식이 효율적인 경우도 많았다. 레이어당 유사도가 높은 경우는 softmax 방식이었다. 

* Prefix-Tuning: Optimizing Continuous Prompts for Generation: Continuous prefix is trained to tackle the task 
 * personalization 에서는 유저마다 prefix token을 생성해서 학슶하였다. continuous vector. Input specific 
 * Controllable generation : Conditional Generation. 생성에 대해서 어떤 값에 의존적으로 생성물을 만드는 것. 
* P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks : prompt tuning 에서 레이어까지 추가로 학습해서, 표현력을 더욱 올렸다. 근본적인 원인은 입력 부분의 표현 부족이었다. 

* Prompt Tuning / Prefix Tuning / P-Tuning : 문장 표현 앞에 추가적인 묘사를 붙여서 에측하는 방법. 해당 방법을 사용한다면, 각 태그마다 학습해야 하며, 생성에 대해서 제약을 걸어야 한다. 또한 정보를 가져다 쓰는데 있어서, 각 테스크를 정의하는 것은 아니다. 멀티 테스크 등, 테스크를 정의하고 각 부분마다 데이터셋을 사용하는 경우에 적합하며, 정보를 가져다 쓰는 것은 Prompt Tuning방식으로는 부적합하다. Controllable, Conditional, 생성이며 Task Specific은 아니다. 

확률적으로 레이어 표현의 유사도를 측정하여, 단축하는 방식의 연구도 있다. 이 방식은 크게 3가지 (softmax distribution, classifier, consequence layer)에 대한 방식으로 차이를 측정한다. 원하는 수준을 설정하고, 측정값을 기반으로 더 깊은 레이어로 가는 것을 멈춘다. GPT의 내부 표현이 많은 레이어를 지날수록 더 심도 있는 단어 표현, 다양한 단어표현을 만들어내며, 적은 레이어만 쓰는 경우, ROUGE-L이 완전하게 일치하지 않지만, 비슷한 수준을 유지한다. 효율성을 원하는 경우라면 적은 레이어에서 멈추는 방식으로 GPT 생성의 효융을 얻을 수 있는 것이다. 

kNN augmented attention layer는 12 레이어 decoder only transformer 모델에서 9번째 레이어로 사용하였다. 

[1] Confident Adaptive Language Modeling
[2] Prefix-Tuning: Optimizing Continuous Prompts for Generation
[3] Optimizing Continuous Prompts for Generation 
[4] Training Language Models with Memory Augmentation
[5] Memorizing Transformers 


----- 

외부 메모리를 사용하여 마지막 레이어에 더하는 경우 [4] 
외부 메모리를 사용하여 중간에 넣는 경우 [5]