---
layout: distill
title:   ' 231202 🚀' 
date: 2023-12-02
giscus_comments : true
description: 
bibliography: all.bib
authors: 
    - name: Bumjin Park
      affiliations:분
        name: KAIST

---


암기하는 것은 privacy 를 침해하며 문장의 퀄리티를 낮추며, 공정성을 해친다. 
그러니까 모델이 데이터를 암기하는지 측정하는 것이 중요하다. [Q] 연구에서 데이터에 대해서 암기와 관련된 세 가지 선형 관계들을 소개한다. 

1. 암기는 모델 사이즈가 증가할수록 증가한다. 
2. 중복될수록 증가한다. 
3. prompt 로 사용된 토큰 단어 수에 따라 증가한다. 


*training data extraction attacks* (Carlini et al., 2020);
1. by querying the GPT-2 language model, Carlini et al. (2020) (manually) identified just 600 memorized training examples out of a 40GB training dataset.
2. In contrast, we are able to show that the 6 billion parameter GPT-J model (Black et al., 2021; Wang and Komatsuzaki, 2021) memorizes at least 1% of its training dataset: The Pile (Gao et al., 2020).
3. While McCoy et al. (2021) broadly study the extent to which language models memorize, their focus is on how to avoid the problem and ensure novelty of model outputs, rather than on studying model risk through identifying the maximal amount of data memorization.

[Q] 논문 이전 연구들은 정보 추출이 가능하다는 점 (URLs, phone numbers, and other personal information) 을 보였고, 이 부분에 대해서 명시적으로 정량화한 연구가 부족했다. 


* differential privacy : removing the personal information from the training data would not hurt the model performance. (Dwork 2006)
* 추가적으로 Kandpal 2022 연구는 학습 데이터가 중복됨에 따라서 모델이 얼마나 데이터를 암기한 데이터를 내보내는지 확인하였다. 
* 

> We first construct a set of prompts from the model’s training set. By feeding prefixes of these prompts into the trained model, we check whether the model has the ability to complete the rest of the example verbatim. (학습데이터의 앞부분을 그대로 집어넣고, 모델이 해당 부분을 기억하는지 확인. )

몇 가지 연결된 선행 연구들 

* our analysis on model scale is informed by preliminary experiments in (Zhang et al., 2017; Carlini et al., ㄴ2020)
* data duplication experiments follow in the line of Lee et al. (2021),
* context length experiments build on hypotheses by Carlini et al. (2020); Ziegler (2021).


**our choice of decoding strategy does not significantly impact our results.**


In this paper we randomly choose subsets of roughly 50,000 sequences, allowing us to efficiently run inference in just a few hours.
Therefore, our second subset is a random sample normalized by both sequence lengths and duplication counts, which allows us to accurately measure memorization of large language models in the worst-case, on highly duplicated data with long prompts.

For each length from 50 to 500 tokens, we collect 50,000 examples duplicated varying numbers of times, totaling roughly 500,000 sequences. ` L − 50 tokens and report the sequence as “extractable” if the model exactly emits the next 50
token suffix of this sequence.


더 큰 모델이 더욱 메모라이제이션을 잘한다. 이는 예측 성능으로 인해서 발생되는 현상은 아니다.
본 연구에서는 discoverability phenomenon 이라는 현상을 제안한다. 해당 현상은 모델에게 더 많은 컨텍스트 정보를 주면 memorization fraction의 비율이 증가하는 현상을 나타낸다. 따라서, prompt 의 길이를 줄여서 해당 현상을 막아야 한다고 말한다. 

---

OpenAI 에서 Embedding 을 가져올 수 있다. 이를 바탕으로 모델에 추가적인 학습을 진행하고, 이 때, 학습을 배포할 때, watermark 를 활용하여, 해당 임베딩으로부터 발생된 컨텐츠라는 점을 명시한다. 


[Q] Quantifying memorization across neural language models 
[H] HARE: Explainable Hate Speech Detection with Step-by-Step Reasoning
[Mc] R. Thomas McCoy, Paul Smolensky, Tal Linzen, Jianfeng Gao, and Asli Celikyilmaz. How much do language models copy from their training data? Evaluating linguistic novelty in text generation using RAVEN. CoRR, abs/2111.09509, 2021.
[Carlini] The secret sharer: Evaluating and testing unintended memorization in neural networks.
[EaaS] Are you copying my model? Protecting the copyright of large language models for EaaS via Backdoor Watermark 