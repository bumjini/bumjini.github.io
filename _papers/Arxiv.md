---
layout: default
title: 'Arxiv'
img: /assets/img/logos/arxiv.png
description: Preprint Papers
gradient: linear-gradient(135deg, #0064e1 0%, #5bd3ff 100%)
hover-gradient: linear-gradient(135deg, #00c6fb 0%, #005bea 100%)
---

| Tag | Title | Year | URL | 
|---| -----| ----| ----| 
| <span class="tag-box mechanistic">Mechanistic </span> | Open Problems in Mechanistic Interpretability | 2025 | - |
| <span class="tag-box interpretability">Interpretability </span>  |  Propositional Interpretability in Artificial Intelligence| 2025 | - |
| | RL + Transformer = A General-Purpose Problem Solver | 2025 | 
| | Harmonic Loss Trains Interpretable AI Models | 2025 | 
| | Kolmogorovâ€“Arnold Transformer | 2024 | 
| | A Comprehensive Survey on Integrating Large Language Models with Knowledge-Based Methods |  2025 | 
| | AXBENCH: Steering LLMs? Even Simple Baselines Outperform Sparse Autoencoders | 2025 | 
| | Why do LLMs attend to the first token | 2025 | 
| | KBLAM: Knowledge base augmented language model | 2025 |
| | Investigating the Effectiveness of a Socratic Chain-of-Thoughts (SocraCoT) Reasoning Method for Task Planning in Robotics, A Case Study | 2025 | 
| | Memory Is All You Need: Testing How Model Memory Affects LLM Performance in Annotation Tasks  | 2025 | 
| | SAEBench: A Comprehensive Benchmark for Sparse Autoencoders in Language Model Interpretability | 2025 | 
| | Optimizing Test-Time Compute via Meta Reinforcement Fine-Tuning | 2025 |
|  | LEANAGENT: Lifelong Learning From Formal Theorem Proving | 2025 | 
| | Cognitive Behaviors that Enable Self-Improving Reasoners, or, Four Habits of Highly Effective STaRs | 2025 |
| | Generative agent-based modeling with actions grounded in physical, social, or digital space using Concordia | 2023 |
 