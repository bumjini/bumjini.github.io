---
layout: default
title: 'Arxiv'
img: /assets/img/logos/arxiv.png
description: Preprint Papers
gradient: linear-gradient(135deg, #0064e1 0%, #5bd3ff 100%)
hover-gradient: linear-gradient(135deg, #00c6fb 0%, #005bea 100%)
---

| Tag | Title | Year | URL | 
|---| -----| ----| ----| 
| <span class="tag-box mechanistic">Mechanistic </span> | Open Problems in Mechanistic Interpretability | 2025 | - |
| <span class="tag-box interpretability">Interpretability </span>  |  Propositional Interpretability in Artificial Intelligence| 2025 | - |
| | RL + Transformer = A General-Purpose Problem Solver | 2025 | 
| | Harmonic Loss Trains Interpretable AI Models | 2025 | 
| | Kolmogorov–Arnold Transformer | 2024 | 
| | A Comprehensive Survey on Integrating Large Language Models with Knowledge-Based Methods |  2025 | 
| | AXBENCH: Steering LLMs? Even Simple Baselines Outperform Sparse Autoencoders | 2025 | 
| | Why do LLMs attend to the first token | 2025 | 
| | KBLAM: Knowledge base augmented language model | 2025 |
| | Investigating the Effectiveness of a Socratic Chain-of-Thoughts (SocraCoT) Reasoning Method for Task Planning in Robotics, A Case Study | 2025 | 
| | Memory Is All You Need: Testing How Model Memory Affects LLM Performance in Annotation Tasks  | 2025 | 
| | SAEBench: A Comprehensive Benchmark for Sparse Autoencoders in Language Model Interpretability | 2025 | 
| | Optimizing Test-Time Compute via Meta Reinforcement Fine-Tuning | 2025 |
|  | LEANAGENT: Lifelong Learning From Formal Theorem Proving | 2025 | 
| | Cognitive Behaviors that Enable Self-Improving Reasoners, or, Four Habits of Highly Effective STaRs | 2025 |
| | Generative agent-based modeling with actions grounded in physical, social, or digital space using Concordia | 2023 |
| IJCAI | Transformers as Soft Reasoners over Language | 2020 | | 
| ICLR | Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks |  2016  | 
| NeurIPS | Interpret Your Decision: Logical Reasoning Regularization for Generalization in Visual Classification | 2024 | 
| EMNLP | CLUTRR: A Diagnostic Benchmark for Inductive Reasoning from Text | 2019
| |  Adaptable Logical Control for Large Language Models | 2024 | 
| NeurIPS  | A Foundation Model for Zero-shot Logical Query Reasoning | 2024 | 
| Tennenbaum | Hybrid Declarative-Imperative Representations for Hybrid Discrete-Continuous Decision-Making | 2024
| <span class="tag-box mechanistic"> | Enhancing Automated Interpretability with Output-Centric Feature Descriptions | 2025 | 
|  ACL | Evaluating the Ripple Effects of Knowledge Editing in Language Models | 2025 | 
|      | Gradient Cuff: Detecting Jailbreak Attacks on Large Language Models by Exploring Refusal Loss Landscapes | 2024
|      | SMOOTHLLM: Defending Large Language Models Against Jailbreaking Attacks | 2024 | 
| ICLR | Visualizing and Understanding Recurrent Networks | 2016 | 
| |  Building Machines That Learn and Think Like People |  2016 | 
| NeurIPS | UKnow: A Unified Knowledge Protocol with Multimodal Knowledge Graph Datasets for Reasoning and Vision-Language Pre-Training | 2024
| NeurIPS | AgentDojo: A Dynamic Environment to Evaluate Prompt Injection Attacks and Defenses for LLM Agents | 2024
|         | Rethinking the Role of Demonstrations: What Makes In-Context LearningWork? | 2022 
|         | Understanding Epistemic Language with a Language-augmented Bayesian Theory of Mind |  2024
| Nature| The effect of ChatGPT on students’ learning performance, learning perception, and higher-order thinking: insights from a meta-analysis | 2025 
|NACCL | Identifying and Mitigating Social Bias Knowledge in Language Models | 2025
| NACCL | OctoTools: An Agentic Framework with Extensible Tools for Complex Reasoning | 2025 
| ICML  | ConceptAttention: Diffusion Transformers Learn Highly Interpretable Features | 2025 
| ICML  | Which Agent Causes Task Failures and When? OnAutomated Failure Attribution of LLM Multi-Agent Systems | 2025 |
| Nature | Assessing and alleviating state anxiety in large language models | 2025

