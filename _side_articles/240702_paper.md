---
layout: distill
authors: 
    - name: Bumjin Park
      affiliations:
        name: KAIST
bibliography: all.bib
giscus_comments: true
disqus_comments: true
date: 2024-07-02
featured: true
title: '7월 논문 리뷰'
description: '7월달에 본 여러 논문들에 대한 리뷰입니다.'
---

## Similarity of Neural Architectures using Adversarial Attack Transferability

* This work proposes Similarity by Attack Transferability (SAT) 
    * Adversarial Attack이 입력 및 그래디언트에 대한 정보를 가지고 있다. 
* Sompalli et al. suggested a quantitative similarity metric based on the on-manifold decision boundary. 
* If two different neural architectures are similar, the AT between the models is high because they share similar vulnerability. 


## Finding Neurons in a Haystack: Case studies with Sparse Probing

* How high-level human-interpretable features are represented within the internal neuron activations of LLMs.
* We train k-sparse linear classifiers on these internal activations to predict the presense of features in the input
    * 초반 레이어는 superposition이 많이 발생한다. 
    * 중간 레이어는 뉴런 자체가 의미를 담고 있는 경우가 많다. 
    * 100개의 특징, 10개의 카테고리, 7 개의 다른 모델 
* Polysemantic neuron은 특정한 특징들에 대해서 강하게 반응한다. French에 대해서 monosemantic한 뉴런도 존재한다. 그들은 태생적으로 다른 거다.
* Adaptive thresholding: at most K nonzero neurons 
* Optimal sparse probing (OSP) : cutting plane algorithm을 사용하여 optimal 한 k개 뉴런을 뽑음. 
* Sparse Classification: A scalable Discrete Optimization Perspective (나중에 공부할 거) 

> https://github.com/wesg52/sparse-probing-paper/blob/main/experiments/probes.py

## Monitoring AI-modified Content at Scale: A case study on the impact of ChatGPT on AI Conference Peer Reviews. 

* 6.5% ~ 16.9% 정도의 peer review 자체가 AI에 의해서 작성되었다. 
* How LLM use is changing our information and knowledge practices. 
* 

