---
layout: distill
authors: 
    - name: Bumjin Park
      affiliations:
        name: KAIST
bibliography: all.bib
giscus_comments: true
disqus_comments: true
date: 2024-03-15
featured: true
toc:
    - name: Introduction
      subsections:
            - name: Q&A
            - name: History
    - name: Preliminaries
      subsections:
            - name: Model based RL
            - name: Meta Learning
    - name: Methods
      subsections:
            - name: Objectives
            - name: Algorithms
    - name: Experiments
      subsections:
            - name: Set Up
            - name: Training and Testing
            - name: Effectiveness
    - name: Notes

title: 'Model-based Meta Learning 논문 리뷰 (ICLR 2019)'
description: '이 포스팅은 Learning to Adapt in Dynamic, Real-World Environments Through Meta-Reinforcement Learning 논문에 대한 공부입니다.'
---

<h2 id="introduction"> 1. Introduction </h2>

강화학습을 모델의 리워드를 최대화하는 방향으로 에이전트를 학습시킨다. 효율적인 강화학습을 위한 방법론은 여러 가지가 있지만, 여기서 중점으로 다루는 방법은 두 가지다. 

1. Model-based RL 
2. Meta-Learning 

이 논문은 MBRL에 대해서 모델을 Meta-learning적으로 trajectory를 샘플링해주는 방법을 연구한다. 바꿔 말하면, **적은 수의 새로운 샘플에 대해서 state distribution을 빠르게 학습하는 meta-learning 적인 environment model 학습**을 목표로 한다. 

적은 수의 샘플에 대해서 다음 환경 상태를 빠르고 정확하게 예측할 수 있다면, 환경모델을 기반으로한  컨트롤은 높은 보상을 얻을 것이라는 가정이다. 이 논문은 2019년에 나왔기에 MBRL에 대한 Meta-learning의 첫 논문인 것으로 보인다. 


<h3 id="q-a"> Pre-Questions and Answers  </h3> 

1. **Meta-learning의 입력과 출력은 무엇인가?** : 입력으로 신경망 파라미터와 테스크에 대한 적은 샘플, 출력으로 업데이트된 파라미터를 준다.  
2. **MBRL + Meta learning으로 컨트롤 하는 방법은 무엇인가?** 모델은 환경에 대해서 다음 state, action을 샘플링하는 용도로 쓰인데, 새로운 데이터에 대해서 적합한 다음 상태와 엑션을 추정하는 것이 목표다. 그 과정에서 action은 MPC와 같은 방식으로 구축될 수 있다. 
3. **학습과 테스트 데이터로 어떻게 제안한 방법론을 검증할 수 있는가?** 학습데이터는 sample-efficiency를 테스트하였고, test 데이터는 학습 때 관찰하지 않은 데이터에 대한 성능을 보고하였다. 

<h3 id="history"> Historical Notes  </h3> 

강화학습의 가장 큰 문제 중 하나는 최적의 policy를 찾기 위해서 수많은 샘플이 사용된다는 점이다. 
Model-based RL은 샘플을 효율적으로 다루기 위해서 많은 연구가 되었다. 대표적으로 Dreamer와 같은 알고리즘은 환경에 대한 모델을 바탕으로 action에 대한 시뮬레이션을 통해서 더 적합한 행동을 찾는다. 

그러나 MBRL은 환경의 transition function이 고정적인 세팅에서 좋은 성능을 보이며, 데이터의 분포가 바뀌는 경우 에러가 누적되어 성능 저하가 나타난다. 이러한 데이터의 변화에 민감하게 모델을 학습하는 대표적인 분야는 meta learning이고, 이 논문은 meta-learning을 환경모델에 적용하여 빠르게 환경의 transition function을 추정하는 방법을 연구한다. 


<h2 id="preliminaries"> 2. Preliminaries </h2>

<h3 id="model-based-rl"> Model-based RL  </h3> 

모델 기반 RL은 transition distribution $p(s' \vert s,a)$을 학습하여, 환경에 대한 모델링을 바탕으로 컨트롤을 효율적으로 학습하거나, 플래닝을 하기 위한 방법이다. 주요 notation으로는 $i,j$ 시간에 대한 trajectory를 나타내는 $\tau(i,j) := (s_i, a_i, \cdots, s_j, a_j, s_{j+1})$이 있다. 

MPC는 $t$ 시간에 대한 행동을 계산하기 위해서 trajectory를 샘플링하고 cost를 계산한다.

1. $t$ 시간에 $[t, t+T]$ 시간에 대한 trajectory를 샘플링 한다. 
2. 각 trajectory에 대한 cost를 계산한다. 
3. $t$ 시간에 가장 적은 cost 를 가지는 행동을 실행한다. 

강화학습에서는 $cost$ 최소화 대신에 보상 최대화를 사용한다. 논문에서는 `model
predictive path integral control (MPPI)` (Williams et al., 2015)를 사용하였다. 

MPC에서는 $k$-번째 rollout에 대해서 컨트롤 입력을 다음과 같이 계산한다. 
MPPI에서는 $u$를 rollout에 대해서 더하는 방식으로 업데이트 한다. 

$$
\begin{gather}
\operatorname{MPC:} a + \delta a_k  \\
\operatorname{MPPI:} a + \delta \frac{\sum_{k} w_k a_k}{\sum_{k} w_k}
\end{gather}
$$

<h3 id="meta-learning"> Meta Learning </h3> 

메타러닝에서는 데이터를 테스크 $\mathcal{T}$ 에 대해서 정의한다. 

* 학습 데이터 : 메타 파라미터를 찾기 위해서 사용되는 데이터
* 테스트 데이터 : 메타 파라미터의 적합성을 확인하기 위해서 사용되는 데이터
* $u$ : 학습데이터와 시작 파라미터로 적합한 파라미터를 반환하는 모델 

$$
\begin{gather}
\min_{\theta, \psi} \mathbf{E}_{\mathcal{T} \sim \rho(\mathcal{T})} 
\big[ \mathcal{L}(\mathcal{D}_{\mathcal{T}}^{\operatorname{test}}, \theta^{'}) \big]  \\ 
 \theta^{'} = u_\psi (\mathcal{D}^{\operatorname{train}}_\mathcal{T} , \theta)
\end{gather}
$$



**Gradient-based meta learning**:
gradient 기반으로 $u_\psi$를 학습하는 방법은 테스크 데이터에 대해서 $\theta$로부터 테스크 목적함수를 최소화하는 방향을 제공하는 것이다. $\theta^{'} = u_\psi(\mathcal{D}_{\mathcal{T}}^{\operatorname{train}}, \theta)$ 는 입력으로 테스크에 대한 데이터와 모델파리미터를 받고, 출력으로 업데이트 되는 파라미터 또는, 방향을 제공한다. 

$$
\begin{equation}
u_\psi(\mathcal{D}^{\operatorname{train}}_{\mathcal{T}}, \theta) 
= \theta - \alpha \nabla_\theta \mathcal{L}(\mathcal{D}_{\mathcal{T}}^{\operatorname{train}}, \theta)
\end{equation}
$$

**Recurrence-based meta-learning**: 방식은 hidden state 를 recurrent 한 방식으로 처리한다. 출력으로 모델 파라미터 또는 테스크에 대한 임베딩을 제공한다. 


<h2 id="methods"> 3. Methods </h2>

환경 $\mathcal{E}$에 대해서 trajectory $\tau_{\mathcal{E}}$ 는 $t$ 시간부터 이후 $K$시간만큼 주어진다.  목적함수는 trajectory를 최대한 복원하는 것이다. 

$$
\begin{equation}
\mathcal{L}(\tau_{\mathcal{E}}(t, t+K), \theta^{'}_{\mathcal{E}}) = 
- \frac{1}{K} \sum_{k=t}^{t+K} \hat{p}_{\theta_{\mathcal{E}}}(s_{m+1}|s_m, a_m)
\end{equation}
$$


<h3 id="objectives"> Objectives  </h3> 

논문에서는 이전 $M$시간부터 이후 $K$시간까지에 대한 trajectory로 식을 작성하였다. 
메타러닝에 대한 수식은 다음과 같다. 

$$
\begin{gather}
\min_{\theta, \psi} 
\mathbf{E}_{\tau_{\mathcal{E}}(t-M, t+K)\sim D}
[\mathcal{L}(\tau_{\mathcal{E}}(t,t+K), \theta_{\mathcal{E}}^{'})]  \\
\theta^{'}_{\mathcal{E}} = u_\psi (\tau_{\mathcal{E}}(t-M, t-1), \theta)
\end{gather}
$$


**Gradient-Based Adaptive Learner (GrBAL)**: 

그래디언트 기반은 시작 파라미터로부터 주어진 데이터에 대한 상태를 최대한 복원하기에 적합한 파라미터를 업데이트한다. 

$$
\begin{equation}
\theta^{'}_{\mathcal{E}} = 
u_{\psi} (\tau_{\mathcal{E}}(t-M, t-1), \theta)  =
\theta_{\mathcal{E}} + \psi \nabla_\theta \frac{1}{M}
\sum_{m=t-M}^{t-1} \log \hat{p}_{\theta_{\mathcal{E}}}(s_{m+1}|s_m, a_m)
\end{equation}
$$

**Recurrence-Based Adaptive Learner (ReBAL)**: recurrent 방식으로 업데이트하는 모델의 파라미터와 모델을 반환한다. gradient 대신 표현학습을 활용하여 업데이트된 모델의 파라미터를 반환한다.  

<h3 id="algorithms"> Algorithms </h3> 

<center>
<img src="https://onedrive.live.com/embed?resid=AE042A624064F8CA%211281&authkey=%21AKjRXYXKSAm70TA&width=1024">
</center>


<h2 id="experiments"> 4. Experiments </h2>


<h3 id="set-up"> Set Up  </h3> 

* Model: the transition probabilities as Gaussian random variables with mean parameterized by a neural network model (3 hidden layers of 512 units each and ReLU activations)


* Model-free RL (TRPO): 다양한 환경에서 학습된 상태 
* Model-free meta-RL (MAML-RL): Finn논문 
* Model-based RL (MB): 기존 MBRL만 사용. 
* Model-based RL with dynamic evaluation (MB+DE): 테스트시 $M$ time step을 관찰하고 모델을 업데이트 하는 경우 

<h3 id="training-and-testing"> Training and Testing </h3> 

학습 과정에서 sample efficiency (모델이 정확할수록 더 적은 데이터로 + 동일한 MBRL시간으로 높은 성능)

<center>
<img src="https://onedrive.live.com/embed?resid=AE042A624064F8CA%211282&authkey=%21AHUKxrEZWBnwBZ0&width=1024">
</center>

테스트 데이터는 학습데이터에서 한 본 환경에 대한 적응력 파악.

<center>
<img src="https://onedrive.live.com/embed?resid=AE042A624064F8CA%211279&authkey=%21AL90UJf82PBXGf4&width=696&height=762" width=50%>
</center>

<h3 id="Effectiveness"> Effectiveness of MB-RL  </h3> 

환경 모델 파라미터를 업데하기 전후로 발생한 에러 정도 (낮을수록 환경을 정확하게 예측)

<center>
<img src="https://onedrive.live.com/embed?resid=AE042A624064F8CA%211280&authkey=%21APo37eLrM-Yets8&width=908&height=674">
</center>

MB-RL이 기존 trajectory를 얼마나 보존하는지 확인한 결과

<center>
<img src="https://onedrive.live.com/embed?resid=AE042A624064F8CA%211278&authkey=%21ANGG4ixz0-bEvOY&width=1024">
</center>

<h3 id="notes"> Notes  </h3> 

MBRL 또한 meta-learning으로 업데이트 해야 한다는 생각은 안해봤기에 신선한 논문이었다. 
이전에 배운 Meta learning을 더욱 이해할 수 있는 논문이었음. 

 

