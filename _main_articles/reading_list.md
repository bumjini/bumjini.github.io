---
layout: distill
authors: 
    - name: Bumjin Park
      affiliations:
        name: KAIST
bibliography: all.bib
giscus_comments: true
disqus_comments: true
date: 2024-08-22 
featured: true
title: 'Reading List'
description: ''
tags: neuron
---



#### 2024.09 

* [Uncovering Intermediate Variables in Transformers using Circuit Probing](https://arxiv.org/abs/2311.04354)
* [Transformer Circuit Faithfulness Metrics Are Not Robust](https://arxiv.org/abs/2407.08734)
* [Multilevel Interpretability Of Artificial Neural Networks: Leveraging Framework And Methods From Neuroscience](https://arxiv.org/abs/2408.12664)
* [Bi-Factorial Preference Optimization: Balancing Safety-Helpfulness in Language Models](https://arxiv.org/abs/2408.15313)


#### 2024.08

* [Enhancing Court View Generation with Knowledge Injection and Guidance](https://aclanthology.org/2024.lrec-main.522/)
* [Precedent-Enhanced Legal Judgment Prediction with LLM and Domain-Model Collaboration](https://aclanthology.org/2023.emnlp-main.740/)
* [Discovering Bias in Latent Space: An Unsupervised Debiasing Approach](https://arxiv.org/abs/2406.03631)
* [Google: 6 incredible images of the human brain built with the help of Google's AI](https://blog.google/technology/research/google-ai-research-new-images-human-brain/)
* [신경학 기초: 뉴런의 기능, 애니메이션](https://www.youtube.com/watch?v=7k5ca2UeLqI&t=69s)
* [Detoxifying Large Language Models via Knowledge Editing](https://arxiv.org/abs/2403.14472)
* [Towards Understanding Sycophancy in Language Models](https://www.anthropic.com/research/towards-understanding-sycophancy-in-language-models)
* [Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training](https://www.anthropic.com/news/sleeper-agents-training-deceptive-llms-that-persist-through-safety-training)
* [Epistemic Injustice in Generative AI](https://arxiv.org/abs/2408.11441)
* [A Mechanistic Understanding of Alignment Algorithms: A Case Study on DPO and Toxicity](https://arxiv.org/abs/2401.01967)
* [Recurrent Neural Networks Learn to Store and Generate Sequences using Non-Linear Representations](https://arxiv.org/abs/2408.10920)
* [Patchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models](https://arxiv.org/abs/2401.06102)
* [The Linear Representation Hypothesis and the Geometry of Large Language Models (ICML 2024)](https://arxiv.org/abs/2311.03658)
* [Function Vectors (ICLR 2024)](https://arxiv.org/abs/2310.15213)
* [Limitations on Formal Verification for AI Safety (Less Wrong)](https://www.lesswrong.com/posts/B2bg677TaS4cmDPzL/limitations-on-formal-verification-for-ai-safety)
* [Provably safe systems: the only path to controllable AGI (Arxiv)](https://arxiv.org/pdf/2309.01933)
* [You don't know how bad most things are nor precisely how they're bad. (Less Wrong)](https://www.lesswrong.com/posts/PJu2HhKsyTEJMxS9a/you-don-t-know-how-bad-most-things-are-nor-precisely-how)