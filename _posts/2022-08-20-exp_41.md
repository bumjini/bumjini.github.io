---
layout: post
title: 'ðŸ”™ #41 Historical Input versus Current State in Reinforcement Learning '
date: 2022-08-20 00:00:00
description: discussions on the historical input
tags: rl
categories: rl
---

## 1. Introduction 

Reinforcement learning methods trains an agent to maximize the sum of rewards, called return. For a given current state, the Markov decision process assumes that the current state has the same amount of information with the history. The current state captures all required information for the history, and therefore, the agent does not need the past information. However, in most cases, this assumption is not true. There are situations where the past n-steps information is needed to take action. For example, the agent can not know the direction of the next ball if the dynamics of the ball is not given. However, RL can be done without giving the history information. In this experiment, we show that the environment where the history is necessary for training the agent and the detailed discussions are present. 

## 2. Training Settings 

There are three environments, Pendulum, CartPole, and Stay. Pendulum and CartPole are OpenAI environments and Stay environment is a customized environment where the reward is obtained when the agent stays at the center of an environment. 

There are two types of environment settings, domain randomization and non-domain randomization. 
The domain randomized environment has non-visible settings such as pole length and gravity which affect an agent action. 
The agent should generalize to all domains to get higher return. In the case of Stay environment, we randomized the wind speed and the direction. 

<center>
<div class="row mt-3">
        {% include figure.html path="assets/img/exp41/model.png" class="img-fluid rounded z-depth-2 max-width-3" zoomable=true %}
</div>
<p> Figure 1. Model archtitectures for PPO and RNN. PPO model just uses the current state. RNN model uses RNN structure to encode the history and concatenate the history embedding and the current state embedding. </p>
</center>


We trained Proximal Policy Optimization (PPO) algorithm for each environment. We also trained RNN version of the PPO where the 5 length history of the current state is given. The RNN model could implicitly capture the non-visible dynamics. 
We trained the PPO model with Tanh activation and RNN model with LSTM. Comparision between two models are in Figure 1.

## 3. Results 

The results are the average of 3 random seed runnings. 

<center>
<div class="row mt-3">
        {% include figure.html path="assets/img/exp41/no_dr.png" class="img-fluid rounded z-depth-2 max-width-3" zoomable=true %}
</div>
<p> Figure 2. No domain Randomization </p>
</center>




<center>
<div class="row mt-3">
        {% include figure.html path="assets/img/exp41/dr.png" class="img-fluid rounded z-depth-2 max-width-3" zoomable=true %}
</div>
<p> Figure 3. Domain Randomization : when dynamics of the enivronment is not visible. </p>
</center>


The non-domain randomized environment result is in Figure 2. PPO showed the higher performance than RNN in Pendulum and CartPole environment, while the RNN performance is higher in the Stay environment. One advantage of RNN is understanding the dynamics. However, learning an action sequence based on the history is more complex than learning on the single observation. We guess that the current state information is enough to train the agent in the control environments, and the history information does not boost the training. In the case of Stay environment, the history information is beneficial to training. 

Therefore, RNN boosts the training when the history has more relationship with an action sequence than the learning speed obtained from current state information 

The domain randomized environment showed that the RNN was superior for all environments (see Figure 3). 
In the setting of domain randomization, the environment dynamics varies significantly. Therefore, the current information is not enough to capture the dynamics and the history information is more useful. 

In all cases, RNN showed the higher variance among seeds while PPO had lower variance which means that the training is not stable. 
 
## 4. Discussion 

In this experiment, we tested whether the history information is useful to agent or not. The history information makes the training faster when **(1) an action is directly related to the history**  and **(2) the current state information is not enough for the agent to learn capture the dynamics.** Even though the RNN has higher performance in many cases, the performance varies among seeds. Therefore, more robust way of training a history-based agent must be presented. 
