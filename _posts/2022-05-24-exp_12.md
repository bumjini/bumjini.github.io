---
layout: post
title: 'ðŸ”® #12 Attention Score in Vision Transformer'
date: 2022-05-24 00:00:00
description: Understanding how each layer focuses image patches in vision transformer
tags: XAI, VISION
categories: XAI
---


# Motivation 

Attention-based deep neural networks such as a transformer have been reached the state-of-the-art performance in natural language and computer vision domains. Before the data is fed into the model, the input data is divided by tokens. Then, the architecture considers the relataionship between tokens. Image classification is done by computing the relationship score between `CLS` token and image tokens. Therefore, it computes normalized token attribution.  Vistion Transformer has several layers and multiple head in each layer. Therefore, there are $N_{layer} \times N_{heads}$ number of attention matrices.  Analyzing individual attention matrix is intractable because there are massive number of samples. In this experiment, we present visualization mechanism and some examples in layer-wise manner. 

## Vision Transformer Architecture

Vision transformer (ViT) is transformer-encoder architecture whose input tokens are sequential image tokens. There are three modules: projection, encoder, classication. 

* **Projection**
  * input  : (3 x 224 x 224 ) image
  * output : (197 x 768), (tokens, hideen dimension)
* **Encoder (12 encoder layers)**
  * input : (197 x 768) 
  * output : (197 x 768)
* **Classification**
  * input : (1 x 768) : class token embedding
  * output : (1 x 1000)  : class logits

In the encoder layer, the multi head attention is applied, in the torch implementation `num_heads`=12, Hence the dimension of each head is `768/12 = 64`
Each head computes attention matrix based on query, key, value embeddings. The Figure 1 shows the attention visualization procedure. The final visualization only considers the attention score of `CLS` token and other image tokens. 

<center>
<div class="row mt-3">
        {% include figure.html path="assets/img/exp12/viT.png" class="img-fluid rounded z-depth-1" zoomable=true %}
</div>
<p> Figure 1. The visualization procedure of a vision transformer  </p>
</center>



### Visualization Questions 

1. The number of patch is discrete, how did we get continuous attention map? : The visualization includes `umsample` and we used `bilinear` interpolation when upsampling `14x14` to `224x224`.

2. Attention Matrix is square matrix, i.e., ther are token pairs. How can we visualize attention score? The plotted image is attention score of `class token` and `image patches`. Therefore, it is attention score at the `class token` row. 

3. There are multiple heads and multiple attention matrices : Normaly, we averge them out. 


## Layer-wise visualization

Here, we present the visualizaiton of attention score for each layer. There are multiple haeds and we take the average of the heads. 
We also visualized the aggregated attention score for all layers by serveral methods. We used `max`, `min`, `average`, and `attention rollout` methods. `attention rollout` is based on the [Quantifying Attention Flow in Transformers (Samira Abnar and Willem Zuidema)](https://arxiv.org/abs/2005.00928). We tested several iamges and divided resutls in four levels by the degree of entropy. 



<br/>
<br/>
<hr/>
<div style="background-color:#FFFEE2">
<h2 style="color:blue"> Spread Level 1</h2>
 
<center>
<div class="row mt-3">
        {% include figure.html path="assets/img/exp12/index_17_quantile_0.png" class="img-fluid rounded z-depth-1" zoomable=true %}
</div>
</center>
<details><summary>(click to expand) <strong>Other images : Level 1</strong></summary>
<center>
<div class="row mt-3">
        {% include figure.html path="assets/img/exp12/index_5_quantile_0.png" class="img-fluid rounded z-depth-1" zoomable=true %}
</div>
</center>

<center>
<div class="row mt-3">
        {% include figure.html path="assets/img/exp12/index_0_quantile_0.png" class="img-fluid rounded z-depth-1" zoomable=true %}
</div>
</center>
</details>
</div>


<br/>
<br/>
<div style="background-color:#FFFDCB">
<h2 style="color:blue"> Spread Level 2</h2>
<center>
<div class="row mt-3">
        {% include figure.html path="assets/img/exp12/index_12_quantile_0.png" class="img-fluid rounded z-depth-1" zoomable=true %}
</div>
</center>

<details><summary>(click to expand) <strong>Other images : Level 2</strong></summary>
<center>
<div class="row mt-3">
        {% include figure.html path="assets/img/exp12/index_2_quantile_0.png" class="img-fluid rounded z-depth-1" zoomable=true %}
</div>
</center>

<center>
<div class="row mt-3">
        {% include figure.html path="assets/img/exp12/index_7_quantile_0.png" class="img-fluid rounded z-depth-1" zoomable=true %}
</div>
</center>
</details>
</div>


<br/>
<br/>
<div style="background-color:#FFFBA0">
<h2 style="color:blue"> Spread Level 3</h2>
<center>
<div class="row mt-3">
        {% include figure.html path="assets/img/exp12/index_16_quantile_0.png" class="img-fluid rounded z-depth-1" zoomable=true %}
</div>
</center>

<details><summary>(click to expand) <strong>Other images  : Level 3</strong></summary>
<center>
<div class="row mt-3">
        {% include figure.html path="assets/img/exp12/index_8_quantile_0.png" class="img-fluid rounded z-depth-1" zoomable=true %}
</div>
</center>
<center>
<div class="row mt-3">
        {% include figure.html path="assets/img/exp12/index_1_quantile_0.png" class="img-fluid rounded z-depth-1" zoomable=true %}
</div>
</center>
</details>
</div>



<br/>
<br/>
<div style="background-color:#FFDEDE">
<h2 style="color:blue"> Spread Level 4</h2>

<center>
<div class="row mt-3">
        {% include figure.html path="assets/img/exp12/index_14_quantile_0.png" class="img-fluid rounded z-depth-1" zoomable=true %}
</div>
</center>
<details><summary>(click to expand) <strong>Other images : Level 4</strong></summary>
<center>
<div class="row mt-3">
        {% include figure.html path="assets/img/exp12/index_13_quantile_0.png" class="img-fluid rounded z-depth-1" zoomable=true %}
</div>
</center>


<center>
<div class="row mt-3">
        {% include figure.html path="assets/img/exp12/index_9_quantile_0.png" class="img-fluid rounded z-depth-1" zoomable=true %}
</div>
</center>
</details>
</div>