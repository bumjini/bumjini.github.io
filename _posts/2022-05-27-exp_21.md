---
layout: post
title: 'ðŸ§® #21 VQ-VAE Architecture and Codebook Size'
date: 2022-05-27 00:00:00
description: Understanding how the number and the size of a codebook affect the performance of VQ-VAE
tags: XAI, VISION
categories: XAI
---


# Introduction 

Auto-Encoders are recently developed by the progress of deep learning. Auto-Encoder learns the low-level representation of input data based on reconstruction objective.  Auto-encoders are widely used to reduce embedding size or to get common vector size for the different types of input. One succesful advance is Vector-Quantized Variational Auto-Encdoer (VQ-VAE) which learns fixed length codes to represent input. VQ-VAE is used in D-ALLE, which is a text-guided image auto-regressive model, to represent an image as tokens. The procedure of VQ-VAE includes replacing an encoded latent vector with the clostest vector in the codebook. Even though the mechanism is simple, it is hard to implement it in differential manner and understanding the overall loss function. In this experiment, we discuss the forward and the backward flow of VQ-VAE and the effect of number of codes in the codebook. 

<br/>

## Forward Pass of VQ-VAE 

Figure 1 shows the forward pass of VQ-VAE. There are three modules: encoder, decoder, and quantizer. The encoder maps input $x$ to $z_e$ which is low-dimensional representation of $x$. Then the quantizer replaces $z_e$ with the closest vectors $e$ in the codebook. Finally, the decoder maps $e$ to $\hat{x}$ to reproduce the input. For example, $x$ of size `32x32x3` becomes `12x12x128` and each `12x12` vectors of size `128` is replaced with closest vector `e` of size `128` in the codebook. When there are `K` number of codes, the computation requires `12x12xK` iteration to construct replaced codes. 


<center>
<div class="row mt-3">
        {% include figure.html path="assets/img/exp21/forward.png" class="img-fluid rounded z-depth-1" zoomable=true %}
</div>
<p> Figure 1. The overall structure of VQ-VAE forward pass encodede latent $z_e$ is replaced by the closest latent $e$ in the codebook </p>
</center>

<br/>


## Backward Pass of VQ-VAE 


The objective is defined by 

$$
\Large{
L = \log p(x|z_q(x))  + ||sg[z_e (x)] - e||_2^2 + \beta||z_e (x) - sg[e]||_2^2}
$$

1. The first is **the reconstruction loss**
2. The second term is for **updating dictionary**, 
3. The third term is **commitment loss** to make sure the encoder commits to the output. 

$sg$ represent stop gradient, that is, the value does not consist of computational graph.  For example, 
$||y-\theta x+sg[\theta^2 x]||$
is just a linear function of $\theta$ because  $sg[\theta^2 x]$ does not backpropagate. 

in the original paper, they used $\beta=0.25$




### Loss 1 : Reconstruction

$$
\Large{
L_{recon} = \log p(x|z_q(x)) }
$$


The reconstruction loss first flows to the decoder, and then encoder. In the implementation the replaced codes are not attached to the computational graph. Therefore, we have to replace $z_e$ vector with values of $e$ by the code below. 

```python
quantized = quantize(input)
input = input - (input.detach() - quantized)  # detach means, stop gradient 
# input.item() = quanized.item() 
```

<center>
<div class="row mt-3">
        {% include figure.html path="assets/img/exp21/recon.png" class="img-fluid rounded z-depth-1" zoomable=true %}
</div>
<p> Figure 2. The backward pass of the reconstruction loss. The red lines represent the computational paths and the edges of red lines are updated modules</p>
</center>


### Loss 2 : Dictionary

$$
\Large{
L_{dict} =||sg[z_e (x)] - e||_2^2 }
$$


The dictionary loss is defined to match the dictionary representation with the encoder representation. As can be seen in Figure 3, the gradient does not flow to the encoder and flow only to the decoder. In the paper, this loss is higher ($\beta=0.25$) to make the encoder representation follows the dictionary representation, that is, the dictionary update is faster. 

<center>
<div class="row mt-3">
        {% include figure.html path="assets/img/exp21/diction.png" class="img-fluid rounded z-depth-1" zoomable=true %}
</div>
<p> Figure 3. The backward pass of the dictionary loss. The red lines represent the computational paths and the edges of red lines are updated modules</p>
</center>


### Loss 3 : Commitment

$$
\Large{
L_{commit} = \beta||z_e (x) - sg[e]||_2^2}
$$

The commitment loss is defined to match the encoder representation with the dictionary representation. As can be seen in Figure 4, the gradient does not flow to the dictionary and flow only to the encoder.

<center>
<div class="row mt-3">
        {% include figure.html path="assets/img/exp21/commit.png" class="img-fluid rounded z-depth-1" zoomable=true %}
</div>
<p> Figure 4. The backward pass of the commitment loss. The red lines represent the computational paths and the edges of red lines are updated modules</p>
</center>

<br/>

# Experiment 

TBD     









