---
layout: post
title: 'üìâ #9 How Gradient-based Input Attribution Differs'
date: 2022-05-06 00:00:00
description: Several gradient methods (gradient, smooth-grad, integrated, input X grad) are discussed with ImageNet Dataset
tags: XAI, VISION
categories: XAI
---


### 1. Motivaiton of this expeirment

Recently, several methods have been developed to explain the decision of deep neural networks. 
Even though the model structure is complex, it can be viewed as a differentiable function with respect to the input and we can compute the derivative of the function to measure how function out come changes as the input changes. There are several methods 

1. Gradient 
2. Input Gradient 
3. Integrated Gradient 
4. Smooth Gradient

In this experiment, we demonstrate the difference between four methods with well known Image-Net dataset. 

### 2. Methods 

Assume we have input data $x$ and classification funcation $f$. 

<style> 
    .class_1 {text-align:left}
    ht,td {padding:10px;}
</style>
<center>    
<table border="2">
    <tr> 
        <td>
            <p>
            <strong >Gradient</strong>
            </p>
            <p class=class_1>
            $$
            \frac{\partial f}{\partial x}
            $$
            </p>
        </td>
        <td>            
            <p  >
                <strong>Input X Gradient</strong>
            </p>
            <p class=class_1>
                $$
                x \times \frac{\partial f}{\partial x}
                $$
            </p> 
        </td>
    </tr>
    <tr padding=15px>
        <td valign="top" padding=15px> 
            <p  >
            <strong>Integrated Gradient</strong>
            </p>
            <p class=class_1>
            Additionally, baselines $\tilde{x}$ is given. 
            integrated gradient is defined by 
            $$
            (x-\tilde{x}) \times \int_0^1 \frac{\partial f}{\partial x} (\alpha x + (1-\alpha) \tilde{x}) \mathrm{d}\alpha
            $$
            </p>
        </td>
        <td  valign="top" > 
            <p  >
            <strong>Smooth Gradient</strong>
            </p>
            <p class=class_1>
            Additionally, noise $\epsilon_i$ is sampled from gaussian distribution with variance $\sigma_i$.
            Smooth-grad is defined by 
            $$
            \sum_{i=1}^M \frac{\partial{f}}{\partial x}(x+\epsilon_i)
            $$
            </p>
        </td>
    </tr>
</table>
</center>    

<br/>

### 3. Results

Here we present 3 types of comparison 

1. comparison between 4 types of gradients 
2. comparison by the number of samples in **IG** and the baseline
3. comparison by the number of samples and the variance of noise for **Smooth-Grad**


<hr/>

#### 3.1 Gradients 


Here are input gradients for 3 images (dog, shark, and husky). We compute the gradients of the pretrained VGG16 model provied by pytorch. We observed two things. 

1. Input x Gradient method generally better than just gradient method. 
2. IG method has little difference with Input x Gradient 
3. Smooth method detects edges well. 

‚ö†Ô∏è You might think, Baselines must be found in better way. Therefore we used other baselines for IG method in Section 3.2. 

<center>
<div class="row mt-3">
        <div class="row-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/exp9/10503_all.png" class="img-fluid rounded z-depth-1" zoomable= true %}
    </div>
    <div class="row-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/exp9/100_all.png" class="img-fluid rounded z-depth-1" zoomable= true %}
    </div>
    <div class="row-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/exp9/12425_all.png" class="img-fluid rounded z-depth-1" zoomable= true %}
            <p style="color:black"> Figure 1. Four types of gradient mehtods. Smooth shows generally better performance. </p>
    </div>
</div>
</center>


<br/>
<hr/>

#### 3.2  Hyperparamter Tuning : Integrated Gradient

* M : the nubmer of gradients :  [10, 20, 50, 100]
* baseline : [zeros, ones, random, random x 2, random x 5]

<details>
<summary><b>üå† See the Figures</b></summary>
<center>
<div class="row mt-3">
        <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/exp9/100_ig.png" class="img-fluid rounded z-depth-1" zoomable= true %}
        </div>
        <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/exp9/10503_ig.png" class="img-fluid rounded z-depth-1" zoomable= true %}
        </div>
</div>
</center>
</details>

<hr/>


#### 3.3  Hyperparamter Tuning : Smooth Grad

* M : the number of gradients : [10, 20, 50, 100]
* Sigma : the standard deviation of gaussian : [0.1, 0.5, 1.0, 2.0, 3.0, 5.0]

<details>
<summary><b>üå† See the Figures</b></summary>
<center>
<div class="row mt-3">
        <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/exp9/100_smooth.png" class="img-fluid rounded z-depth-1" zoomable= true %}
    </div>
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/exp9/10503_smooth.png" class="img-fluid rounded z-depth-1" zoomable= true %}
    </div>
</div>
</center>
</details>




<br/>

### 5. Conclsution

Gradient methods generally capture the important parts of an image. However, the interpretation of negative and positive gradients are not very promising. There are positive and negative parts on edges of an object. In other words, the pixels on the edges have both positive effect and negative effect of an image while other pixels do not affect the function score. One possible explanation is that "when we increase the pixel value, the image becomes more likely to the class object". 