---
layout: post
title: 'ðŸ’¦ #37 Linaer Interpretability in the Representation Space by Classficiation Logit'
date: 2022-07-23 00:00:00
description: constructed represntation space tested with MNIST dataset. 
tags: xai, vision
categories: xai
---

# 1. Introduction

This experiment is based on a question. Why we need a separate class token in the classification task of transformer architecture? The classification can be done with any embedding token such as the first embedding or the pooled embedding. I wanted to know the importance of the class token and why it is required.  Therefore, I took an experiment to test the usefulness of the class token by comparing the representation spaces with and without the class token. 


<blockquote>
Is the class token really necessary? -> Yes 
</blockquote>

The answer starts with the embedding spaces of the tokens. Self-attention mechanism uses variational size of tokens as input and all the tokens are properly represented in a vector space by considering the relationship with other tokens. In the perspective of relationship, the representation of class token is the embedding of relationship between data tokens such as words (NLP) and patches (Vision). 


# 2. Represntation Space for Embeddings

The key property is that all the tokens are in the same vector space.  Therefore, analyzing the positions of embedding of tokens are meaningful. In the case of class token, the representation space could be distinguished with data representation. Now, the classification layer has important effect on the space. The layer has weights to compute the logit score of a label and the score is higher when an embedding vector has higher similarity with the weights. That is, the class token is biased to the direction of the weight vector. 

I considered three cases:
1.	Class token is used for the classification
2.	Class token is not used and the first token is used for the classification 
3.	Class token is not used and a random token is used for the classification 

In the settings 2 and 3, a regularization is emerged on the data representation space. Case 2 is for the first token space and case 3 is for the all data. In Case 1, only the class token is biased to the weight so that other data representations are not biased to the weight vector.  
The Figure below shows the schematic view of the representation space. 

<center>
<div class="w-100">    
        {% include figure.html path="assets/img/exp37/structure.png" class="img-fluid-100 rounded z-depth-2 max-width-2" zoomable=true %}
</div>
<p> Figure 1. The representation spaces formed by different mechanisms </p>
</center>



# 3. Model Structure

The model is implmented with Pytorch. The modules in the model are as follows

```bash
AttentionPool(
  (patch_encoder): Sequential(
    (0): Linear(in_features=49, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=32, bias=True)
    (3): ReLU()
  )
  (positional_encoding): PositionalEncoding(
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (self_attention): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)
  )
  (classifier): Sequential(
    (0): ReLU()
    (1): Linear(in_features=32, out_features=10, bias=True)
  )
)

```


Iâ€™ve tested the effect on the representation space with MNIST dataset. 

First, I changed the input space as tokens with 16 patches of 7x7 size. 


<center>
<div class="row mt-3">
        {% include figure.html path="assets/img/exp37/data.png" class="img-fluid rounded z-depth-2 max-width-3" zoomable=true %}
</div>
<p> Figure 2. Image to patches </p>
</center>


# 4. Training Result


The training loss and the validation score are presented in Figure. 

<center>
<div class="row mt-3">
        {% include figure.html path="assets/img/exp37/train.png" class="img-fluid-100 rounded z-depth-2 max-width-3" zoomable=true %}
</div>
<p> Figure 3. Training results </p>
</center>


As the embedding space is regularized more, the performance drops. It is a bit surprising that using the first token has little difference with the class token. I guess there are two reasons. We used positional encoding which can distinguish the representation for the tokens and this help making a proper representation space without the class token. Another reason is the subspace of the first position tokens are not recover large amount and the separation of the subspaces are done properly. 

This experiment shows that the linear interpretability is a regularization on the representations space that is biased to the weight vector.  This bias could be helpful to the prediction and naturally emerges in computer vision dataset known as a concept. 

