---
layout: post
title: '〽️ #24 DQN buffer size and target update frequency'
date: 2022-06-17 00:00:00
description: Understanding the effect of buffer size and target update frequency on the performance.  
tags: RL
categories: RL
---


# Introduction 

In my experience, training a reinforcement learning model is the hardest task among NLP, Vision, and ML. It is due to the fact that the learning environment is quite dynamic and the trained model should be used to get further samples. There are many RL algorithms and DQN is one of most frequently used algorithm. There are two sensitive hyperparameters: buffer size and target update frequency. Two parameters severely affect the performance, but there is little understanding on the different hyperparameters. Therefore, I conducted an experiments on the performance comparison for different buffer size and target update frequency. 



<center>
<div class="row mt-3">
        {% include figure.html path="assets/img/exp24/model.png" class="img-fluid rounded z-depth-1" zoomable=false %}
</div>
<p> Figure 7. VQ-VAE reconstruction in the early stage. The training captures the overall structure first. Details and color of the image are captured later. </p>
</center>


# Experiments 

I assumed that other hyperparmeters (model structure, learning rate, epsilon strategy) are not sensitive than batch size and target update freqency.  Also notice that

* Neural networks are different for each environment.
* We used the same model architecture for different buffer size and target update frequency. 
* We used same values for other hyperparameters in an environment. Each environment has different values. 


Environments are as follows.

1. `CartPole-v1` 
2. `LunarLander-v2` 
4. `BreakoutNoFrameskip-v4` : complex image based environment.



# Results 

## CartPole

Here we show Q-loss and returns while training and evaluation steps.

<center>
<div class="row mt-3">
        {% include figure.html path="assets/img/exp24/cartpole_train.png" class="img-fluid rounded z-depth-1" zoomable=false %}
</div>
<p> Figure 7. VQ-VAE reconstruction in the early stage. The training captures the overall structure first. Details and color of the image are captured later. </p>
</center>


<center>
<div class="row mt-3">
        {% include figure.html path="assets/img/exp24/cartpole_q.png" class="img-fluid rounded z-depth-1" zoomable=false %}
</div>
<p> Figure 7. VQ-VAE reconstruction in the early stage. The training captures the overall structure first. Details and color of the image are captured later. </p>
</center>



## Lunar Lander V2 

The figure below shows the Q-loss while training the model. 



<center>
<div class="row mt-3">
        {% include figure.html path="assets/img/exp24/lunar_train.png" class="img-fluid rounded z-depth-1" zoomable=false %}
</div>
<p> Figure 7. VQ-VAE reconstruction in the early stage. The training captures the overall structure first. Details and color of the image are captured later. </p>
</center>



<center>
<div class="row mt-3">
        {% include figure.html path="assets/img/exp24/lunar_q.png" class="img-fluid rounded z-depth-1" zoomable=false %}
</div>
<p> Figure 7. VQ-VAE reconstruction in the early stage. The training captures the overall structure first. Details and color of the image are captured later. </p>
</center>




# Conclusion

<hr/>
<hr/>

# Appendix  
## Training configuration

All the models are trained with 

* optimzer : RMSprop
* training samples : 1e5
* batch size : 32
* epsilon_init : 1.0
* epsilon_final : 0.05
* epsilon_max_timesteps : 0.5e5 
* discount_factor : 0.999


#### CartPole Training Configuration


```python
# Encoder
Sequential(
  (0): Linear(in_features=4, out_features=64, bias=True)
  (1): Tanh()
)
# Q Network
Sequential(
  (0): Linear(in_features=64, out_features=32, bias=True)
  (1): Tanh()
  (2): Linear(in_features=32, out_features=2, bias=True)
)
```

#### Lunar Lander v2 Training Configuration


```python
# Encoder
Sequential(
  (0): Linear(in_features=4, out_features=64, bias=True)
  (1): Tanh()
)
# Q Network
Sequential(
  (0): Linear(in_features=64, out_features=32, bias=True)
  (1): Tanh()
  (2): Linear(in_features=32, out_features=2, bias=True)
)
```

#### BreakoutNoFrameskip-v4 Training Configuration

```python
# Encoder
Sequential(
  (0): Conv2d(4, 32, kernel_size=(4, 4), stride=(3, 3))
  (1): Tanh()
  (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (3): Conv2d(32, 64, kernel_size=(4, 4), stride=(3, 3))
  (4): Tanh()
  (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (6): Flatten(start_dim=1, end_dim=-1)
)
# Q Network
Sequential(
  (0): Linear(in_features=256, out_features=64, bias=True)
  (1): Tanh()
  (2): Linear(in_features=64, out_features=4, bias=True)
)
```





