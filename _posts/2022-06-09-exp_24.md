---
layout: post
title: '〽️ #24 DQN buffer size and target update frequency'
date: 2022-06-17 00:00:00
description: Understanding the effect of buffer size and target update frequency on the performance.  
tags: RL
categories: RL
---


# Introduction 

In my experience, training a reinforcement learning model is the hardest task among NLP, Vision, and ML. It is due to the fact that the learning environment is quite dynamic and the trained model should be used to get further samples. There are many RL algorithms and DQN is one of most frequently used algorithm. There are two sensitive hyperparameters: buffer size and target update frequency. Two parameters severely affect the performance, but there is little understanding on the different hyperparameters. Therefore, I conducted an experiments on the performance comparison for different buffer size and target update frequency. 



Figure 1 shows the model architecture of DQN network. Detailed implementation is listed in the appendix. 

<center>
<div class="row mt-3">
        {% include figure.html path="assets/img/exp24/model.png" class="img-fluid rounded z-depth-1" zoomable=false %}
</div>
<p> Figure 1.Structure of DQN. The encoder makes the representation of a state and the Q-network computes the action value of the state.  </p>
</center>


# Experiments 

I assumed that other hyperparmeters (model structure, learning rate, epsilon strategy) are not sensitive than batch size and target update freqency.  Also notice that

* Neural networks are different for each environment.
* We used the same model architecture for different buffer size and target update frequency. 
* We used same values for other hyperparameters in an environment. Each environment has different values. 

Environments are as follows.

1. `CartPole-v1` 
2. `LunarLander-v2` 
4. `BreakoutNoFrameskip-v4` : complex image based environment.



# Results 

This secction presents the training result of DQN model in each environment. 

<hr/>
## CartPole

<p style="text-align:center;">
<img style="border: 3px solid gold; border-radius: 7px;"  width="400px" src="https://www.gymlibrary.ml/_images/cart_pole.gif">
</p>
For detailed information on the environment, see [OpenAI CartPole Documentation](https://www.gymlibrary.ml/environments/classic_control/cart_pole/)


Figure 2 shows returns in the training and the evaluation. Buffer size of 10K shows the best performance, but other buffer sizes showed also comparable performance. The target update frequuency of 1000 showed the  most stable performance in evaluation. 

<center>
<div class="row mt-3">
        {% include figure.html path="assets/img/exp24/cartpole_train.png" class="img-fluid rounded z-depth-1" zoomable=false %}
</div>
<p> Figure 2. Training and evaluation result of DQN in the Cartpole environment.  </p>
</center>

Figure 3 shows Q-loss while training. We can see that large buffer size with frequently changing target network shows high loss in the training.  

<center>
<div class="row mt-3">
        {% include figure.html path="assets/img/exp24/cartpole_q.png" class="img-fluid rounded z-depth-1" zoomable=false %}
</div>
<p> Figure 3. Q-loss in training DQN in the Cartpole environment.</p>
</center>

The performance of the least frequently changing target showed the best performance. In our opinion, it is due to the fact that Cartpole environment does not require much exploration and can be solved easily. 


<hr/>
## Lunar Lander V2 


<p style="text-align:center;">
<img style="border: 3px solid gold; border-radius: 7px;"  width="400px" src="https://www.gymlibrary.ml/_images/lunar_lander.gif">
</p>
For detailed information on the environment, see [OpenAI LunarLander Documentation](https://www.gymlibrary.ml/environments/box2d/lunar_lander/)


Figure 4 shows  returns in the training and the evaluation. The best performance is achieved with buffersize 100K and target update frequency 100. Even though other hyperparamters showed comparable performance, this setting showed the fastest performance increase by updating the target network frequently. 

<center>
<div class="row mt-3">
        {% include figure.html path="assets/img/exp24/lunar_train.png" class="img-fluid rounded z-depth-1" zoomable=false %}
</div>
<p> Figure 4. Training and evaluation result of DQN in the LunarLander-v2 environment.  </p>

</center>

Figure 5 shows Q-loss in the training. The best performance case showed increasing Q-loss. The fastest update of the target made the Q-loss increasing. Note that smaller buffer size showed decreasing trend.  The performance is not directly related to the late update of the target network.


<center>
<div class="row mt-3">
        {% include figure.html path="assets/img/exp24/lunar_q.png" class="img-fluid rounded z-depth-1" zoomable=false %}
</div>
<p> Figure 5. Q-loss in training DQN in the LunarLander-v2 environment.</p>

</center>


<hr/>
## Breakout


<p style="text-align:center;">
<img style="border: 3px solid gold; border-radius: 7px;"  width="200px" src="https://www.gymlibrary.ml/_images/breakout.gif">
</p>
For detailed information on the environment, see [OpenAI Breakout Documentation](https://www.gymlibrary.ml/environments/atari/breakout/)

In the original DQN paper, they trained model more than 1M timesteps, but here we trained model only 0.1M steps. Therefore, the experiment only shows the performance comparison in early stage of training.  Figure 6 shows the training result in the BreakoutNoFrameskip-v4 environment. The result shows that updating target frequently results in the fast training. 


<center>
<div class="row mt-3">
        {% include figure.html path="assets/img/exp24/breakout_train.png" class="img-fluid rounded z-depth-1" zoomable=false %}
</div>
<p> Figure 6. Training and evaluation result of DQN in the BreakoutNoFrameskip-v4 environment.  </p>

</center>

Figure 7 shows Q-loss in the training. The error increasing when the target is updated frequently. With the same udpate frequency, larger the buffer size, higher the erorr. 


<center>
<div class="row mt-3">
        {% include figure.html path="assets/img/exp24/breakout_q.png" class="img-fluid rounded z-depth-1" zoomable=false %}
</div>
<p> Figure 7. Q-loss in training DQN in the BreakoutNoFrameskip-v4 environment.</p>

</center>

# Conclusion

1.	Target update frequency determines the performance of an agent by targeting the improved bellman optimality. If the update is done frequently, the agent Q-loss is high and reward increases. On the other hand, if the update is slow, the agent tends to have slow improvement. Stability vs Fast Learning. 
2.	Buffer size also controls the speed of learning. There is little gap when the buffer size is big enough. Therefore, we may avoid small buffer size. 
3.	Use buffer size big enough, and tune the target update frequency. 

Note that the experiments checked only the early part of training.  We do not show the final performance on full training. 

<hr/>


# Appendix  
## Training configuration

All the models are trained with 

* optimzer : RMSprop
* training samples : 1e5
* batch size : 32
* epsilon_init : 1.0
* epsilon_final : 0.05
* epsilon_max_timesteps : 0.5e5 
* discount_factor : 0.999


#### CartPole Training Configuration


```python
# Encoder
Sequential(
  (0): Linear(in_features=4, out_features=64, bias=True)
  (1): Tanh()
)
# Q Network
Sequential(
  (0): Linear(in_features=64, out_features=32, bias=True)
  (1): Tanh()
  (2): Linear(in_features=32, out_features=2, bias=True)
)
```

#### Lunar Lander v2 Training Configuration


```python
# Encoder
Sequential(
  (0): Linear(in_features=4, out_features=64, bias=True)
  (1): Tanh()
)
# Q Network
Sequential(
  (0): Linear(in_features=64, out_features=32, bias=True)
  (1): Tanh()
  (2): Linear(in_features=32, out_features=2, bias=True)
)
```

#### BreakoutNoFrameskip-v4 Training Configuration

```python
# Encoder
Sequential(
  (0): Conv2d(4, 32, kernel_size=(4, 4), stride=(3, 3))
  (1): Tanh()
  (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (3): Conv2d(32, 64, kernel_size=(4, 4), stride=(3, 3))
  (4): Tanh()
  (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (6): Flatten(start_dim=1, end_dim=-1)
)
# Q Network
Sequential(
  (0): Linear(in_features=256, out_features=64, bias=True)
  (1): Tanh()
  (2): Linear(in_features=64, out_features=4, bias=True)
)
```





