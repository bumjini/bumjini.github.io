---
layout: post
title: 'üéØ #2 The Dynamics of LRP in CIFAR 10 Training : Entropy'
date: 2022-04-27 11:12:00-0400
description: an example of a blog post with some math
tags: RL LRP XAI
categories: XAI-posts
---

<h1 style="font-family:Cursive"> Abstract </h1>

Measuring the difference between relevances is important. In this experiment I conducted fair relevance comparison by suggesting normalization. 
In addition, I calculated the entropy of relevance scores by considering it as a probability density function. I found that the entropy generally decreases while trianing. 


<h1 style="font-family:Cursive">  üéØ Setup</h1>

It follows the previous experiment [exp1](). Therefore, the setup is right after exp1. 

<center>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/exp2/previous_work.png" class="img-fluid rounded z-depth-1" zoomable=false %}
        <p style="color:blue"> Figure 1. CIFAR 10 images  </p>
    </div>
</div>
</center>

<h2 style="font-family:Cursive">  üéØ Normalized Comparison</h2>

We comptue the normalized relevance score by min-max scaling so that,


### Normalize 

1. Sum to one : $\sum_p R_p(x) = 1$

2. Min max scaling :   $R_p(x) = \frac{R_p(x) - min(R_p(x))}{max(R_p(x)) - min(R_p(x))}$


To accomplish the properties we normalized by

\begin{equation}
\label{normalize}
R'_p (x) = minmax(\frac{R_p(x)}{\sum_p R_p(x)})
\end{equation}


You can see that the relevance score increases while trianing because the prediction score increases in Figure 2. Therefore, it is necessary to normalize the score by Eq (\ref{normalize}). 

<center>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/exp2/unnormalized_relevance_score.png" class="img-fluid rounded z-depth-0" zoomable=true %}
                <p style="color:black"> Figure 2. $\sum_p R_p$ while training </p>
    </div>
        <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/exp2/normalized_relevance_score.png" class="img-fluid rounded z-depth-0 " zoomable=true %}
        <p>
        Figure 3. Normalized  $\sum_p R_p^{'}$ while training
        </p>
    </div>
</div>
</center>


#### ‚ö†Ô∏è Warning 

May relevance score already be in range $R_p \in [0, f(x)]$, so min clipping is unnecessary. 



---
<br/>

<h1 style="font-family:Cursive">  üöÄ Result</h1>


### Baslines vs Fair

Frankly, the normalization does not change the overall behavior of the learning dynamics. As the Figure 4 and 5 show, there is almost zero difference when applying normalization. **However, It is still necessary to fairly compare the relevance scores.**


<center>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/exp2/Dynamics.png" class="img-fluid rounded z-depth-1" zoomable=false %}
                <p style="color:black"> Figure 4. Difference between successive iteration for relevance $|R_{i+1} - R_i|$ </p>
    </div>
</div>
</center>


<center>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/exp2/Difference.png" class="img-fluid rounded z-depth-1" zoomable=false %}
                <p style="color:black"> Figure 5. relevance $|R_{i} - R_0|$ </p>
    </div>
</div>
</center>


### Entropy 

Here, we propose an entropy perspective on the relevance score. The `normalized relevance score` can be considred as distribution over pixels because it is positive and sum to 1 over pixels. 


\begin{equation}
\sum_p R_p(x) = 1 
\end{equation}

\begin{equation}
R_p(x) \ge 0
\end{equation}

The `entropy of normalized relevance score` is given by 

$$
H(R'(x)) = - \sum_p R'(p) \log R'(p) 
$$


 We measured the entropy of relevance score over pixels for `100` samples. Figure 6 shows the entropy of relevance for each samples while training. We can see that it decreases almost monotonically. 




<center>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/exp2/entropy_mean.png" class="img-fluid rounded z-depth-1" zoomable=false %}
                <p style="color:black"> Figure 6. $H(R'(x))$ for 100 sample images </p>
    </div>
</div>
</center>


Figure 7 shows the dynamics of entropy for few samples. The decreasing trend is not same for all samples. I guess there are some patterns. 



<center>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/exp2/entropy_samples.png" class="img-fluid rounded z-depth-2" zoomable=true %}
                <p style="color:black"> Figure 7. $H(R'(x))$  for individual samples  </p>
</div>
</div>
</center>


### Pixel-wise entropy 

Instead of considering the density over pixels, we can measure the density over training for each pixel. The density of pixel $p$ over training is defined by 

$$
M_p^x(t) = \frac{R^t_p(x)}{\sum_{t'} R^{t'}_p (x)   }
$$

where $$R^t_p(x)$$ represents the relevance score at time step $t$ for pixel $p$ of input image $x$.  Figure 8 shows the entropy over training for some samples and Figure 9 shows sorted plot. The result indicates there are some images whose dynamical change occurs over all pixel positions while some are not. 


<center>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/exp2/pixel_entropy.png" class="img-fluid rounded z-depth-2" zoomable=true %}
                <p style="color:black"> Figure 8. $H(M_p^x)$ for 49 samples   </p>
    </div>
        <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/exp2/pixel_entropy_sorted.png" class="img-fluid rounded z-depth-2 " zoomable=true %}
        <p>
        Figure 9. $H(M_p^x)$ sorted by entropy for 49 samples . 
        </p>
    </div>
</div>
</center>





<center>
<div class="row mt-3">

</div>
</center>

<h1 style="font-family:Cursive">  What I've learned</h1>


* I think row entropy pixel positions are important and may represent something like edges. 
* The entropy decreaes almost monotonically, but the ratio is not same for all images 
* considering the relevance as distribution is nice. 

---