---
layout: post
title: 'üéØ #1 The Dynamics of LRP in Reinforcement Learning'
date: 2022-04-15 11:12:00-0400
description: an example of a blog post with some math
tags: RL LRP XAI
categories: XAI-posts
---

## Motivation / Overview / Goal

**Training** deep reinforcement learning requires massive amount of data and model parameters. The training procedure is not even interpretable and not directly analyzed because the dynamic change of the training is too complicate. However, it is well known that a model has a bias and training follows a specific path. Therefore, it is desirable to see the training procedure. **This experiment demonstrate a way to visualize and check the training procedure of deep reinforcement learning** with popular RL methods PPO. The training is environment is Atari-Breakout environment.




<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/dynamics_of_lrp/observation_rgb.png" class="img-fluid rounded z-depth-10" zoomable=false %}
    </div>
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/dynamics_of_lrp/observation.png" class="img-fluid rounded z-depth-1" zoomable=false %}
    </div>
</div>

<div class="caption">
    [LEFT] orginal RGB observation example <br/> [RIGHT] 4 stacked 84x84x1 low resolution image for training. 
</div>



<br>

## Experiment Setting

We trained CNN model with PPO algorithm in Atari-Breakout environment. We stored LRP flow of specific observation set $$C$$ at each training procedure $$t$$ (here we used frequency of 15000 time step to store samples). We produce a set of LRP scores $$\{ R_1^c, R_2^c, \cdots \}$$ for each observation $$c\in C$$. In this experiment we used only the first observation of the episode (the start state of the Breakout game).


Here is our CNN Model structure...

```python
conv1 = nn.Conv2d(in_channels= 4, out_channels=32, kernel_size=8, stride=4)	
conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2)
conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1)
cnn_final_size = 3164

fc1 = nn.Linear(in_features=3164, out_features=512)

action_branch = nn.Linear(in_features=512, out_features=4)
value_branch = nn.Linear(in_features=512, out_features=1)
```

---

The reward graph is in the Figure below. We trained the model for 1M samples. Although we can get higher score. The most dynamical part is in the beginning. 


<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/dynamics_of_lrp/Reward.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>


---
<br/>

## üöÄ Experiment! 

I'm going to track the change of LRP scores for a single observation. 
We tested three types of LRP rules to visually check the dynamics of LRPs


* *gamma - epsilon - zero* : negative result $$\rightarrow$$ no pattern can be seen
* *gamma - epsilon - zero* (lower weight) : negative result $$\rightarrow$$ no pattern can be seen
* **z_plus**  : positive result $$\rightarrow$$ LRP score formulate some patterns.

---

<br/>

### ‚ùå First Result (Negative )

```python
# applied in the order of [0,0,1,2,2] 
rules = [
    {"rule": "gamma", "gamma":0.25, "eps":1e-9},  # gamma 
    {"rule": "zero",  "eps":0.25},                # eps 
    {"rule": "zero",  "eps":1e-10}                # zero
]
```

Hello World Here!

Other Image is here.

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/dynamics_of_lrp/lrps_action_1.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/dynamics_of_lrp/lrps_value_1.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

<div class="caption">
    [LEFT] action lrp <br/> 
    [RIGHT] value lrp<br/>
     The first panel is the input of the LRP model. 
</div>


<br/>

### ‚ùå  Second Result (Negative )

```python
# applied in the order of [0,0,1,2,2]
rules = [
    {"rule": "gamma", "gamma":0.10, "eps":1e-9},  # gamma 
    {"rule": "zero",  "eps":0.10},                # eps 
    {"rule": "zero",  "eps":1e-10}                # zero
]

```

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/dynamics_of_lrp/lrps_action_2.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/dynamics_of_lrp/lrps_value_2.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

<div class="caption">
    The scale did affect the result much. So I changed the rule after this setting. 
</div>

<br/>

###  ‚úÖ Third Result (BingGo!)


```python
# applied to all layers.
rules = [
    {"rule": "z_plus", "eps":1e-9},  # z_plus
]
```

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/dynamics_of_lrp/lrps_action_3.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/dynamics_of_lrp/lrps_value_3.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

<div class="caption">
    Z_plus preserves the overall input space and there is a change in the highlighted parts and the LRP scores can be clustered. Thereofre, I found that z_plus rule can detect the dynamics of LRP scores in Reinforcement Learning.
</div>


<br/>

### Conclusion

I basically want to detect the change of LRP flows and therefore, the LRP socres should not change much. LRP rules of gamma, epsilon, and zero show more dynamicall change than z_plus rule which preserves only positive propagation. Therefore, it can be used for futher experiment. 


<br/>

#### Appendix A 

PPO training configures.

```bash
--ppo-sgd-iter 100
--ppo-lambda 0.95
--ppo-kl-coefficient 0.5
--ppo-kl-target 0.01
--ppo-sgd-minibatch-size 500
--ppo-entropy-coefficient 0.01
--ppo-clip-parameter 0.1
--ppo-vf-clip-param 10.0
--ppo-grad-clip 10000
--ppo-num-sgd-iter 10
--horizon 4000000
--train-batch-size 5000
--rollout-fragment-length 100
--target-network-update-freq 12800
--num-envs-per-worker 2
```