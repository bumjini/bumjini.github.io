---
layout: post
title: 'ðŸŽ¯ #8 LRP scores while training for rosbot navigation'
date: 2022-04-28 11:53:01
description: an example of a blog post with some math
tags: Vision, Visualization
categories: Vision
---


# 1. Motivaiton of this expeirment

In recent years, deep learning has achieved state-of-the-art control in many fields, including robotics, medicals, and visions. There are many studies to explain the decision process of the deep neural network. Most of the studies assumed that the model is fully trained. However, there is another branch which describes the dynamics of training itself. As the usage of deep learning increases, it is necessary to explain the change of the decision process while training. In this experiments we demonstrate the scores of Layer-wise relevance propgation (LRP) for robot navigation task. 


## 2. Training 

We used Rosbot simulator in Gazebo and 3 layer fully connected model to train the policy of the model. Our training algorithm is Proximal Policy Optimizer (PPO) which is widely used on-policy method to train a reinforcement leraning agent. Even though PPO can be trained with continuous actions, we used discrete action to clearly distinguish the actions. Our observation space includes the kinematic information of the robot and the position of the target.


## 3. Tracking LRP scores while training

Firstly, we trained the model $\pi(a|s;\theta)$ for `1M` samples and stored the model parameters. 
We collected `100` random state samples from the environment. We computed normalized LRP scores of the model to gaurentee that $\sum_p R_p = 1$. It is necessary for the fair comparison between LRP scores. 


## 4. Results

Figure 1 shows the mean LRP scores of 100 states plotted over training time.


<center>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/exp2/Dynamics.png" class="img-fluid rounded z-depth-1" zoomable=false %}
            <p style="color:black"> Figure 4. Difference between successive iteration for relevance $|R_{i+1} - R_i|$ </p>
    </div>
</div>
</center>